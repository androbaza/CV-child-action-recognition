{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44ef7d31-79f4-4b8d-84b7-03a87ce489d9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bf8PpPXtVvmg",
    "outputId": "2c685a33-474b-4e71-8f98-c2533c66095e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2019 NVIDIA Corporation\n",
      "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
      "Cuda compilation tools, release 10.1, V10.1.243\n",
      "gcc (Ubuntu 8.4.0-3ubuntu2) 8.4.0\n",
      "Copyright (C) 2018 Free Software Foundation, Inc.\n",
      "This is free software; see the source for copying conditions.  There is NO\n",
      "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check nvcc version\n",
    "!nvcc -V\n",
    "# Check GCC version\n",
    "!gcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bb1a682-5796-45c9-be5f-570b0f780a26",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "No_zZAFpWC-a",
    "outputId": "1d425eea-d44e-434a-991c-01eb15abaab2",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0 True\n",
      "0.16.0\n",
      "10.2\n",
      "GCC 7.3\n"
     ]
    }
   ],
   "source": [
    "# Check Pytorch installation\n",
    "import torch, torchvision\n",
    "print(torch.__version__, torch.cuda.is_available())\n",
    "\n",
    "# Check MMAction2 installation\n",
    "import mmaction\n",
    "print(mmaction.__version__)\n",
    "\n",
    "# Check MMCV installation\n",
    "from mmcv.ops import get_compiling_cuda_version, get_compiler_version\n",
    "print(get_compiling_cuda_version())\n",
    "print(get_compiler_version())\n",
    "\n",
    "import sys\n",
    "# sys.path.append('/home/actrec/.local/lib/python3.6/site-packages/decord-0.5.3-py3.6-linux-x86_64.egg')\n",
    "# import decord\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "916b8b4d-5611-4ec9-9411-451c41a2f4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/robt427nv/childact\n"
     ]
    }
   ],
   "source": [
    "cd /home/robt427nv/childact/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981efb1d-74e1-464e-aaef-6ce83a034aea",
   "metadata": {},
   "source": [
    "# CSN age BOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec97f7fb-9aae-4f6b-9238-706fd00a86ec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "64CW6d_AaT-Q",
    "outputId": "3b284fd8-4ee7-4a34-90d7-5023cd123a04",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-07-21 17:39:20--  https://download.openmmlab.com/mmaction/recognition/csn/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth\n",
      "Resolving download.openmmlab.com (download.openmmlab.com)... 47.75.20.25\n",
      "Connecting to download.openmmlab.com (download.openmmlab.com)|47.75.20.25|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 119580180 (114M) [application/octet-stream]\n",
      "Saving to: ‘checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth’\n",
      "\n",
      "checkpoints/ircsn_i 100%[===================>] 114,04M  6,78MB/s    in 17s     \n",
      "\n",
      "2021-07-21 17:39:39 (6,87 MB/s) - ‘checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth’ saved [119580180/119580180]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir checkpoints\n",
    "!wget -c https://download.openmmlab.com/mmaction/recognition/csn/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth \\\n",
    "      -O checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "775b5624-5baf-41d7-b8e6-e9a274bb317b",
   "metadata": {
    "id": "LjCcmCKOjktc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mmcv import Config\n",
    "cfg = Config.fromfile('mmaction2/configs/recognition/csn/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1cdd7df-c256-4d1a-b47b-1a027c33d315",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "tlhu9byjjt-K",
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "outputId": "13d1bb01-f351-48c0-9efb-0bdf2c125d29",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "model = dict(\n",
      "    type='Recognizer3D',\n",
      "    backbone=dict(\n",
      "        type='ResNet3dCSN',\n",
      "        pretrained2d=False,\n",
      "        pretrained=\n",
      "        'https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth',\n",
      "        depth=152,\n",
      "        with_pool2=False,\n",
      "        bottleneck_mode='ir',\n",
      "        norm_eval=True,\n",
      "        zero_init_residual=False,\n",
      "        bn_frozen=True),\n",
      "    cls_head=dict(\n",
      "        type='I3DHead',\n",
      "        num_classes=6,\n",
      "        in_channels=2048,\n",
      "        spatial_type='avg',\n",
      "        dropout_ratio=0.5,\n",
      "        init_std=0.01),\n",
      "    train_cfg=None,\n",
      "    test_cfg=dict(average_clips='prob'))\n",
      "checkpoint_config = dict(interval=20)\n",
      "log_config = dict(interval=100, hooks=[dict(type='TextLoggerHook')])\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = 'checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth'\n",
      "resume_from = None\n",
      "workflow = [('train', 1)]\n",
      "dataset_type = 'RawframeDataset'\n",
      "data_root = 'age-gender-3split-rgb-frames/'\n",
      "data_root_val = 'age-gender-3split-rgb-frames/val/'\n",
      "ann_file_train = 'age-gender-3split-rgb-frames/childact_train_rgb320_age_box.txt'\n",
      "ann_file_val = 'age-gender-3split-rgb-frames/childact_val_rgb320_age_box.txt'\n",
      "ann_file_test = 'age-gender-3split-rgb-frames/childact_test_rgb320_age_box.txt'\n",
      "img_norm_cfg = dict(\n",
      "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_bgr=False)\n",
      "train_pipeline = [\n",
      "    dict(type='SampleFrames', clip_len=32, frame_interval=2, num_clips=1),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='RandomResizedCrop'),\n",
      "    dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "    dict(type='Flip', flip_ratio=0.5),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "]\n",
      "val_pipeline = [\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=32,\n",
      "        frame_interval=2,\n",
      "        num_clips=1,\n",
      "        test_mode=True),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='CenterCrop', crop_size=224),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=32,\n",
      "        frame_interval=2,\n",
      "        num_clips=10,\n",
      "        test_mode=True),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='ThreeCrop', crop_size=256),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "data = dict(\n",
      "    videos_per_gpu=4,\n",
      "    workers_per_gpu=4,\n",
      "    train=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_train_rgb320_age_box.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/train/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=1),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='RandomResizedCrop'),\n",
      "            dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "            dict(type='Flip', flip_ratio=0.5),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'),\n",
      "    val=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file='age-gender-3split-rgb-frames/childact_val_rgb320_age_box.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/val/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=1,\n",
      "                test_mode=True),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='CenterCrop', crop_size=224),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'),\n",
      "    test=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_test_rgb320_age_box.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/test/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=10,\n",
      "                test_mode=True),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='ThreeCrop', crop_size=256),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'))\n",
      "evaluation = dict(\n",
      "    interval=5, metrics=['top_k_accuracy', 'mean_class_accuracy'])\n",
      "optimizer = dict(type='SGD', lr=0.000125, momentum=0.9, weight_decay=0.0001)\n",
      "optimizer_config = dict(grad_clip=dict(max_norm=40, norm_type=2))\n",
      "lr_config = dict(\n",
      "    policy='step',\n",
      "    step=[32, 48],\n",
      "    warmup='linear',\n",
      "    warmup_ratio=0.1,\n",
      "    warmup_by_epoch=True,\n",
      "    warmup_iters=16)\n",
      "total_epochs = 50\n",
      "work_dir = './childact-checkpoints/CSN-age-box'\n",
      "find_unused_parameters = True\n",
      "omnisource = False\n",
      "seed = 42\n",
      "gpu_ids = range(0, 1)\n",
      "output_config = dict(out='./childact-checkpoints/CSN-age-box/results.json')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mmcv.runner import set_random_seed\n",
    "\n",
    "# Modify dataset type and path\n",
    "# cfg.dataset_type = 'VideoDataset'\n",
    "cfg.data_root = 'age-gender-3split-rgb-frames/'\n",
    "cfg.data_root_val = 'age-gender-3split-rgb-frames/val/'\n",
    "cfg.ann_file_train = 'age-gender-3split-rgb-frames/childact_train_rgb320_age_box.txt'\n",
    "cfg.ann_file_val = 'age-gender-3split-rgb-frames/childact_val_rgb320_age_box.txt'\n",
    "cfg.ann_file_test = 'age-gender-3split-rgb-frames/childact_test_rgb320_age_box.txt'\n",
    "\n",
    "# cfg.data.test.type = 'VideoDataset'\n",
    "cfg.data.test.ann_file = 'age-gender-3split-rgb-frames/childact_test_rgb320_age_box.txt'\n",
    "cfg.data.test.data_prefix = 'age-gender-3split-rgb-frames/test/'\n",
    "\n",
    "# cfg.data.train.type = 'VideoDataset'\n",
    "cfg.data.train.ann_file = 'age-gender-3split-rgb-frames/childact_train_rgb320_age_box.txt'\n",
    "cfg.data.train.data_prefix = 'age-gender-3split-rgb-frames/train/'\n",
    "\n",
    "# cfg.data.val.type = 'VideoDataset'\n",
    "cfg.data.val.ann_file = 'age-gender-3split-rgb-frames/childact_val_rgb320_age_box.txt'\n",
    "cfg.data.val.data_prefix = 'age-gender-3split-rgb-frames/val/'\n",
    "\n",
    "# cfg.data.test.modality = 'Flow'\n",
    "# cfg.data.val.modality = 'Flow'\n",
    "# cfg.data.train.modality = 'Flow'\n",
    "\n",
    "# cfg.data.train.start_index = 0\n",
    "# cfg.data.test.start_index = 0\n",
    "# cfg.data.val.start_index = 0\n",
    "\n",
    "cfg.data.test.filename_tmpl = '{:03}.jpeg'\n",
    "cfg.data.train.filename_tmpl = '{:03}.jpeg'\n",
    "cfg.data.val.filename_tmpl = '{:03}.jpeg'\n",
    "# The flag is used to determine whether it is omnisource training\n",
    "cfg.setdefault('omnisource', False)\n",
    "# Modify num classes of the model in cls_head\n",
    "cfg.model.cls_head.num_classes = 6\n",
    "# We can use the pre-trained TSN model\n",
    "cfg.load_from = 'checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth'\n",
    "# cfg.resume_from = './childact-mm/latest.pth'\n",
    "\n",
    "# Set up working dir to save files and logs.\n",
    "cfg.work_dir = './childact-checkpoints/CSN-age-box'\n",
    "\n",
    "cfg.total_epochs = 50\n",
    "\n",
    "# cfg.momentum_config = dict(\n",
    "#     policy='cyclic',\n",
    "#     target_ratio=(0.85 / 0.95, 1),\n",
    "#     cyclic_times=1,\n",
    "#     step_ratio_up=0.4,\n",
    "# )\n",
    "\n",
    "# We can set the checkpoint saving interval to reduce the storage cost\n",
    "cfg.checkpoint_config.interval = 20\n",
    "# We can set the log print interval to reduce the the times of printing log\n",
    "cfg.log_config.interval = 100\n",
    "\n",
    "# Set seed thus the results are more reproducible\n",
    "cfg.seed = 42\n",
    "set_random_seed(42, deterministic=False)\n",
    "cfg.gpu_ids = range(1)\n",
    "\n",
    "cfg.data.videos_per_gpu=4\n",
    "\n",
    "# cfg.model.backbone.in_channels = 2\n",
    "\n",
    "cfg.output_config = dict(out=f'{cfg.work_dir}/results.json')\n",
    "# We can initialize the logger for training and have a look\n",
    "# at the final config used for training\n",
    "# del cfg.optimizer['momentum']\n",
    "print(f'Config:\\n{cfg.pretty_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b2dfc99-ac49-4b23-be8d-379dc9fb4d2b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "dDBWkdDRk6oz",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "85a52ef3-7b5c-4c52-8fef-00322a8c65e6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 08:19:33,116 - mmaction - INFO - load model from: https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth\n",
      "2021-08-10 08:19:33,117 - mmaction - INFO - Use load_from_http loader\n",
      "2021-08-10 08:19:35,230 - mmaction - INFO - load checkpoint from checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth\n",
      "2021-08-10 08:19:35,231 - mmaction - INFO - Use load_from_local loader\n",
      "2021-08-10 08:19:35,407 - mmaction - WARNING - The model and loaded state dict do not match exactly\n",
      "\n",
      "size mismatch for cls_head.fc_cls.weight: copying a param with shape torch.Size([400, 2048]) from checkpoint, the shape in current model is torch.Size([6, 2048]).\n",
      "size mismatch for cls_head.fc_cls.bias: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([6]).\n",
      "2021-08-10 08:19:35,411 - mmaction - INFO - Start running, host: robt427nv@robt427NV, work_dir: /home/robt427nv/childact/childact-checkpoints/CSN-age-box\n",
      "2021-08-10 08:19:35,412 - mmaction - INFO - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(ABOVE_NORMAL) OptimizerHook                      \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "2021-08-10 08:19:35,413 - mmaction - INFO - workflow: [('train', 1)], max: 50 epochs\n",
      "/home/robt427nv/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/mmcv/runner/hooks/evaluation.py:190: UserWarning: runner.meta is None. Creating an empty one.\n",
      "  warnings.warn('runner.meta is None. Creating an empty one.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 11/11, 9.7 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 08:21:45,103 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 08:21:45,105 - mmaction - INFO - \n",
      "top1_acc\t0.2727\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 08:21:45,105 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 08:21:45,107 - mmaction - INFO - \n",
      "mean_acc\t0.2500\n",
      "2021-08-10 08:21:45,432 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_5.pth.\n",
      "2021-08-10 08:21:45,433 - mmaction - INFO - Best top1_acc is 0.2727 at 5 epoch.\n",
      "2021-08-10 08:21:45,433 - mmaction - INFO - Epoch(val) [5][3]\ttop1_acc: 0.2727, top5_acc: 1.0000, mean_class_accuracy: 0.2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 11/11, 9.0 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 08:23:56,335 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 08:23:56,337 - mmaction - INFO - \n",
      "top1_acc\t0.1818\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 08:23:56,337 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 08:23:56,338 - mmaction - INFO - \n",
      "mean_acc\t0.1667\n",
      "2021-08-10 08:23:56,338 - mmaction - INFO - Epoch(val) [10][3]\ttop1_acc: 0.1818, top5_acc: 1.0000, mean_class_accuracy: 0.1667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 11/11, 9.7 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 08:26:01,773 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 08:26:01,775 - mmaction - INFO - \n",
      "top1_acc\t0.2727\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 08:26:01,775 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 08:26:01,777 - mmaction - INFO - \n",
      "mean_acc\t0.2500\n",
      "2021-08-10 08:26:01,777 - mmaction - INFO - Epoch(val) [15][3]\ttop1_acc: 0.2727, top5_acc: 1.0000, mean_class_accuracy: 0.2500\n",
      "2021-08-10 08:28:08,388 - mmaction - INFO - Saving checkpoint at 20 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 11/11, 9.5 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 08:28:09,906 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 08:28:09,907 - mmaction - INFO - \n",
      "top1_acc\t0.1818\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 08:28:09,908 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 08:28:09,909 - mmaction - INFO - \n",
      "mean_acc\t0.1333\n",
      "2021-08-10 08:28:09,909 - mmaction - INFO - Epoch(val) [20][3]\ttop1_acc: 0.1818, top5_acc: 1.0000, mean_class_accuracy: 0.1333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 11/11, 9.1 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 08:30:15,204 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 08:30:15,206 - mmaction - INFO - \n",
      "top1_acc\t0.2727\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 08:30:15,207 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 08:30:15,208 - mmaction - INFO - \n",
      "mean_acc\t0.1833\n",
      "2021-08-10 08:30:15,209 - mmaction - INFO - Epoch(val) [25][3]\ttop1_acc: 0.2727, top5_acc: 1.0000, mean_class_accuracy: 0.1833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 11/11, 9.6 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 08:32:21,612 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 08:32:21,614 - mmaction - INFO - \n",
      "top1_acc\t0.3636\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 08:32:21,615 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 08:32:21,617 - mmaction - INFO - \n",
      "mean_acc\t0.3125\n",
      "2021-08-10 08:32:21,940 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_30.pth.\n",
      "2021-08-10 08:32:21,941 - mmaction - INFO - Best top1_acc is 0.3636 at 30 epoch.\n",
      "2021-08-10 08:32:21,942 - mmaction - INFO - Epoch(val) [30][3]\ttop1_acc: 0.3636, top5_acc: 1.0000, mean_class_accuracy: 0.3125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 11/11, 9.6 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 08:34:28,553 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 08:34:28,554 - mmaction - INFO - \n",
      "top1_acc\t0.2727\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 08:34:28,555 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 08:34:28,555 - mmaction - INFO - \n",
      "mean_acc\t0.1833\n",
      "2021-08-10 08:34:28,556 - mmaction - INFO - Epoch(val) [35][3]\ttop1_acc: 0.2727, top5_acc: 1.0000, mean_class_accuracy: 0.1833\n",
      "2021-08-10 08:36:37,867 - mmaction - INFO - Saving checkpoint at 40 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 11/11, 9.3 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 08:36:39,421 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 08:36:39,423 - mmaction - INFO - \n",
      "top1_acc\t0.2727\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 08:36:39,423 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 08:36:39,424 - mmaction - INFO - \n",
      "mean_acc\t0.1833\n",
      "2021-08-10 08:36:39,426 - mmaction - INFO - Epoch(val) [40][3]\ttop1_acc: 0.2727, top5_acc: 1.0000, mean_class_accuracy: 0.1833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 11/11, 9.1 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 08:38:50,814 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 08:38:50,816 - mmaction - INFO - \n",
      "top1_acc\t0.2727\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 08:38:50,817 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 08:38:50,819 - mmaction - INFO - \n",
      "mean_acc\t0.1833\n",
      "2021-08-10 08:38:50,820 - mmaction - INFO - Epoch(val) [45][3]\ttop1_acc: 0.2727, top5_acc: 1.0000, mean_class_accuracy: 0.1833\n",
      "2021-08-10 08:41:01,028 - mmaction - INFO - Saving checkpoint at 50 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 11/11, 9.5 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 08:41:02,547 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 08:41:02,549 - mmaction - INFO - \n",
      "top1_acc\t0.1818\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 08:41:02,550 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 08:41:02,553 - mmaction - INFO - \n",
      "mean_acc\t0.1333\n",
      "2021-08-10 08:41:02,554 - mmaction - INFO - Epoch(val) [50][3]\ttop1_acc: 0.1818, top5_acc: 1.0000, mean_class_accuracy: 0.1333\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "from mmaction.datasets import build_dataset\n",
    "from mmaction.models import build_model\n",
    "from mmaction.apis import train_model\n",
    "\n",
    "import mmcv\n",
    "\n",
    "# Build the dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# Build the recognizer\n",
    "model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.train_cfg, test_cfg=cfg.test_cfg)\n",
    "# CUDA_LAUNCH_BLOCKING=1\n",
    "# Create work_dir\n",
    "mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
    "train_model(model, datasets, cfg, distributed=False, validate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f5b739c-1b26-40ec-b6d6-74ee4d0b8a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f\"{cfg.work_dir}/model50e\", 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03c849b7-ddd1-4743-97f2-f98f7186016c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eyY3hCMwyTct",
    "outputId": "200e37c7-0da4-421f-98da-41418c3110ca",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 8/8, 0.4 task/s, elapsed: 18s, ETA:     0s\n",
      "Evaluating top_k_accuracy ...\n",
      "\n",
      "top1_acc\t0.0000\n",
      "top5_acc\t0.8750\n",
      "\n",
      "Evaluating mean_class_accuracy ...\n",
      "\n",
      "mean_acc\t0.0000\n",
      "top1_acc: 0.0000\n",
      "top5_acc: 0.8750\n",
      "mean_class_accuracy: 0.0000\n"
     ]
    }
   ],
   "source": [
    "from mmaction.apis import single_gpu_test\n",
    "from mmaction.datasets import build_dataloader\n",
    "from mmcv.parallel import MMDataParallel\n",
    "# from mmaction.models import build_model\n",
    "# from mmaction.datasets import build_dataset\n",
    "\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# Build a test dataloader\n",
    "dataset = build_dataset(cfg.data.test, dict(test_mode=True))\n",
    "data_loader = build_dataloader(\n",
    "        dataset,\n",
    "        videos_per_gpu=1,\n",
    "        workers_per_gpu=2,\n",
    "        dist=False,\n",
    "        shuffle=False)\n",
    "model = MMDataParallel(model, device_ids=[0])\n",
    "outputs = single_gpu_test(model, data_loader)\n",
    "\n",
    "eval_config = cfg.evaluation\n",
    "eval_config.pop('interval')\n",
    "eval_res = dataset.evaluate(outputs, **eval_config)\n",
    "for name, val in eval_res.items():\n",
    "    print(f'{name}: {val:.04f}')\n",
    "    \n",
    "output_config = cfg.output_config\n",
    "dataset.dump_results(outputs, **output_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35c36b81-8229-4d07-80c1-521fc7dae107",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from mmaction.core import confusion_matrix \n",
    "\n",
    "gt_labels = [ann['label'] for ann in dataset.load_annotations()]\n",
    "pred = np.argmax(outputs, axis=1)\n",
    "cf_mat = confusion_matrix(pred, gt_labels).astype(float)\n",
    "cmap = sns.cubehelix_palette(50, hue=0.05, rot=0, light=0.9, dark=0, as_cmap=True)\n",
    "# /np.sum(cf_mat), fmt='.2%',\n",
    "sns.heatmap(cf_mat, cmap=cmap, annot=True, xticklabels = ['6yo', '7yo', '8yo', '9yo', '10yo', '11yo'], yticklabels = ['6yo', '7yo', '8yo', '9yo', '10yo', '11yo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6944e37-59d8-437c-a112-a147ce045619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "mean_error = (1+5+1+1)/8 #number of all tested videos\n",
    "print(mean_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c47bde-6314-4b20-a9f5-2bd279e7868a",
   "metadata": {},
   "source": [
    "# CSN age clap "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "261aa6f1-84a1-457d-95ee-d807eee5ddc4",
   "metadata": {
    "id": "LjCcmCKOjktc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mmcv import Config\n",
    "cfg = Config.fromfile('mmaction2/configs/recognition/csn/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27880464-a44d-4f37-9223-27f0dc2fc8bf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "tlhu9byjjt-K",
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "outputId": "13d1bb01-f351-48c0-9efb-0bdf2c125d29",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "model = dict(\n",
      "    type='Recognizer3D',\n",
      "    backbone=dict(\n",
      "        type='ResNet3dCSN',\n",
      "        pretrained2d=False,\n",
      "        pretrained=\n",
      "        'https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth',\n",
      "        depth=152,\n",
      "        with_pool2=False,\n",
      "        bottleneck_mode='ir',\n",
      "        norm_eval=True,\n",
      "        zero_init_residual=False,\n",
      "        bn_frozen=True),\n",
      "    cls_head=dict(\n",
      "        type='I3DHead',\n",
      "        num_classes=6,\n",
      "        in_channels=2048,\n",
      "        spatial_type='avg',\n",
      "        dropout_ratio=0.5,\n",
      "        init_std=0.01),\n",
      "    train_cfg=None,\n",
      "    test_cfg=dict(average_clips='prob'))\n",
      "checkpoint_config = dict(interval=20)\n",
      "log_config = dict(interval=100, hooks=[dict(type='TextLoggerHook')])\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = 'checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth'\n",
      "resume_from = None\n",
      "workflow = [('train', 1)]\n",
      "dataset_type = 'RawframeDataset'\n",
      "data_root = 'age-gender-3split-rgb-frames/'\n",
      "data_root_val = 'age-gender-3split-rgb-frames/val/'\n",
      "ann_file_train = 'age-gender-3split-rgb-frames/childact_train_rgb320_age_clap.txt'\n",
      "ann_file_val = 'age-gender-3split-rgb-frames/childact_val_rgb320_age_clap.txt'\n",
      "ann_file_test = 'age-gender-3split-rgb-frames/childact_test_rgb320_age_clap.txt'\n",
      "img_norm_cfg = dict(\n",
      "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_bgr=False)\n",
      "train_pipeline = [\n",
      "    dict(type='SampleFrames', clip_len=32, frame_interval=2, num_clips=1),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='RandomResizedCrop'),\n",
      "    dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "    dict(type='Flip', flip_ratio=0.5),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "]\n",
      "val_pipeline = [\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=32,\n",
      "        frame_interval=2,\n",
      "        num_clips=1,\n",
      "        test_mode=True),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='CenterCrop', crop_size=224),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=32,\n",
      "        frame_interval=2,\n",
      "        num_clips=10,\n",
      "        test_mode=True),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='ThreeCrop', crop_size=256),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "data = dict(\n",
      "    videos_per_gpu=4,\n",
      "    workers_per_gpu=4,\n",
      "    train=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_train_rgb320_age_clap.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/train/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=1),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='RandomResizedCrop'),\n",
      "            dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "            dict(type='Flip', flip_ratio=0.5),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'),\n",
      "    val=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_val_rgb320_age_clap.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/val/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=1,\n",
      "                test_mode=True),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='CenterCrop', crop_size=224),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'),\n",
      "    test=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_test_rgb320_age_clap.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/test/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=10,\n",
      "                test_mode=True),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='ThreeCrop', crop_size=256),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'))\n",
      "evaluation = dict(\n",
      "    interval=5, metrics=['top_k_accuracy', 'mean_class_accuracy'])\n",
      "optimizer = dict(type='SGD', lr=0.000125, momentum=0.9, weight_decay=0.0001)\n",
      "optimizer_config = dict(grad_clip=dict(max_norm=40, norm_type=2))\n",
      "lr_config = dict(\n",
      "    policy='step',\n",
      "    step=[32, 48],\n",
      "    warmup='linear',\n",
      "    warmup_ratio=0.1,\n",
      "    warmup_by_epoch=True,\n",
      "    warmup_iters=16)\n",
      "total_epochs = 50\n",
      "work_dir = './childact-checkpoints/CSN-age-box'\n",
      "find_unused_parameters = True\n",
      "omnisource = False\n",
      "seed = 42\n",
      "gpu_ids = range(0, 1)\n",
      "output_config = dict(out='./childact-checkpoints/CSN-age-box/results.json')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mmcv.runner import set_random_seed\n",
    "\n",
    "# Modify dataset type and path\n",
    "# cfg.dataset_type = 'VideoDataset'\n",
    "cfg.data_root = 'age-gender-3split-rgb-frames/'\n",
    "cfg.data_root_val = 'age-gender-3split-rgb-frames/val/'\n",
    "cfg.ann_file_train = 'age-gender-3split-rgb-frames/childact_train_rgb320_age_clap.txt'\n",
    "cfg.ann_file_val = 'age-gender-3split-rgb-frames/childact_val_rgb320_age_clap.txt'\n",
    "cfg.ann_file_test = 'age-gender-3split-rgb-frames/childact_test_rgb320_age_clap.txt'\n",
    "\n",
    "# cfg.data.test.type = 'VideoDataset'\n",
    "cfg.data.test.ann_file = 'age-gender-3split-rgb-frames/childact_test_rgb320_age_clap.txt'\n",
    "cfg.data.test.data_prefix = 'age-gender-3split-rgb-frames/test/'\n",
    "\n",
    "# cfg.data.train.type = 'VideoDataset'\n",
    "cfg.data.train.ann_file = 'age-gender-3split-rgb-frames/childact_train_rgb320_age_clap.txt'\n",
    "cfg.data.train.data_prefix = 'age-gender-3split-rgb-frames/train/'\n",
    "\n",
    "# cfg.data.val.type = 'VideoDataset'\n",
    "cfg.data.val.ann_file = 'age-gender-3split-rgb-frames/childact_val_rgb320_age_clap.txt'\n",
    "cfg.data.val.data_prefix = 'age-gender-3split-rgb-frames/val/'\n",
    "\n",
    "# cfg.data.test.modality = 'Flow'\n",
    "# cfg.data.val.modality = 'Flow'\n",
    "# cfg.data.train.modality = 'Flow'\n",
    "\n",
    "# cfg.data.train.start_index = 0\n",
    "# cfg.data.test.start_index = 0\n",
    "# cfg.data.val.start_index = 0\n",
    "\n",
    "cfg.data.test.filename_tmpl = '{:03}.jpeg'\n",
    "cfg.data.train.filename_tmpl = '{:03}.jpeg'\n",
    "cfg.data.val.filename_tmpl = '{:03}.jpeg'\n",
    "# The flag is used to determine whether it is omnisource training\n",
    "cfg.setdefault('omnisource', False)\n",
    "# Modify num classes of the model in cls_head\n",
    "cfg.model.cls_head.num_classes = 6\n",
    "# We can use the pre-trained TSN model\n",
    "cfg.load_from = 'checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth'\n",
    "# cfg.resume_from = './childact-mm/latest.pth'\n",
    "\n",
    "# Set up working dir to save files and logs.\n",
    "cfg.work_dir = './childact-checkpoints/CSN-age-box'\n",
    "\n",
    "cfg.total_epochs = 50\n",
    "\n",
    "# cfg.momentum_config = dict(\n",
    "#     policy='cyclic',\n",
    "#     target_ratio=(0.85 / 0.95, 1),\n",
    "#     cyclic_times=1,\n",
    "#     step_ratio_up=0.4,\n",
    "# )\n",
    "\n",
    "# We can set the checkpoint saving interval to reduce the storage cost\n",
    "cfg.checkpoint_config.interval = 20\n",
    "# We can set the log print interval to reduce the the times of printing log\n",
    "cfg.log_config.interval = 100\n",
    "\n",
    "# Set seed thus the results are more reproducible\n",
    "cfg.seed = 42\n",
    "set_random_seed(42, deterministic=False)\n",
    "cfg.gpu_ids = range(1)\n",
    "\n",
    "cfg.data.videos_per_gpu=4\n",
    "\n",
    "# cfg.model.backbone.in_channels = 2\n",
    "\n",
    "cfg.output_config = dict(out=f'{cfg.work_dir}/results.json')\n",
    "# We can initialize the logger for training and have a look\n",
    "# at the final config used for training\n",
    "# del cfg.optimizer['momentum']\n",
    "print(f'Config:\\n{cfg.pretty_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd758729-843c-4476-b478-97d77772b037",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "dDBWkdDRk6oz",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "85a52ef3-7b5c-4c52-8fef-00322a8c65e6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 08:52:09,598 - mmaction - INFO - load model from: https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth\n",
      "2021-08-10 08:52:09,599 - mmaction - INFO - Use load_from_http loader\n",
      "2021-08-10 08:52:11,641 - mmaction - INFO - load checkpoint from checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth\n",
      "2021-08-10 08:52:11,642 - mmaction - INFO - Use load_from_local loader\n",
      "2021-08-10 08:52:11,809 - mmaction - WARNING - The model and loaded state dict do not match exactly\n",
      "\n",
      "size mismatch for cls_head.fc_cls.weight: copying a param with shape torch.Size([400, 2048]) from checkpoint, the shape in current model is torch.Size([6, 2048]).\n",
      "size mismatch for cls_head.fc_cls.bias: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([6]).\n",
      "2021-08-10 08:52:11,815 - mmaction - INFO - Start running, host: robt427nv@robt427NV, work_dir: /home/robt427nv/childact/childact-checkpoints/CSN-age-box\n",
      "2021-08-10 08:52:11,816 - mmaction - INFO - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(ABOVE_NORMAL) OptimizerHook                      \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "2021-08-10 08:52:11,816 - mmaction - INFO - workflow: [('train', 1)], max: 50 epochs\n",
      "/home/robt427nv/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/mmcv/runner/hooks/evaluation.py:190: UserWarning: runner.meta is None. Creating an empty one.\n",
      "  warnings.warn('runner.meta is None. Creating an empty one.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 14/14, 9.8 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 08:54:38,069 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 08:54:38,070 - mmaction - INFO - \n",
      "top1_acc\t0.2143\n",
      "top5_acc\t0.9286\n",
      "2021-08-10 08:54:38,071 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 08:54:38,073 - mmaction - INFO - \n",
      "mean_acc\t0.1905\n",
      "2021-08-10 08:54:38,394 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_5.pth.\n",
      "2021-08-10 08:54:38,396 - mmaction - INFO - Best top1_acc is 0.2143 at 5 epoch.\n",
      "2021-08-10 08:54:38,396 - mmaction - INFO - Epoch(val) [5][4]\ttop1_acc: 0.2143, top5_acc: 0.9286, mean_class_accuracy: 0.1905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 14/14, 9.8 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 08:57:05,509 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 08:57:05,511 - mmaction - INFO - \n",
      "top1_acc\t0.5000\n",
      "top5_acc\t0.9286\n",
      "2021-08-10 08:57:05,511 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 08:57:05,512 - mmaction - INFO - \n",
      "mean_acc\t0.1667\n",
      "2021-08-10 08:57:05,849 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_10.pth.\n",
      "2021-08-10 08:57:05,850 - mmaction - INFO - Best top1_acc is 0.5000 at 10 epoch.\n",
      "2021-08-10 08:57:05,851 - mmaction - INFO - Epoch(val) [10][4]\ttop1_acc: 0.5000, top5_acc: 0.9286, mean_class_accuracy: 0.1667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 14/14, 10.0 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 08:59:33,436 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 08:59:33,438 - mmaction - INFO - \n",
      "top1_acc\t0.4286\n",
      "top5_acc\t0.9286\n",
      "2021-08-10 08:59:33,439 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 08:59:33,441 - mmaction - INFO - \n",
      "mean_acc\t0.2857\n",
      "2021-08-10 08:59:33,442 - mmaction - INFO - Epoch(val) [15][4]\ttop1_acc: 0.4286, top5_acc: 0.9286, mean_class_accuracy: 0.2857\n",
      "2021-08-10 09:01:59,151 - mmaction - INFO - Saving checkpoint at 20 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 14/14, 10.5 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 09:02:00,960 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 09:02:00,962 - mmaction - INFO - \n",
      "top1_acc\t0.5000\n",
      "top5_acc\t0.8571\n",
      "2021-08-10 09:02:00,963 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 09:02:00,964 - mmaction - INFO - \n",
      "mean_acc\t0.4286\n",
      "2021-08-10 09:02:00,965 - mmaction - INFO - Epoch(val) [20][4]\ttop1_acc: 0.5000, top5_acc: 0.8571, mean_class_accuracy: 0.4286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 14/14, 10.6 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 09:04:28,396 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 09:04:28,398 - mmaction - INFO - \n",
      "top1_acc\t0.3571\n",
      "top5_acc\t0.9286\n",
      "2021-08-10 09:04:28,398 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 09:04:28,399 - mmaction - INFO - \n",
      "mean_acc\t0.3214\n",
      "2021-08-10 09:04:28,399 - mmaction - INFO - Epoch(val) [25][4]\ttop1_acc: 0.3571, top5_acc: 0.9286, mean_class_accuracy: 0.3214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 14/14, 10.2 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 09:06:55,330 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 09:06:55,331 - mmaction - INFO - \n",
      "top1_acc\t0.3571\n",
      "top5_acc\t0.9286\n",
      "2021-08-10 09:06:55,332 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 09:06:55,333 - mmaction - INFO - \n",
      "mean_acc\t0.3810\n",
      "2021-08-10 09:06:55,334 - mmaction - INFO - Epoch(val) [30][4]\ttop1_acc: 0.3571, top5_acc: 0.9286, mean_class_accuracy: 0.3810\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 14/14, 10.1 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 09:09:21,996 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 09:09:21,998 - mmaction - INFO - \n",
      "top1_acc\t0.3571\n",
      "top5_acc\t0.9286\n",
      "2021-08-10 09:09:21,998 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 09:09:22,000 - mmaction - INFO - \n",
      "mean_acc\t0.3214\n",
      "2021-08-10 09:09:22,001 - mmaction - INFO - Epoch(val) [35][4]\ttop1_acc: 0.3571, top5_acc: 0.9286, mean_class_accuracy: 0.3214\n",
      "2021-08-10 09:11:48,431 - mmaction - INFO - Saving checkpoint at 40 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 14/14, 10.0 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 09:11:50,333 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 09:11:50,334 - mmaction - INFO - \n",
      "top1_acc\t0.2857\n",
      "top5_acc\t0.9286\n",
      "2021-08-10 09:11:50,335 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 09:11:50,336 - mmaction - INFO - \n",
      "mean_acc\t0.2976\n",
      "2021-08-10 09:11:50,336 - mmaction - INFO - Epoch(val) [40][4]\ttop1_acc: 0.2857, top5_acc: 0.9286, mean_class_accuracy: 0.2976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 14/14, 10.1 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 09:14:17,482 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 09:14:17,483 - mmaction - INFO - \n",
      "top1_acc\t0.3571\n",
      "top5_acc\t0.9286\n",
      "2021-08-10 09:14:17,483 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 09:14:17,484 - mmaction - INFO - \n",
      "mean_acc\t0.3810\n",
      "2021-08-10 09:14:17,484 - mmaction - INFO - Epoch(val) [45][4]\ttop1_acc: 0.3571, top5_acc: 0.9286, mean_class_accuracy: 0.3810\n",
      "2021-08-10 09:16:43,161 - mmaction - INFO - Saving checkpoint at 50 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 14/14, 10.0 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 09:16:45,035 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 09:16:45,037 - mmaction - INFO - \n",
      "top1_acc\t0.3571\n",
      "top5_acc\t0.9286\n",
      "2021-08-10 09:16:45,037 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 09:16:45,038 - mmaction - INFO - \n",
      "mean_acc\t0.3810\n",
      "2021-08-10 09:16:45,039 - mmaction - INFO - Epoch(val) [50][4]\ttop1_acc: 0.3571, top5_acc: 0.9286, mean_class_accuracy: 0.3810\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "from mmaction.datasets import build_dataset\n",
    "from mmaction.models import build_model\n",
    "from mmaction.apis import train_model\n",
    "\n",
    "import mmcv\n",
    "\n",
    "# Build the dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# Build the recognizer\n",
    "model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.train_cfg, test_cfg=cfg.test_cfg)\n",
    "# CUDA_LAUNCH_BLOCKING=1\n",
    "# Create work_dir\n",
    "mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
    "train_model(model, datasets, cfg, distributed=False, validate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e16f52ee-f73c-439a-b579-637a777dcf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f\"{cfg.work_dir}/model50e\", 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "293f078b-0dd0-4912-82a8-7a3bfdd1ea91",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eyY3hCMwyTct",
    "outputId": "200e37c7-0da4-421f-98da-41418c3110ca",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 18/18, 0.6 task/s, elapsed: 29s, ETA:     0s\n",
      "Evaluating top_k_accuracy ...\n",
      "\n",
      "top1_acc\t0.5000\n",
      "top5_acc\t0.9444\n",
      "\n",
      "Evaluating mean_class_accuracy ...\n",
      "\n",
      "mean_acc\t0.2569\n",
      "top1_acc: 0.5000\n",
      "top5_acc: 0.9444\n",
      "mean_class_accuracy: 0.2569\n"
     ]
    }
   ],
   "source": [
    "from mmaction.apis import single_gpu_test\n",
    "from mmaction.datasets import build_dataloader\n",
    "from mmcv.parallel import MMDataParallel\n",
    "# from mmaction.models import build_model\n",
    "# from mmaction.datasets import build_dataset\n",
    "\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# Build a test dataloader\n",
    "dataset = build_dataset(cfg.data.test, dict(test_mode=True))\n",
    "data_loader = build_dataloader(\n",
    "        dataset,\n",
    "        videos_per_gpu=1,\n",
    "        workers_per_gpu=2,\n",
    "        dist=False,\n",
    "        shuffle=False)\n",
    "model = MMDataParallel(model, device_ids=[0])\n",
    "outputs = single_gpu_test(model, data_loader)\n",
    "\n",
    "eval_config = cfg.evaluation\n",
    "eval_config.pop('interval')\n",
    "eval_res = dataset.evaluate(outputs, **eval_config)\n",
    "for name, val in eval_res.items():\n",
    "    print(f'{name}: {val:.04f}')\n",
    "    \n",
    "output_config = cfg.output_config\n",
    "dataset.dump_results(outputs, **output_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f5f6358-f472-459d-8b27-7b51bb8e18ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from mmaction.core import confusion_matrix \n",
    "\n",
    "gt_labels = [ann['label'] for ann in dataset.load_annotations()]\n",
    "pred = np.argmax(outputs, axis=1)\n",
    "cf_mat = confusion_matrix(pred, gt_labels).astype(float)\n",
    "cmap = sns.cubehelix_palette(50, hue=0.05, rot=0, light=0.9, dark=0, as_cmap=True)\n",
    "# /np.sum(cf_mat), fmt='.2%',\n",
    "sns.heatmap(cf_mat, cmap=cmap, annot=True, xticklabels = ['6yo', '7yo', '8yo', '9yo', '10yo', '11yo'], yticklabels = ['6yo', '7yo', '8yo', '9yo', '10yo', '11yo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65a7f8bc-5787-4ea2-88c0-f8f35ae396bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "mean_error = (1+1+1+1+3+1+1)/18 #number of all tested videos\n",
    "print(mean_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0d3c5c-7c01-473b-8c2e-1fe84570c1ef",
   "metadata": {},
   "source": [
    "# CSN age go "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9532f2b7-9f58-44ce-9119-6f4ec27a7457",
   "metadata": {
    "id": "LjCcmCKOjktc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mmcv import Config\n",
    "cfg = Config.fromfile('mmaction2/configs/recognition/csn/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a49c91d-d2a5-44dd-96ed-e38f1616e463",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "tlhu9byjjt-K",
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "outputId": "13d1bb01-f351-48c0-9efb-0bdf2c125d29",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "model = dict(\n",
      "    type='Recognizer3D',\n",
      "    backbone=dict(\n",
      "        type='ResNet3dCSN',\n",
      "        pretrained2d=False,\n",
      "        pretrained=\n",
      "        'https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth',\n",
      "        depth=152,\n",
      "        with_pool2=False,\n",
      "        bottleneck_mode='ir',\n",
      "        norm_eval=True,\n",
      "        zero_init_residual=False,\n",
      "        bn_frozen=True),\n",
      "    cls_head=dict(\n",
      "        type='I3DHead',\n",
      "        num_classes=6,\n",
      "        in_channels=2048,\n",
      "        spatial_type='avg',\n",
      "        dropout_ratio=0.5,\n",
      "        init_std=0.01),\n",
      "    train_cfg=None,\n",
      "    test_cfg=dict(average_clips='prob'))\n",
      "checkpoint_config = dict(interval=20)\n",
      "log_config = dict(interval=100, hooks=[dict(type='TextLoggerHook')])\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = 'checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth'\n",
      "resume_from = None\n",
      "workflow = [('train', 1)]\n",
      "dataset_type = 'RawframeDataset'\n",
      "data_root = 'age-gender-3split-rgb-frames/'\n",
      "data_root_val = 'age-gender-3split-rgb-frames/val/'\n",
      "ann_file_train = 'age-gender-3split-rgb-frames/childact_train_rgb320_age_go.txt'\n",
      "ann_file_val = 'age-gender-3split-rgb-frames/childact_val_rgb320_age_go.txt'\n",
      "ann_file_test = 'age-gender-3split-rgb-frames/childact_test_rgb320_age_go.txt'\n",
      "img_norm_cfg = dict(\n",
      "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_bgr=False)\n",
      "train_pipeline = [\n",
      "    dict(type='SampleFrames', clip_len=32, frame_interval=2, num_clips=1),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='RandomResizedCrop'),\n",
      "    dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "    dict(type='Flip', flip_ratio=0.5),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "]\n",
      "val_pipeline = [\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=32,\n",
      "        frame_interval=2,\n",
      "        num_clips=1,\n",
      "        test_mode=True),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='CenterCrop', crop_size=224),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=32,\n",
      "        frame_interval=2,\n",
      "        num_clips=10,\n",
      "        test_mode=True),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='ThreeCrop', crop_size=256),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "data = dict(\n",
      "    videos_per_gpu=4,\n",
      "    workers_per_gpu=4,\n",
      "    train=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_train_rgb320_age_go.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/train/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=1),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='RandomResizedCrop'),\n",
      "            dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "            dict(type='Flip', flip_ratio=0.5),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'),\n",
      "    val=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file='age-gender-3split-rgb-frames/childact_val_rgb320_age_go.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/val/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=1,\n",
      "                test_mode=True),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='CenterCrop', crop_size=224),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'),\n",
      "    test=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file='age-gender-3split-rgb-frames/childact_test_rgb320_age_go.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/test/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=10,\n",
      "                test_mode=True),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='ThreeCrop', crop_size=256),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'))\n",
      "evaluation = dict(\n",
      "    interval=5, metrics=['top_k_accuracy', 'mean_class_accuracy'])\n",
      "optimizer = dict(type='SGD', lr=0.000125, momentum=0.9, weight_decay=0.0001)\n",
      "optimizer_config = dict(grad_clip=dict(max_norm=40, norm_type=2))\n",
      "lr_config = dict(\n",
      "    policy='step',\n",
      "    step=[32, 48],\n",
      "    warmup='linear',\n",
      "    warmup_ratio=0.1,\n",
      "    warmup_by_epoch=True,\n",
      "    warmup_iters=16)\n",
      "total_epochs = 50\n",
      "work_dir = './childact-checkpoints/CSN-age-go'\n",
      "find_unused_parameters = True\n",
      "omnisource = False\n",
      "seed = 42\n",
      "gpu_ids = range(0, 1)\n",
      "output_config = dict(out='./childact-checkpoints/CSN-age-go/results.json')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mmcv.runner import set_random_seed\n",
    "\n",
    "# Modify dataset type and path\n",
    "# cfg.dataset_type = 'VideoDataset'\n",
    "cfg.data_root = 'age-gender-3split-rgb-frames/'\n",
    "cfg.data_root_val = 'age-gender-3split-rgb-frames/val/'\n",
    "cfg.ann_file_train = 'age-gender-3split-rgb-frames/childact_train_rgb320_age_go.txt'\n",
    "cfg.ann_file_val = 'age-gender-3split-rgb-frames/childact_val_rgb320_age_go.txt'\n",
    "cfg.ann_file_test = 'age-gender-3split-rgb-frames/childact_test_rgb320_age_go.txt'\n",
    "\n",
    "# cfg.data.test.type = 'VideoDataset'\n",
    "cfg.data.test.ann_file = 'age-gender-3split-rgb-frames/childact_test_rgb320_age_go.txt'\n",
    "cfg.data.test.data_prefix = 'age-gender-3split-rgb-frames/test/'\n",
    "\n",
    "# cfg.data.train.type = 'VideoDataset'\n",
    "cfg.data.train.ann_file = 'age-gender-3split-rgb-frames/childact_train_rgb320_age_go.txt'\n",
    "cfg.data.train.data_prefix = 'age-gender-3split-rgb-frames/train/'\n",
    "\n",
    "# cfg.data.val.type = 'VideoDataset'\n",
    "cfg.data.val.ann_file = 'age-gender-3split-rgb-frames/childact_val_rgb320_age_go.txt'\n",
    "cfg.data.val.data_prefix = 'age-gender-3split-rgb-frames/val/'\n",
    "\n",
    "# cfg.data.test.modality = 'Flow'\n",
    "# cfg.data.val.modality = 'Flow'\n",
    "# cfg.data.train.modality = 'Flow'\n",
    "\n",
    "# cfg.data.train.start_index = 0\n",
    "# cfg.data.test.start_index = 0\n",
    "# cfg.data.val.start_index = 0\n",
    "\n",
    "cfg.data.test.filename_tmpl = '{:03}.jpeg'\n",
    "cfg.data.train.filename_tmpl = '{:03}.jpeg'\n",
    "cfg.data.val.filename_tmpl = '{:03}.jpeg'\n",
    "# The flag is used to determine whether it is omnisource training\n",
    "cfg.setdefault('omnisource', False)\n",
    "# Modify num classes of the model in cls_head\n",
    "cfg.model.cls_head.num_classes = 6\n",
    "# We can use the pre-trained TSN model\n",
    "cfg.load_from = 'checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth'\n",
    "# cfg.resume_from = './childact-mm/latest.pth'\n",
    "\n",
    "# Set up working dir to save files and logs.\n",
    "cfg.work_dir = './childact-checkpoints/CSN-age-go'\n",
    "\n",
    "cfg.total_epochs = 50\n",
    "\n",
    "# cfg.momentum_config = dict(\n",
    "#     policy='cyclic',\n",
    "#     target_ratio=(0.85 / 0.95, 1),\n",
    "#     cyclic_times=1,\n",
    "#     step_ratio_up=0.4,\n",
    "# )\n",
    "\n",
    "# We can set the checkpoint saving interval to reduce the storage cost\n",
    "cfg.checkpoint_config.interval = 20\n",
    "# We can set the log print interval to reduce the the times of printing log\n",
    "cfg.log_config.interval = 100\n",
    "\n",
    "# Set seed thus the results are more reproducible\n",
    "cfg.seed = 42\n",
    "set_random_seed(42, deterministic=False)\n",
    "cfg.gpu_ids = range(1)\n",
    "\n",
    "cfg.data.videos_per_gpu=4\n",
    "\n",
    "# cfg.model.backbone.in_channels = 2\n",
    "\n",
    "cfg.output_config = dict(out=f'{cfg.work_dir}/results.json')\n",
    "# We can initialize the logger for training and have a look\n",
    "# at the final config used for training\n",
    "# del cfg.optimizer['momentum']\n",
    "print(f'Config:\\n{cfg.pretty_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15612c13-8dea-4515-b084-43d185f78fe7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "dDBWkdDRk6oz",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "85a52ef3-7b5c-4c52-8fef-00322a8c65e6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 09:40:44,136 - mmaction - INFO - load model from: https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth\n",
      "2021-08-10 09:40:44,137 - mmaction - INFO - Use load_from_http loader\n",
      "2021-08-10 09:40:44,381 - mmaction - INFO - load checkpoint from checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth\n",
      "2021-08-10 09:40:44,382 - mmaction - INFO - Use load_from_local loader\n",
      "2021-08-10 09:40:44,546 - mmaction - WARNING - The model and loaded state dict do not match exactly\n",
      "\n",
      "size mismatch for cls_head.fc_cls.weight: copying a param with shape torch.Size([400, 2048]) from checkpoint, the shape in current model is torch.Size([6, 2048]).\n",
      "size mismatch for cls_head.fc_cls.bias: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([6]).\n",
      "2021-08-10 09:40:44,549 - mmaction - INFO - Start running, host: robt427nv@robt427NV, work_dir: /home/robt427nv/childact/childact-checkpoints/CSN-age-go\n",
      "2021-08-10 09:40:44,551 - mmaction - INFO - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(ABOVE_NORMAL) OptimizerHook                      \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "2021-08-10 09:40:44,552 - mmaction - INFO - workflow: [('train', 1)], max: 50 epochs\n",
      "/home/robt427nv/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/mmcv/runner/hooks/evaluation.py:190: UserWarning: runner.meta is None. Creating an empty one.\n",
      "  warnings.warn('runner.meta is None. Creating an empty one.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 18/18, 10.1 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 09:42:50,544 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 09:42:50,545 - mmaction - INFO - \n",
      "top1_acc\t0.3889\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 09:42:50,546 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 09:42:50,547 - mmaction - INFO - \n",
      "mean_acc\t0.2500\n",
      "2021-08-10 09:42:50,846 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_5.pth.\n",
      "2021-08-10 09:42:50,847 - mmaction - INFO - Best top1_acc is 0.3889 at 5 epoch.\n",
      "2021-08-10 09:42:50,848 - mmaction - INFO - Epoch(val) [5][5]\ttop1_acc: 0.3889, top5_acc: 1.0000, mean_class_accuracy: 0.2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 18/18, 10.3 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 09:44:57,985 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 09:44:57,988 - mmaction - INFO - \n",
      "top1_acc\t0.5000\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 09:44:57,988 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 09:44:57,990 - mmaction - INFO - \n",
      "mean_acc\t0.3000\n",
      "2021-08-10 09:44:58,325 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_10.pth.\n",
      "2021-08-10 09:44:58,326 - mmaction - INFO - Best top1_acc is 0.5000 at 10 epoch.\n",
      "2021-08-10 09:44:58,327 - mmaction - INFO - Epoch(val) [10][5]\ttop1_acc: 0.5000, top5_acc: 1.0000, mean_class_accuracy: 0.3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 18/18, 10.7 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 09:47:05,944 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 09:47:05,946 - mmaction - INFO - \n",
      "top1_acc\t0.6111\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 09:47:05,946 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 09:47:05,948 - mmaction - INFO - \n",
      "mean_acc\t0.4550\n",
      "2021-08-10 09:47:06,291 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_15.pth.\n",
      "2021-08-10 09:47:06,292 - mmaction - INFO - Best top1_acc is 0.6111 at 15 epoch.\n",
      "2021-08-10 09:47:06,293 - mmaction - INFO - Epoch(val) [15][5]\ttop1_acc: 0.6111, top5_acc: 1.0000, mean_class_accuracy: 0.4550\n",
      "2021-08-10 09:49:12,101 - mmaction - INFO - Saving checkpoint at 20 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 18/18, 10.9 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 09:49:14,144 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 09:49:14,146 - mmaction - INFO - \n",
      "top1_acc\t0.4444\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 09:49:14,146 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 09:49:14,147 - mmaction - INFO - \n",
      "mean_acc\t0.2300\n",
      "2021-08-10 09:49:14,147 - mmaction - INFO - Epoch(val) [20][5]\ttop1_acc: 0.4444, top5_acc: 1.0000, mean_class_accuracy: 0.2300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 18/18, 10.3 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 09:51:21,538 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 09:51:21,539 - mmaction - INFO - \n",
      "top1_acc\t0.6667\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 09:51:21,539 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 09:51:21,540 - mmaction - INFO - \n",
      "mean_acc\t0.5100\n",
      "2021-08-10 09:51:21,858 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_25.pth.\n",
      "2021-08-10 09:51:21,859 - mmaction - INFO - Best top1_acc is 0.6667 at 25 epoch.\n",
      "2021-08-10 09:51:21,860 - mmaction - INFO - Epoch(val) [25][5]\ttop1_acc: 0.6667, top5_acc: 1.0000, mean_class_accuracy: 0.5100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 18/18, 10.7 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 09:53:29,780 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 09:53:29,782 - mmaction - INFO - \n",
      "top1_acc\t0.3333\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 09:53:29,783 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 09:53:29,784 - mmaction - INFO - \n",
      "mean_acc\t0.2250\n",
      "2021-08-10 09:53:29,785 - mmaction - INFO - Epoch(val) [30][5]\ttop1_acc: 0.3333, top5_acc: 1.0000, mean_class_accuracy: 0.2250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 18/18, 10.5 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 09:55:37,088 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 09:55:37,090 - mmaction - INFO - \n",
      "top1_acc\t0.6667\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 09:55:37,090 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 09:55:37,091 - mmaction - INFO - \n",
      "mean_acc\t0.6450\n",
      "2021-08-10 09:55:37,092 - mmaction - INFO - Epoch(val) [35][5]\ttop1_acc: 0.6667, top5_acc: 1.0000, mean_class_accuracy: 0.6450\n",
      "2021-08-10 09:57:42,632 - mmaction - INFO - Saving checkpoint at 40 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 18/18, 10.6 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 09:57:44,741 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 09:57:44,742 - mmaction - INFO - \n",
      "top1_acc\t0.6667\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 09:57:44,743 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 09:57:44,744 - mmaction - INFO - \n",
      "mean_acc\t0.6450\n",
      "2021-08-10 09:57:44,745 - mmaction - INFO - Epoch(val) [40][5]\ttop1_acc: 0.6667, top5_acc: 1.0000, mean_class_accuracy: 0.6450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 18/18, 10.6 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 09:59:51,790 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 09:59:51,792 - mmaction - INFO - \n",
      "top1_acc\t0.6667\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 09:59:51,793 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 09:59:51,794 - mmaction - INFO - \n",
      "mean_acc\t0.6450\n",
      "2021-08-10 09:59:51,795 - mmaction - INFO - Epoch(val) [45][5]\ttop1_acc: 0.6667, top5_acc: 1.0000, mean_class_accuracy: 0.6450\n",
      "2021-08-10 10:01:57,274 - mmaction - INFO - Saving checkpoint at 50 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 18/18, 11.0 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 10:01:59,299 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 10:01:59,301 - mmaction - INFO - \n",
      "top1_acc\t0.6667\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 10:01:59,302 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 10:01:59,303 - mmaction - INFO - \n",
      "mean_acc\t0.5850\n",
      "2021-08-10 10:01:59,304 - mmaction - INFO - Epoch(val) [50][5]\ttop1_acc: 0.6667, top5_acc: 1.0000, mean_class_accuracy: 0.5850\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "from mmaction.datasets import build_dataset\n",
    "from mmaction.models import build_model\n",
    "from mmaction.apis import train_model\n",
    "\n",
    "import mmcv\n",
    "\n",
    "# Build the dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# Build the recognizer\n",
    "model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.train_cfg, test_cfg=cfg.test_cfg)\n",
    "# CUDA_LAUNCH_BLOCKING=1\n",
    "# Create work_dir\n",
    "mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
    "train_model(model, datasets, cfg, distributed=False, validate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3fa717db-1fa7-4a43-abe7-3da42cd8d3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f\"{cfg.work_dir}/model50e\", 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22098c4d-b3a1-4c46-abad-e4ef9f36332c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 10:10:19,285 - mmaction - INFO - load model from: https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth\n",
      "2021-08-10 10:10:19,286 - mmaction - INFO - Use load_from_http loader\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "from mmaction.datasets import build_dataset\n",
    "from mmaction.models import build_model\n",
    "from mmaction.apis import train_model\n",
    "import pickle\n",
    "import mmcv\n",
    "# Build the dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# Build the recognizer\n",
    "model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "\n",
    "model = pickle.load(open(f\"{cfg.work_dir}/model50e\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d475d2b-9e9b-4016-ac3a-511eaf41e2c4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eyY3hCMwyTct",
    "outputId": "200e37c7-0da4-421f-98da-41418c3110ca",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 22/22, 0.7 task/s, elapsed: 33s, ETA:     0s\n",
      "Evaluating top_k_accuracy ...\n",
      "\n",
      "top1_acc\t0.5455\n",
      "top5_acc\t0.9091\n",
      "\n",
      "Evaluating mean_class_accuracy ...\n",
      "\n",
      "mean_acc\t0.3847\n",
      "top1_acc: 0.5455\n",
      "top5_acc: 0.9091\n",
      "mean_class_accuracy: 0.3847\n"
     ]
    }
   ],
   "source": [
    "from mmaction.apis import single_gpu_test\n",
    "from mmaction.datasets import build_dataloader\n",
    "from mmcv.parallel import MMDataParallel\n",
    "# from mmaction.models import build_model\n",
    "# from mmaction.datasets import build_dataset\n",
    "\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# Build a test dataloader\n",
    "dataset = build_dataset(cfg.data.test, dict(test_mode=True))\n",
    "data_loader = build_dataloader(\n",
    "        dataset,\n",
    "        videos_per_gpu=1,\n",
    "        workers_per_gpu=2,\n",
    "        dist=False,\n",
    "        shuffle=False)\n",
    "model = MMDataParallel(model, device_ids=[0])\n",
    "outputs = single_gpu_test(model, data_loader)\n",
    "\n",
    "eval_config = cfg.evaluation\n",
    "eval_config.pop('interval')\n",
    "eval_res = dataset.evaluate(outputs, **eval_config)\n",
    "for name, val in eval_res.items():\n",
    "    print(f'{name}: {val:.04f}')\n",
    "    \n",
    "output_config = cfg.output_config\n",
    "dataset.dump_results(outputs, **output_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0bd7db4-ae6b-4f08-8164-bfef79f6e59c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from mmaction.core import confusion_matrix \n",
    "\n",
    "gt_labels = [ann['label'] for ann in dataset.load_annotations()]\n",
    "pred = np.argmax(outputs, axis=1)\n",
    "cf_mat = confusion_matrix(pred, gt_labels).astype(float)\n",
    "cmap = sns.cubehelix_palette(50, hue=0.05, rot=0, light=0.9, dark=0, as_cmap=True)\n",
    "# /np.sum(cf_mat), fmt='.2%',\n",
    "sns.heatmap(cf_mat, cmap=cmap, annot=True, xticklabels = ['6yo', '7yo', '8yo', '9yo', '10yo', '11yo'], yticklabels = ['6yo', '7yo', '8yo', '9yo', '10yo', '11yo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5a001d4-df1e-46d5-9f5e-805438ba858d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5909090909090909\n"
     ]
    }
   ],
   "source": [
    "mean_error = (4+2+1+2+2+1+1)/22 #number of all tested videos\n",
    "print(mean_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650c7218-bd9f-4399-996b-98206ce1b259",
   "metadata": {},
   "source": [
    "# CSN age jog "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "177ce2db-e34c-4ebf-b704-60eb60dd370f",
   "metadata": {
    "id": "LjCcmCKOjktc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mmcv import Config\n",
    "cfg = Config.fromfile('mmaction2/configs/recognition/csn/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21f37bad-181b-4687-aeb7-c7b4cb202f52",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "tlhu9byjjt-K",
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "outputId": "13d1bb01-f351-48c0-9efb-0bdf2c125d29",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "model = dict(\n",
      "    type='Recognizer3D',\n",
      "    backbone=dict(\n",
      "        type='ResNet3dCSN',\n",
      "        pretrained2d=False,\n",
      "        pretrained=\n",
      "        'https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth',\n",
      "        depth=152,\n",
      "        with_pool2=False,\n",
      "        bottleneck_mode='ir',\n",
      "        norm_eval=True,\n",
      "        zero_init_residual=False,\n",
      "        bn_frozen=True),\n",
      "    cls_head=dict(\n",
      "        type='I3DHead',\n",
      "        num_classes=6,\n",
      "        in_channels=2048,\n",
      "        spatial_type='avg',\n",
      "        dropout_ratio=0.5,\n",
      "        init_std=0.01),\n",
      "    train_cfg=None,\n",
      "    test_cfg=dict(average_clips='prob'))\n",
      "checkpoint_config = dict(interval=20)\n",
      "log_config = dict(interval=100, hooks=[dict(type='TextLoggerHook')])\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = 'checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth'\n",
      "resume_from = None\n",
      "workflow = [('train', 1)]\n",
      "dataset_type = 'RawframeDataset'\n",
      "data_root = 'age-gender-3split-rgb-frames/'\n",
      "data_root_val = 'age-gender-3split-rgb-frames/val/'\n",
      "ann_file_train = 'age-gender-3split-rgb-frames/childact_train_rgb320_age_jog.txt'\n",
      "ann_file_val = 'age-gender-3split-rgb-frames/childact_val_rgb320_age_jog.txt'\n",
      "ann_file_test = 'age-gender-3split-rgb-frames/childact_test_rgb320_age_jog.txt'\n",
      "img_norm_cfg = dict(\n",
      "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_bgr=False)\n",
      "train_pipeline = [\n",
      "    dict(type='SampleFrames', clip_len=32, frame_interval=2, num_clips=1),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='RandomResizedCrop'),\n",
      "    dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "    dict(type='Flip', flip_ratio=0.5),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "]\n",
      "val_pipeline = [\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=32,\n",
      "        frame_interval=2,\n",
      "        num_clips=1,\n",
      "        test_mode=True),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='CenterCrop', crop_size=224),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=32,\n",
      "        frame_interval=2,\n",
      "        num_clips=10,\n",
      "        test_mode=True),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='ThreeCrop', crop_size=256),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "data = dict(\n",
      "    videos_per_gpu=4,\n",
      "    workers_per_gpu=4,\n",
      "    train=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_train_rgb320_age_jog.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/train/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=1),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='RandomResizedCrop'),\n",
      "            dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "            dict(type='Flip', flip_ratio=0.5),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'),\n",
      "    val=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file='age-gender-3split-rgb-frames/childact_val_rgb320_age_jog.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/val/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=1,\n",
      "                test_mode=True),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='CenterCrop', crop_size=224),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'),\n",
      "    test=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_test_rgb320_age_jog.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/test/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=10,\n",
      "                test_mode=True),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='ThreeCrop', crop_size=256),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'))\n",
      "evaluation = dict(\n",
      "    interval=5, metrics=['top_k_accuracy', 'mean_class_accuracy'])\n",
      "optimizer = dict(type='SGD', lr=0.000125, momentum=0.9, weight_decay=0.0001)\n",
      "optimizer_config = dict(grad_clip=dict(max_norm=40, norm_type=2))\n",
      "lr_config = dict(\n",
      "    policy='step',\n",
      "    step=[32, 48],\n",
      "    warmup='linear',\n",
      "    warmup_ratio=0.1,\n",
      "    warmup_by_epoch=True,\n",
      "    warmup_iters=16)\n",
      "total_epochs = 50\n",
      "work_dir = './childact-checkpoints/CSN-age-jog'\n",
      "find_unused_parameters = True\n",
      "omnisource = False\n",
      "seed = 42\n",
      "gpu_ids = range(0, 1)\n",
      "output_config = dict(out='./childact-checkpoints/CSN-age-jog/results.json')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mmcv.runner import set_random_seed\n",
    "\n",
    "# Modify dataset type and path\n",
    "# cfg.dataset_type = 'VideoDataset'\n",
    "cfg.data_root = 'age-gender-3split-rgb-frames/'\n",
    "cfg.data_root_val = 'age-gender-3split-rgb-frames/val/'\n",
    "cfg.ann_file_train = 'age-gender-3split-rgb-frames/childact_train_rgb320_age_jog.txt'\n",
    "cfg.ann_file_val = 'age-gender-3split-rgb-frames/childact_val_rgb320_age_jog.txt'\n",
    "cfg.ann_file_test = 'age-gender-3split-rgb-frames/childact_test_rgb320_age_jog.txt'\n",
    "\n",
    "# cfg.data.test.type = 'VideoDataset'\n",
    "cfg.data.test.ann_file = 'age-gender-3split-rgb-frames/childact_test_rgb320_age_jog.txt'\n",
    "cfg.data.test.data_prefix = 'age-gender-3split-rgb-frames/test/'\n",
    "\n",
    "# cfg.data.train.type = 'VideoDataset'\n",
    "cfg.data.train.ann_file = 'age-gender-3split-rgb-frames/childact_train_rgb320_age_jog.txt'\n",
    "cfg.data.train.data_prefix = 'age-gender-3split-rgb-frames/train/'\n",
    "\n",
    "# cfg.data.val.type = 'VideoDataset'\n",
    "cfg.data.val.ann_file = 'age-gender-3split-rgb-frames/childact_val_rgb320_age_jog.txt'\n",
    "cfg.data.val.data_prefix = 'age-gender-3split-rgb-frames/val/'\n",
    "\n",
    "# cfg.data.test.modality = 'Flow'\n",
    "# cfg.data.val.modality = 'Flow'\n",
    "# cfg.data.train.modality = 'Flow'\n",
    "\n",
    "# cfg.data.train.start_index = 0\n",
    "# cfg.data.test.start_index = 0\n",
    "# cfg.data.val.start_index = 0\n",
    "\n",
    "cfg.data.test.filename_tmpl = '{:03}.jpeg'\n",
    "cfg.data.train.filename_tmpl = '{:03}.jpeg'\n",
    "cfg.data.val.filename_tmpl = '{:03}.jpeg'\n",
    "# The flag is used to determine whether it is omnisource training\n",
    "cfg.setdefault('omnisource', False)\n",
    "# Modify num classes of the model in cls_head\n",
    "cfg.model.cls_head.num_classes = 6\n",
    "# We can use the pre-trained TSN model\n",
    "cfg.load_from = 'checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth'\n",
    "# cfg.resume_from = './childact-mm/latest.pth'\n",
    "\n",
    "# Set up working dir to save files and logs.\n",
    "cfg.work_dir = './childact-checkpoints/CSN-age-jog'\n",
    "\n",
    "cfg.total_epochs = 50\n",
    "\n",
    "# cfg.momentum_config = dict(\n",
    "#     policy='cyclic',\n",
    "#     target_ratio=(0.85 / 0.95, 1),\n",
    "#     cyclic_times=1,\n",
    "#     step_ratio_up=0.4,\n",
    "# )\n",
    "\n",
    "# We can set the checkpoint saving interval to reduce the storage cost\n",
    "cfg.checkpoint_config.interval = 20\n",
    "# We can set the log print interval to reduce the the times of printing log\n",
    "cfg.log_config.interval = 100\n",
    "\n",
    "# Set seed thus the results are more reproducible\n",
    "cfg.seed = 42\n",
    "set_random_seed(42, deterministic=False)\n",
    "cfg.gpu_ids = range(1)\n",
    "\n",
    "cfg.data.videos_per_gpu=4\n",
    "\n",
    "# cfg.model.backbone.in_channels = 2\n",
    "\n",
    "cfg.output_config = dict(out=f'{cfg.work_dir}/results.json')\n",
    "# We can initialize the logger for training and have a look\n",
    "# at the final config used for training\n",
    "# del cfg.optimizer['momentum']\n",
    "print(f'Config:\\n{cfg.pretty_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ba29a54-96ed-4dee-a29e-8fc6715f5dbd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "dDBWkdDRk6oz",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "85a52ef3-7b5c-4c52-8fef-00322a8c65e6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 16:16:38,766 - mmaction - INFO - load model from: https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth\n",
      "2021-08-10 16:16:38,767 - mmaction - INFO - Use load_from_http loader\n",
      "2021-08-10 16:16:39,003 - mmaction - INFO - load checkpoint from checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth\n",
      "2021-08-10 16:16:39,003 - mmaction - INFO - Use load_from_local loader\n",
      "2021-08-10 16:16:39,149 - mmaction - WARNING - The model and loaded state dict do not match exactly\n",
      "\n",
      "size mismatch for cls_head.fc_cls.weight: copying a param with shape torch.Size([400, 2048]) from checkpoint, the shape in current model is torch.Size([6, 2048]).\n",
      "size mismatch for cls_head.fc_cls.bias: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([6]).\n",
      "2021-08-10 16:16:39,150 - mmaction - INFO - Start running, host: robt427nv@robt427NV, work_dir: /home/robt427nv/childact/childact-checkpoints/CSN-age-jog\n",
      "2021-08-10 16:16:39,151 - mmaction - INFO - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(ABOVE_NORMAL) OptimizerHook                      \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "2021-08-10 16:16:39,151 - mmaction - INFO - workflow: [('train', 1)], max: 50 epochs\n",
      "/home/robt427nv/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/mmcv/runner/hooks/evaluation.py:190: UserWarning: runner.meta is None. Creating an empty one.\n",
      "  warnings.warn('runner.meta is None. Creating an empty one.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 15/15, 9.2 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 16:19:18,896 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 16:19:18,898 - mmaction - INFO - \n",
      "top1_acc\t0.7333\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 16:19:18,899 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 16:19:18,900 - mmaction - INFO - \n",
      "mean_acc\t0.3750\n",
      "2021-08-10 16:19:19,209 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_5.pth.\n",
      "2021-08-10 16:19:19,210 - mmaction - INFO - Best top1_acc is 0.7333 at 5 epoch.\n",
      "2021-08-10 16:19:19,210 - mmaction - INFO - Epoch(val) [5][4]\ttop1_acc: 0.7333, top5_acc: 1.0000, mean_class_accuracy: 0.3750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 15/15, 9.7 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 16:22:00,722 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 16:22:00,724 - mmaction - INFO - \n",
      "top1_acc\t0.6000\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 16:22:00,724 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 16:22:00,726 - mmaction - INFO - \n",
      "mean_acc\t0.2250\n",
      "2021-08-10 16:22:00,726 - mmaction - INFO - Epoch(val) [10][4]\ttop1_acc: 0.6000, top5_acc: 1.0000, mean_class_accuracy: 0.2250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 15/15, 9.5 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 16:24:39,911 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 16:24:39,914 - mmaction - INFO - \n",
      "top1_acc\t0.4000\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 16:24:39,914 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 16:24:39,917 - mmaction - INFO - \n",
      "mean_acc\t0.2500\n",
      "2021-08-10 16:24:39,918 - mmaction - INFO - Epoch(val) [15][4]\ttop1_acc: 0.4000, top5_acc: 1.0000, mean_class_accuracy: 0.2500\n",
      "2021-08-10 16:27:12,426 - mmaction - INFO - Saving checkpoint at 20 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 15/15, 9.5 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 16:27:14,392 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 16:27:14,394 - mmaction - INFO - \n",
      "top1_acc\t0.5333\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 16:27:14,394 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 16:27:14,396 - mmaction - INFO - \n",
      "mean_acc\t0.3000\n",
      "2021-08-10 16:27:14,396 - mmaction - INFO - Epoch(val) [20][4]\ttop1_acc: 0.5333, top5_acc: 1.0000, mean_class_accuracy: 0.3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 15/15, 9.8 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 16:29:49,037 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 16:29:49,039 - mmaction - INFO - \n",
      "top1_acc\t0.6000\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 16:29:49,039 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 16:29:49,040 - mmaction - INFO - \n",
      "mean_acc\t0.3250\n",
      "2021-08-10 16:29:49,041 - mmaction - INFO - Epoch(val) [25][4]\ttop1_acc: 0.6000, top5_acc: 1.0000, mean_class_accuracy: 0.3250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 15/15, 9.0 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 16:32:24,040 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 16:32:24,085 - mmaction - INFO - \n",
      "top1_acc\t0.2667\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 16:32:24,103 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 16:32:24,196 - mmaction - INFO - \n",
      "mean_acc\t0.4250\n",
      "2021-08-10 16:32:24,237 - mmaction - INFO - Epoch(val) [30][4]\ttop1_acc: 0.2667, top5_acc: 1.0000, mean_class_accuracy: 0.4250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 15/15, 9.6 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 16:34:59,790 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 16:34:59,792 - mmaction - INFO - \n",
      "top1_acc\t0.4667\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 16:34:59,793 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 16:34:59,795 - mmaction - INFO - \n",
      "mean_acc\t0.2750\n",
      "2021-08-10 16:34:59,796 - mmaction - INFO - Epoch(val) [35][4]\ttop1_acc: 0.4667, top5_acc: 1.0000, mean_class_accuracy: 0.2750\n",
      "2021-08-10 16:37:32,558 - mmaction - INFO - Saving checkpoint at 40 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 15/15, 9.2 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 16:37:34,605 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 16:37:34,606 - mmaction - INFO - \n",
      "top1_acc\t0.4667\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 16:37:34,607 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 16:37:34,608 - mmaction - INFO - \n",
      "mean_acc\t0.2750\n",
      "2021-08-10 16:37:34,609 - mmaction - INFO - Epoch(val) [40][4]\ttop1_acc: 0.4667, top5_acc: 1.0000, mean_class_accuracy: 0.2750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 15/15, 9.5 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 16:40:09,047 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 16:40:09,049 - mmaction - INFO - \n",
      "top1_acc\t0.4667\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 16:40:09,049 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 16:40:09,050 - mmaction - INFO - \n",
      "mean_acc\t0.2750\n",
      "2021-08-10 16:40:09,051 - mmaction - INFO - Epoch(val) [45][4]\ttop1_acc: 0.4667, top5_acc: 1.0000, mean_class_accuracy: 0.2750\n",
      "2021-08-10 16:42:41,388 - mmaction - INFO - Saving checkpoint at 50 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 15/15, 9.9 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 16:42:43,314 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 16:42:43,316 - mmaction - INFO - \n",
      "top1_acc\t0.5333\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 16:42:43,317 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 16:42:43,319 - mmaction - INFO - \n",
      "mean_acc\t0.5250\n",
      "2021-08-10 16:42:43,320 - mmaction - INFO - Epoch(val) [50][4]\ttop1_acc: 0.5333, top5_acc: 1.0000, mean_class_accuracy: 0.5250\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "from mmaction.datasets import build_dataset\n",
    "from mmaction.models import build_model\n",
    "from mmaction.apis import train_model\n",
    "\n",
    "import mmcv\n",
    "\n",
    "# Build the dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# Build the recognizer\n",
    "model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.train_cfg, test_cfg=cfg.test_cfg)\n",
    "# CUDA_LAUNCH_BLOCKING=1\n",
    "# Create work_dir\n",
    "mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
    "train_model(model, datasets, cfg, distributed=False, validate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a41036b7-3d48-4055-b643-c3660bf5763e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f\"{cfg.work_dir}/model50e\", 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc25eda3-6a08-41d9-908c-8b016de5c947",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 16:54:18,016 - mmaction - INFO - load model from: https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth\n",
      "2021-08-10 16:54:18,017 - mmaction - INFO - Use load_from_http loader\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "from mmaction.datasets import build_dataset\n",
    "from mmaction.models import build_model\n",
    "from mmaction.apis import train_model\n",
    "import pickle\n",
    "import mmcv\n",
    "# Build the dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# Build the recognizer\n",
    "model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "\n",
    "model = pickle.load(open(f\"{cfg.work_dir}/model50e\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d24a99fd-5a76-492b-98b9-8ee6427e00b9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eyY3hCMwyTct",
    "outputId": "200e37c7-0da4-421f-98da-41418c3110ca",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 10/10, 0.6 task/s, elapsed: 17s, ETA:     0s\n",
      "Evaluating top_k_accuracy ...\n",
      "\n",
      "top1_acc\t0.4000\n",
      "top5_acc\t1.0000\n",
      "\n",
      "Evaluating mean_class_accuracy ...\n",
      "\n",
      "mean_acc\t0.5000\n",
      "top1_acc: 0.4000\n",
      "top5_acc: 1.0000\n",
      "mean_class_accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "from mmaction.apis import single_gpu_test\n",
    "from mmaction.datasets import build_dataloader\n",
    "from mmcv.parallel import MMDataParallel\n",
    "# from mmaction.models import build_model\n",
    "# from mmaction.datasets import build_dataset\n",
    "\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# Build a test dataloader\n",
    "dataset = build_dataset(cfg.data.test, dict(test_mode=True))\n",
    "data_loader = build_dataloader(\n",
    "        dataset,\n",
    "        videos_per_gpu=1,\n",
    "        workers_per_gpu=1,\n",
    "        dist=False,\n",
    "        shuffle=False)\n",
    "model = MMDataParallel(model, device_ids=[0])\n",
    "outputs = single_gpu_test(model, data_loader)\n",
    "\n",
    "eval_config = cfg.evaluation\n",
    "eval_config.pop('interval')\n",
    "eval_res = dataset.evaluate(outputs, **eval_config)\n",
    "for name, val in eval_res.items():\n",
    "    print(f'{name}: {val:.04f}')\n",
    "    \n",
    "output_config = cfg.output_config\n",
    "dataset.dump_results(outputs, **output_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd2bb12b-8dba-4586-a0f3-b0b61985ba00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from mmaction.core import confusion_matrix \n",
    "\n",
    "gt_labels = [ann['label'] for ann in dataset.load_annotations()]\n",
    "pred = np.argmax(outputs, axis=1)\n",
    "cf_mat = confusion_matrix(pred, gt_labels).astype(float)\n",
    "cmap = sns.cubehelix_palette(50, hue=0.05, rot=0, light=0.9, dark=0, as_cmap=True)\n",
    "# /np.sum(cf_mat), fmt='.2%',\n",
    "sns.heatmap(cf_mat, cmap=cmap, annot=True, xticklabels = ['6yo', '7yo', '8yo', '9yo'], yticklabels = ['6yo', '7yo', '8yo', '9yo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7230d4f-ba5e-4c7f-ac0a-cbf11a679a8f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n"
     ]
    }
   ],
   "source": [
    "mean_error = (1+4+3)/10 #number of all tested videos\n",
    "print(mean_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef536a9f-1d69-40dc-aa45-66373942e6aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed0ef921-f91a-4eb4-8434-23c61aa04646",
   "metadata": {},
   "source": [
    "# CSN age run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1df6aaf-c0ef-4d6d-8db0-6ff71344c370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/robt427nv/childact\n"
     ]
    }
   ],
   "source": [
    "cd /home/robt427nv/childact/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d8ca5e9-46c8-4fe5-a41e-d22359ef9dd3",
   "metadata": {
    "id": "LjCcmCKOjktc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mmcv import Config\n",
    "cfg = Config.fromfile('mmaction2/configs/recognition/csn/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c82d6fcb-4f24-4912-b0b8-d1fe792a94cf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "tlhu9byjjt-K",
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "outputId": "13d1bb01-f351-48c0-9efb-0bdf2c125d29",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "model = dict(\n",
      "    type='Recognizer3D',\n",
      "    backbone=dict(\n",
      "        type='ResNet3dCSN',\n",
      "        pretrained2d=False,\n",
      "        pretrained=\n",
      "        'https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth',\n",
      "        depth=152,\n",
      "        with_pool2=False,\n",
      "        bottleneck_mode='ir',\n",
      "        norm_eval=True,\n",
      "        zero_init_residual=False,\n",
      "        bn_frozen=True),\n",
      "    cls_head=dict(\n",
      "        type='I3DHead',\n",
      "        num_classes=6,\n",
      "        in_channels=2048,\n",
      "        spatial_type='avg',\n",
      "        dropout_ratio=0.5,\n",
      "        init_std=0.01),\n",
      "    train_cfg=None,\n",
      "    test_cfg=dict(average_clips='prob'))\n",
      "checkpoint_config = dict(interval=20)\n",
      "log_config = dict(interval=100, hooks=[dict(type='TextLoggerHook')])\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = 'checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth'\n",
      "resume_from = None\n",
      "workflow = [('train', 1)]\n",
      "dataset_type = 'RawframeDataset'\n",
      "data_root = 'age-gender-3split-rgb-frames/'\n",
      "data_root_val = 'age-gender-3split-rgb-frames/val/'\n",
      "ann_file_train = 'age-gender-3split-rgb-frames/childact_train_rgb320_age_run.txt'\n",
      "ann_file_val = 'age-gender-3split-rgb-frames/childact_val_rgb320_age_run.txt'\n",
      "ann_file_test = 'age-gender-3split-rgb-frames/childact_test_rgb320_age_run.txt'\n",
      "img_norm_cfg = dict(\n",
      "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_bgr=False)\n",
      "train_pipeline = [\n",
      "    dict(type='SampleFrames', clip_len=32, frame_interval=2, num_clips=1),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='RandomResizedCrop'),\n",
      "    dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "    dict(type='Flip', flip_ratio=0.5),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "]\n",
      "val_pipeline = [\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=32,\n",
      "        frame_interval=2,\n",
      "        num_clips=1,\n",
      "        test_mode=True),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='CenterCrop', crop_size=224),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=32,\n",
      "        frame_interval=2,\n",
      "        num_clips=10,\n",
      "        test_mode=True),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='ThreeCrop', crop_size=256),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "data = dict(\n",
      "    videos_per_gpu=6,\n",
      "    workers_per_gpu=4,\n",
      "    train=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_train_rgb320_age_run.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/train/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=1),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='RandomResizedCrop'),\n",
      "            dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "            dict(type='Flip', flip_ratio=0.5),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'),\n",
      "    val=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file='age-gender-3split-rgb-frames/childact_val_rgb320_age_run.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/val/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=1,\n",
      "                test_mode=True),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='CenterCrop', crop_size=224),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'),\n",
      "    test=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_test_rgb320_age_run.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/test/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=10,\n",
      "                test_mode=True),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='ThreeCrop', crop_size=256),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'))\n",
      "evaluation = dict(\n",
      "    interval=5, metrics=['top_k_accuracy', 'mean_class_accuracy'])\n",
      "optimizer = dict(type='SGD', lr=0.000125, momentum=0.9, weight_decay=0.0001)\n",
      "optimizer_config = dict(grad_clip=dict(max_norm=40, norm_type=2))\n",
      "lr_config = dict(\n",
      "    policy='step',\n",
      "    step=[32, 48],\n",
      "    warmup='linear',\n",
      "    warmup_ratio=0.1,\n",
      "    warmup_by_epoch=True,\n",
      "    warmup_iters=16)\n",
      "total_epochs = 50\n",
      "work_dir = './childact-checkpoints/CSN-age-run'\n",
      "find_unused_parameters = True\n",
      "omnisource = False\n",
      "seed = 42\n",
      "gpu_ids = range(0, 1)\n",
      "output_config = dict(out='./childact-checkpoints/CSN-age-run/results.json')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mmcv.runner import set_random_seed\n",
    "\n",
    "# Modify dataset type and path\n",
    "# cfg.dataset_type = 'VideoDataset'\n",
    "cfg.data_root = 'age-gender-3split-rgb-frames/'\n",
    "cfg.data_root_val = 'age-gender-3split-rgb-frames/val/'\n",
    "cfg.ann_file_train = 'age-gender-3split-rgb-frames/childact_train_rgb320_age_run.txt'\n",
    "cfg.ann_file_val = 'age-gender-3split-rgb-frames/childact_val_rgb320_age_run.txt'\n",
    "cfg.ann_file_test = 'age-gender-3split-rgb-frames/childact_test_rgb320_age_run.txt'\n",
    "\n",
    "# cfg.data.test.type = 'VideoDataset'\n",
    "cfg.data.test.ann_file = 'age-gender-3split-rgb-frames/childact_test_rgb320_age_run.txt'\n",
    "cfg.data.test.data_prefix = 'age-gender-3split-rgb-frames/test/'\n",
    "\n",
    "# cfg.data.train.type = 'VideoDataset'\n",
    "cfg.data.train.ann_file = 'age-gender-3split-rgb-frames/childact_train_rgb320_age_run.txt'\n",
    "cfg.data.train.data_prefix = 'age-gender-3split-rgb-frames/train/'\n",
    "\n",
    "# cfg.data.val.type = 'VideoDataset'\n",
    "cfg.data.val.ann_file = 'age-gender-3split-rgb-frames/childact_val_rgb320_age_run.txt'\n",
    "cfg.data.val.data_prefix = 'age-gender-3split-rgb-frames/val/'\n",
    "\n",
    "# cfg.data.test.modality = 'Flow'\n",
    "# cfg.data.val.modality = 'Flow'\n",
    "# cfg.data.train.modality = 'Flow'\n",
    "\n",
    "# cfg.data.train.start_index = 0\n",
    "# cfg.data.test.start_index = 0\n",
    "# cfg.data.val.start_index = 0\n",
    "\n",
    "cfg.data.test.filename_tmpl = '{:03}.jpeg'\n",
    "cfg.data.train.filename_tmpl = '{:03}.jpeg'\n",
    "cfg.data.val.filename_tmpl = '{:03}.jpeg'\n",
    "# The flag is used to determine whether it is omnisource training\n",
    "cfg.setdefault('omnisource', False)\n",
    "# Modify num classes of the model in cls_head\n",
    "cfg.model.cls_head.num_classes = 6\n",
    "# We can use the pre-trained TSN model\n",
    "cfg.load_from = 'checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth'\n",
    "# cfg.resume_from = './childact-mm/latest.pth'\n",
    "\n",
    "# Set up working dir to save files and logs.\n",
    "cfg.work_dir = './childact-checkpoints/CSN-age-run'\n",
    "\n",
    "cfg.total_epochs = 50\n",
    "\n",
    "# cfg.momentum_config = dict(\n",
    "#     policy='cyclic',\n",
    "#     target_ratio=(0.85 / 0.95, 1),\n",
    "#     cyclic_times=1,\n",
    "#     step_ratio_up=0.4,\n",
    "# )\n",
    "\n",
    "# We can set the checkpoint saving interval to reduce the storage cost\n",
    "cfg.checkpoint_config.interval = 20\n",
    "# We can set the log print interval to reduce the the times of printing log\n",
    "cfg.log_config.interval = 100\n",
    "\n",
    "# Set seed thus the results are more reproducible\n",
    "cfg.seed = 42\n",
    "set_random_seed(42, deterministic=False)\n",
    "cfg.gpu_ids = range(1)\n",
    "\n",
    "cfg.data.videos_per_gpu=6\n",
    "\n",
    "# cfg.model.backbone.in_channels = 2\n",
    "\n",
    "cfg.output_config = dict(out=f'{cfg.work_dir}/results.json')\n",
    "# We can initialize the logger for training and have a look\n",
    "# at the final config used for training\n",
    "# del cfg.optimizer['momentum']\n",
    "print(f'Config:\\n{cfg.pretty_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "241f22d5-8cb5-40d8-a59d-739ab6886dd8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dDBWkdDRk6oz",
    "outputId": "85a52ef3-7b5c-4c52-8fef-00322a8c65e6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-23 00:04:56,437 - mmaction - INFO - load model from: https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth\n",
      "2021-08-23 00:04:56,438 - mmaction - INFO - Use load_from_http loader\n",
      "2021-08-23 00:04:58,498 - mmaction - INFO - load checkpoint from checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth\n",
      "2021-08-23 00:04:58,499 - mmaction - INFO - Use load_from_local loader\n",
      "2021-08-23 00:04:58,651 - mmaction - WARNING - The model and loaded state dict do not match exactly\n",
      "\n",
      "size mismatch for cls_head.fc_cls.weight: copying a param with shape torch.Size([400, 2048]) from checkpoint, the shape in current model is torch.Size([6, 2048]).\n",
      "size mismatch for cls_head.fc_cls.bias: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([6]).\n",
      "2021-08-23 00:04:58,657 - mmaction - INFO - Start running, host: robt427nv@robt427NV, work_dir: /home/robt427nv/childact/childact-checkpoints/CSN-age-run\n",
      "2021-08-23 00:04:58,658 - mmaction - INFO - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(ABOVE_NORMAL) OptimizerHook                      \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "2021-08-23 00:04:58,658 - mmaction - INFO - workflow: [('train', 1)], max: 50 epochs\n",
      "/home/robt427nv/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/mmcv/runner/hooks/evaluation.py:190: UserWarning: runner.meta is None. Creating an empty one.\n",
      "  warnings.warn('runner.meta is None. Creating an empty one.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 23/23, 10.9 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-23 00:06:56,485 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-23 00:06:56,488 - mmaction - INFO - \n",
      "top1_acc\t0.3913\n",
      "top5_acc\t0.8696\n",
      "2021-08-23 00:06:56,489 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-23 00:06:56,491 - mmaction - INFO - \n",
      "mean_acc\t0.2000\n",
      "2021-08-23 00:06:56,802 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_5.pth.\n",
      "2021-08-23 00:06:56,803 - mmaction - INFO - Best top1_acc is 0.3913 at 5 epoch.\n",
      "2021-08-23 00:06:56,803 - mmaction - INFO - Epoch(val) [5][4]\ttop1_acc: 0.3913, top5_acc: 0.8696, mean_class_accuracy: 0.2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 23/23, 11.7 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-23 00:08:53,711 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-23 00:08:53,712 - mmaction - INFO - \n",
      "top1_acc\t0.3478\n",
      "top5_acc\t0.8696\n",
      "2021-08-23 00:08:53,713 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-23 00:08:53,714 - mmaction - INFO - \n",
      "mean_acc\t0.1481\n",
      "2021-08-23 00:08:53,715 - mmaction - INFO - Epoch(val) [10][4]\ttop1_acc: 0.3478, top5_acc: 0.8696, mean_class_accuracy: 0.1481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 23/23, 11.3 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-23 00:10:50,507 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-23 00:10:50,509 - mmaction - INFO - \n",
      "top1_acc\t0.4783\n",
      "top5_acc\t0.8696\n",
      "2021-08-23 00:10:50,510 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-23 00:10:50,510 - mmaction - INFO - \n",
      "mean_acc\t0.3156\n",
      "2021-08-23 00:10:50,859 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_15.pth.\n",
      "2021-08-23 00:10:50,860 - mmaction - INFO - Best top1_acc is 0.4783 at 15 epoch.\n",
      "2021-08-23 00:10:50,860 - mmaction - INFO - Epoch(val) [15][4]\ttop1_acc: 0.4783, top5_acc: 0.8696, mean_class_accuracy: 0.3156\n",
      "2021-08-23 00:12:45,802 - mmaction - INFO - Saving checkpoint at 20 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 23/23, 11.1 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-23 00:12:48,284 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-23 00:12:48,286 - mmaction - INFO - \n",
      "top1_acc\t0.3043\n",
      "top5_acc\t0.8696\n",
      "2021-08-23 00:12:48,286 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-23 00:12:48,287 - mmaction - INFO - \n",
      "mean_acc\t0.2185\n",
      "2021-08-23 00:12:48,287 - mmaction - INFO - Epoch(val) [20][4]\ttop1_acc: 0.3043, top5_acc: 0.8696, mean_class_accuracy: 0.2185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 23/23, 11.4 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-23 00:14:45,022 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-23 00:14:45,024 - mmaction - INFO - \n",
      "top1_acc\t0.3478\n",
      "top5_acc\t0.8696\n",
      "2021-08-23 00:14:45,024 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-23 00:14:45,025 - mmaction - INFO - \n",
      "mean_acc\t0.1778\n",
      "2021-08-23 00:14:45,026 - mmaction - INFO - Epoch(val) [25][4]\ttop1_acc: 0.3478, top5_acc: 0.8696, mean_class_accuracy: 0.1778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 23/23, 11.5 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-23 00:16:41,882 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-23 00:16:41,884 - mmaction - INFO - \n",
      "top1_acc\t0.3913\n",
      "top5_acc\t0.8696\n",
      "2021-08-23 00:16:41,885 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-23 00:16:41,886 - mmaction - INFO - \n",
      "mean_acc\t0.1963\n",
      "2021-08-23 00:16:41,886 - mmaction - INFO - Epoch(val) [30][4]\ttop1_acc: 0.3913, top5_acc: 0.8696, mean_class_accuracy: 0.1963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 23/23, 11.5 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-23 00:18:38,544 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-23 00:18:38,545 - mmaction - INFO - \n",
      "top1_acc\t0.3913\n",
      "top5_acc\t0.8696\n",
      "2021-08-23 00:18:38,546 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-23 00:18:38,547 - mmaction - INFO - \n",
      "mean_acc\t0.2259\n",
      "2021-08-23 00:18:38,548 - mmaction - INFO - Epoch(val) [35][4]\ttop1_acc: 0.3913, top5_acc: 0.8696, mean_class_accuracy: 0.2259\n",
      "2021-08-23 00:20:33,574 - mmaction - INFO - Saving checkpoint at 40 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 23/23, 11.3 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-23 00:20:36,022 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-23 00:20:36,023 - mmaction - INFO - \n",
      "top1_acc\t0.4348\n",
      "top5_acc\t0.8696\n",
      "2021-08-23 00:20:36,023 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-23 00:20:36,024 - mmaction - INFO - \n",
      "mean_acc\t0.3111\n",
      "2021-08-23 00:20:36,024 - mmaction - INFO - Epoch(val) [40][4]\ttop1_acc: 0.4348, top5_acc: 0.8696, mean_class_accuracy: 0.3111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 23/23, 11.2 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-23 00:22:32,715 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-23 00:22:32,717 - mmaction - INFO - \n",
      "top1_acc\t0.3913\n",
      "top5_acc\t0.8696\n",
      "2021-08-23 00:22:32,717 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-23 00:22:32,718 - mmaction - INFO - \n",
      "mean_acc\t0.2711\n",
      "2021-08-23 00:22:32,719 - mmaction - INFO - Epoch(val) [45][4]\ttop1_acc: 0.3913, top5_acc: 0.8696, mean_class_accuracy: 0.2711\n",
      "2021-08-23 00:24:27,755 - mmaction - INFO - Saving checkpoint at 50 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 23/23, 10.6 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-23 00:24:30,351 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-23 00:24:30,354 - mmaction - INFO - \n",
      "top1_acc\t0.3913\n",
      "top5_acc\t0.8696\n",
      "2021-08-23 00:24:30,354 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-23 00:24:30,356 - mmaction - INFO - \n",
      "mean_acc\t0.2259\n",
      "2021-08-23 00:24:30,357 - mmaction - INFO - Epoch(val) [50][4]\ttop1_acc: 0.3913, top5_acc: 0.8696, mean_class_accuracy: 0.2259\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "from mmaction.datasets import build_dataset\n",
    "from mmaction.models import build_model\n",
    "from mmaction.apis import train_model\n",
    "\n",
    "import mmcv\n",
    "\n",
    "# Build the dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# Build the recognizer\n",
    "model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.train_cfg, test_cfg=cfg.test_cfg)\n",
    "# CUDA_LAUNCH_BLOCKING=1\n",
    "# Create work_dir\n",
    "mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
    "train_model(model, datasets, cfg, distributed=False, validate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d6de14c-7f8d-4397-b11c-4bdcb42bb850",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f\"{cfg.work_dir}/model50e\", 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce03ef4d-608d-4108-b6da-5768cfc5b83c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 16:54:18,016 - mmaction - INFO - load model from: https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth\n",
      "2021-08-10 16:54:18,017 - mmaction - INFO - Use load_from_http loader\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "from mmaction.datasets import build_dataset\n",
    "from mmaction.models import build_model\n",
    "from mmaction.apis import train_model\n",
    "import pickle\n",
    "import mmcv\n",
    "# Build the dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# Build the recognizer\n",
    "model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "\n",
    "model = pickle.load(open(f\"{cfg.work_dir}/model50e\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39c23b5e-3e29-444f-beff-579b92ec9f1b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eyY3hCMwyTct",
    "outputId": "200e37c7-0da4-421f-98da-41418c3110ca",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 26/26, 0.6 task/s, elapsed: 42s, ETA:     0s\n",
      "Evaluating top_k_accuracy ...\n",
      "\n",
      "top1_acc\t0.5000\n",
      "top5_acc\t0.9615\n",
      "\n",
      "Evaluating mean_class_accuracy ...\n",
      "\n",
      "mean_acc\t0.4048\n",
      "top1_acc: 0.5000\n",
      "top5_acc: 0.9615\n",
      "mean_class_accuracy: 0.4048\n"
     ]
    }
   ],
   "source": [
    "from mmaction.apis import single_gpu_test\n",
    "from mmaction.datasets import build_dataloader\n",
    "from mmcv.parallel import MMDataParallel\n",
    "# from mmaction.models import build_model\n",
    "# from mmaction.datasets import build_dataset\n",
    "\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# Build a test dataloader\n",
    "dataset = build_dataset(cfg.data.test, dict(test_mode=True))\n",
    "data_loader = build_dataloader(\n",
    "        dataset,\n",
    "        videos_per_gpu=1,\n",
    "        workers_per_gpu=1,\n",
    "        dist=False,\n",
    "        shuffle=False)\n",
    "model = MMDataParallel(model, device_ids=[0])\n",
    "outputs = single_gpu_test(model, data_loader)\n",
    "\n",
    "eval_config = cfg.evaluation\n",
    "eval_config.pop('interval')\n",
    "eval_res = dataset.evaluate(outputs, **eval_config)\n",
    "for name, val in eval_res.items():\n",
    "    print(f'{name}: {val:.04f}')\n",
    "    \n",
    "output_config = cfg.output_config\n",
    "dataset.dump_results(outputs, **output_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f01ead32-8af1-42bb-b420-17b49f9eed24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from mmaction.core import confusion_matrix \n",
    "\n",
    "gt_labels = [ann['label'] for ann in dataset.load_annotations()]\n",
    "pred = np.argmax(outputs, axis=1)\n",
    "cf_mat = confusion_matrix(pred, gt_labels).astype(float)\n",
    "cmap = sns.cubehelix_palette(50, hue=0.05, rot=0, light=0.9, dark=0, as_cmap=True)\n",
    "# /np.sum(cf_mat), fmt='.2%',\n",
    "sns.heatmap(cf_mat, cmap=cmap, annot=True, xticklabels = ['6yo', '7yo', '8yo', '9yo', '10yo', '11yo'], yticklabels = ['6yo', '7yo', '8yo', '9yo', '10yo', '11yo'])\n",
    "# sns.heatmap(cf_mat, cmap=cmap, annot=True, xticklabels = ['7yo', '8yo', '9yo', '10yo'], yticklabels = ['7yo', '8yo', '9yo', '10yo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "857793bd-40c8-41c9-9ad8-01c3e9099da5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5384615384615384\n"
     ]
    }
   ],
   "source": [
    "mean_error = (1+5+2+5+1)/26 #number of all tested videos\n",
    "print(mean_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad56099-6810-496c-9b39-e7e3422a358d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/open-mmlab/mmaction2/blob/master/demo/mmaction2_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VcjSRFELVbNk",
    "tags": []
   },
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bf8PpPXtVvmg",
    "outputId": "2c685a33-474b-4e71-8f98-c2533c66095e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2017 NVIDIA Corporation\n",
      "Built on Fri_Nov__3_21:07:56_CDT_2017\n",
      "Cuda compilation tools, release 9.1, V9.1.85\n",
      "gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\n",
      "Copyright (C) 2017 Free Software Foundation, Inc.\n",
      "This is free software; see the source for copying conditions.  There is NO\n",
      "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check nvcc version\n",
    "!nvcc -V\n",
    "# Check GCC version\n",
    "!gcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5PAJ4ArzV5Ry",
    "outputId": "e48dbf61-fae0-431c-e964-04c7caaee4bc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install some optional requirements\n",
    "# !pip install -r requirements/optional.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "No_zZAFpWC-a",
    "outputId": "1d425eea-d44e-434a-991c-01eb15abaab2",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.0+cu110 True\n",
      "0.12.0\n",
      "11.0\n",
      "GCC 7.5\n"
     ]
    }
   ],
   "source": [
    "# Check Pytorch installation\n",
    "import torch, torchvision\n",
    "print(torch.__version__, torch.cuda.is_available())\n",
    "\n",
    "# Check MMAction2 installation\n",
    "import mmaction\n",
    "print(mmaction.__version__)\n",
    "\n",
    "# Check MMCV installation\n",
    "from mmcv.ops import get_compiling_cuda_version, get_compiler_version\n",
    "print(get_compiling_cuda_version())\n",
    "print(get_compiler_version())\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/actrec/.local/lib/python3.6/site-packages/decord-0.5.3-py3.6-linux-x86_64.egg')\n",
    "import decord\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/actrec/.virtualenvs/mmaction/mmaction2\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mcheckpoints\u001b[0m/  \u001b[01;34mdemo\u001b[0m/        LICENSE              README.md         setup.cfg\n",
      "\u001b[01;34mchildact-mm\u001b[0m/  \u001b[01;34mdocker\u001b[0m/      \u001b[01;34mmmaction\u001b[0m/            README_zh-CN.md   setup.py\n",
      "\u001b[01;34mconfigs\u001b[0m/      \u001b[01;34mdocs\u001b[0m/        \u001b[01;34mmmaction2.egg-info\u001b[0m/  \u001b[01;34mrequirements\u001b[0m/     \u001b[01;34mtests\u001b[0m/\n",
      "\u001b[01;34mdata\u001b[0m/         \u001b[01;34mdocs_zh_CN\u001b[0m/  my-mmaction.ipynb    requirements.txt  \u001b[01;34mtools\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "# TSN 94.44% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "64CW6d_AaT-Q",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "3b284fd8-4ee7-4a34-90d7-5023cd123a04",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-03-22 01:46:07--  https://download.openmmlab.com/mmaction/recognition/tsn/tsn_r50_video_1x1x8_100e_kinetics600_rgb/tsn_r50_video_1x1x8_100e_kinetics600_rgb_20201015-4db3c461.pth\n",
      "Resolving download.openmmlab.com (download.openmmlab.com)... 47.75.20.25\n",
      "Connecting to download.openmmlab.com (download.openmmlab.com)|47.75.20.25|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 99220779 (95M) [application/octet-stream]\n",
      "Saving to: ‘checkpoints/tsn_r50_video_1x1x8_100e_kinetics600_rgb_20201015-4db3c461.pth’\n",
      "\n",
      "checkpoints/tsn_r50 100%[===================>]  94,62M  10,5MB/s    in 11s     \n",
      "\n",
      "2021-03-22 01:46:22 (8,61 MB/s) - ‘checkpoints/tsn_r50_video_1x1x8_100e_kinetics600_rgb_20201015-4db3c461.pth’ saved [99220779/99220779]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !mkdir checkpoints\n",
    "# !wget -c https://download.openmmlab.com/   .pth \\\n",
    "#       -O checkpoints/db3c461.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "LjCcmCKOjktc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mmcv import Config\n",
    "cfg = Config.fromfile('./configs/recognition/tsn/tsn_r50_video_1x1x8_100e_kinetics400_rgb.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "tlhu9byjjt-K",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "13d1bb01-f351-48c0-9efb-0bdf2c125d29",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "model = dict(\n",
      "    type='Recognizer2D',\n",
      "    backbone=dict(\n",
      "        type='ResNet',\n",
      "        pretrained='torchvision://resnet50',\n",
      "        depth=50,\n",
      "        norm_eval=False),\n",
      "    cls_head=dict(\n",
      "        type='TSNHead',\n",
      "        num_classes=7,\n",
      "        in_channels=2048,\n",
      "        spatial_type='avg',\n",
      "        consensus=dict(type='AvgConsensus', dim=1),\n",
      "        dropout_ratio=0.4,\n",
      "        init_std=0.01),\n",
      "    train_cfg=None,\n",
      "    test_cfg=dict(average_clips=None))\n",
      "optimizer = dict(type='SGD', lr=0.0001, momentum=0.9, weight_decay=0.0001)\n",
      "optimizer_config = dict(grad_clip=dict(max_norm=40, norm_type=2))\n",
      "lr_config = dict(\n",
      "    policy='cyclic',\n",
      "    target_ratio=(10, 0.0001),\n",
      "    cyclic_times=1,\n",
      "    step_ratio_up=0.4)\n",
      "total_epochs = 40\n",
      "checkpoint_config = dict(interval=2)\n",
      "log_config = dict(interval=20, hooks=[dict(type='TextLoggerHook')])\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = './childact-checkpoints/childact-tsn/epoch_30.pth'\n",
      "resume_from = './childact-checkpoints/childact-tsn/epoch_30.pth'\n",
      "workflow = [('train', 1)]\n",
      "dataset_type = 'VideoDataset'\n",
      "data_root = 'data/childact_split/train/'\n",
      "data_root_val = 'data/childact_split/val/'\n",
      "ann_file_train = 'data/childact_split/childact_train_video.txt'\n",
      "ann_file_val = 'data/childact_split/childact_val_video.txt'\n",
      "ann_file_test = 'data/childact_split/childact_test_video.txt'\n",
      "img_norm_cfg = dict(\n",
      "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_bgr=False)\n",
      "train_pipeline = [\n",
      "    dict(type='DecordInit'),\n",
      "    dict(type='SampleFrames', clip_len=1, frame_interval=1, num_clips=8),\n",
      "    dict(type='DecordDecode'),\n",
      "    dict(\n",
      "        type='MultiScaleCrop',\n",
      "        input_size=224,\n",
      "        scales=(1, 0.875, 0.75, 0.66),\n",
      "        random_crop=False,\n",
      "        max_wh_scale_gap=1),\n",
      "    dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "    dict(type='Flip', flip_ratio=0.5),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "]\n",
      "val_pipeline = [\n",
      "    dict(type='DecordInit'),\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=1,\n",
      "        frame_interval=1,\n",
      "        num_clips=8,\n",
      "        test_mode=True),\n",
      "    dict(type='DecordDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='CenterCrop', crop_size=224),\n",
      "    dict(type='Flip', flip_ratio=0),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(type='DecordInit'),\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=1,\n",
      "        frame_interval=1,\n",
      "        num_clips=25,\n",
      "        test_mode=True),\n",
      "    dict(type='DecordDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='ThreeCrop', crop_size=256),\n",
      "    dict(type='Flip', flip_ratio=0),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "data = dict(\n",
      "    videos_per_gpu=32,\n",
      "    workers_per_gpu=4,\n",
      "    train=dict(\n",
      "        type='VideoDataset',\n",
      "        ann_file='data/childact_split/childact_train_video.txt',\n",
      "        data_prefix='data/childact_split/train/',\n",
      "        pipeline=[\n",
      "            dict(type='DecordInit'),\n",
      "            dict(\n",
      "                type='SampleFrames', clip_len=1, frame_interval=1,\n",
      "                num_clips=8),\n",
      "            dict(type='DecordDecode'),\n",
      "            dict(\n",
      "                type='MultiScaleCrop',\n",
      "                input_size=224,\n",
      "                scales=(1, 0.875, 0.75, 0.66),\n",
      "                random_crop=False,\n",
      "                max_wh_scale_gap=1),\n",
      "            dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "            dict(type='Flip', flip_ratio=0.5),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "        ]),\n",
      "    val=dict(\n",
      "        type='VideoDataset',\n",
      "        ann_file='data/childact_split/childact_val_video.txt',\n",
      "        data_prefix='data/childact_split/val/',\n",
      "        pipeline=[\n",
      "            dict(type='DecordInit'),\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=1,\n",
      "                frame_interval=1,\n",
      "                num_clips=8,\n",
      "                test_mode=True),\n",
      "            dict(type='DecordDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='CenterCrop', crop_size=224),\n",
      "            dict(type='Flip', flip_ratio=0),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ]),\n",
      "    test=dict(\n",
      "        type='VideoDataset',\n",
      "        ann_file='data/childact_split/childact_test_video.txt',\n",
      "        data_prefix='data/childact_split/test/',\n",
      "        pipeline=[\n",
      "            dict(type='DecordInit'),\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=1,\n",
      "                frame_interval=1,\n",
      "                num_clips=25,\n",
      "                test_mode=True),\n",
      "            dict(type='DecordDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='ThreeCrop', crop_size=256),\n",
      "            dict(type='Flip', flip_ratio=0),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ]))\n",
      "evaluation = dict(\n",
      "    interval=2, metrics=['top_k_accuracy', 'mean_class_accuracy'])\n",
      "work_dir = './childact-checkpoints/childact-tsn-2/'\n",
      "omnisource = False\n",
      "seed = 42\n",
      "gpu_ids = range(0, 1)\n",
      "output_config = dict(out='./childact-checkpoints/childact-tsn-2//results.json')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mmcv.runner import set_random_seed\n",
    "\n",
    "# Modify dataset type and path\n",
    "cfg.dataset_type = 'VideoDataset'\n",
    "cfg.data_root = 'data/childact_split/train/'\n",
    "cfg.data_root_val = 'data/childact_split/val/'\n",
    "cfg.ann_file_train = 'data/childact_split/childact_train_video.txt'\n",
    "cfg.ann_file_val = 'data/childact_split/childact_val_video.txt'\n",
    "cfg.ann_file_test = 'data/childact_split/childact_test_video.txt'\n",
    "\n",
    "cfg.data.test.type = 'VideoDataset'\n",
    "cfg.data.test.ann_file = 'data/childact_split/childact_test_video.txt'\n",
    "cfg.data.test.data_prefix = 'data/childact_split/test/'\n",
    "\n",
    "cfg.data.train.type = 'VideoDataset'\n",
    "cfg.data.train.ann_file = 'data/childact_split/childact_train_video.txt'\n",
    "cfg.data.train.data_prefix = 'data/childact_split/train/'\n",
    "\n",
    "cfg.data.val.type = 'VideoDataset'\n",
    "cfg.data.val.ann_file = 'data/childact_split/childact_val_video.txt'\n",
    "cfg.data.val.data_prefix = 'data/childact_split/val/'\n",
    "\n",
    "# The flag is used to determine whether it is omnisource training\n",
    "cfg.setdefault('omnisource', False)\n",
    "# Modify num classes of the model in cls_head\n",
    "cfg.model.cls_head.num_classes = 7\n",
    "# We can use the pre-trained TSN model\n",
    "cfg.load_from = './childact-checkpoints/childact-tsn/epoch_30.pth'\n",
    "cfg.resume_from = './childact-checkpoints/childact-tsn/epoch_30.pth'\n",
    "\n",
    "# Set up working dir to save files and logs.\n",
    "cfg.work_dir = './childact-checkpoints/childact-tsn-2/'\n",
    "\n",
    "# The original learning rate (LR) is set for 8-GPU training.\n",
    "# We divide it by 8 since we only use one GPU.\n",
    "cfg.data.videos_per_gpu = 32\n",
    "cfg.optimizer.lr = 0.0001\n",
    "# cfg.lr_config.type = 'cyclic'\n",
    "cfg.total_epochs = 40\n",
    "\n",
    "cfg.lr_config = dict(\n",
    "    policy='cyclic',\n",
    "    target_ratio=(10, 1e-4),\n",
    "    cyclic_times=1,\n",
    "    step_ratio_up=0.4,\n",
    ")\n",
    "\n",
    "# We can set the checkpoint saving interval to reduce the storage cost\n",
    "cfg.checkpoint_config.interval = 2\n",
    "# We can set the log print interval to reduce the the times of printing log\n",
    "cfg.log_config.interval = 20\n",
    "cfg.evaluation.interval = 2\n",
    "# Set seed thus the results are more reproducible\n",
    "cfg.seed = 42\n",
    "set_random_seed(42, deterministic=False)\n",
    "cfg.gpu_ids = range(1)\n",
    "\n",
    "# We can initialize the logger for training and have a look\n",
    "# at the final config used for training\n",
    "cfg.output_config = dict(out=f'{cfg.work_dir}/results.json')\n",
    "print(f'Config:\\n{cfg.pretty_text}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tES-qnZ3k38Z",
    "tags": []
   },
   "source": [
    "### Train a new recognizer\n",
    "\n",
    "Finally, lets initialize the dataset and recognizer, then train a new recognizer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "dDBWkdDRk6oz",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "85a52ef3-7b5c-4c52-8fef-00322a8c65e6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-25 14:12:39,300 - mmaction - INFO - These parameters in pretrained checkpoint are not loaded: {'fc.weight', 'fc.bias'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use load_from_torchvision loader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-25 14:12:39,359 - mmaction - INFO - load checkpoint from ./childact-checkpoints/childact-tsn/epoch_30.pth\n",
      "2021-03-25 14:12:39,360 - mmaction - INFO - Use load_from_local loader\n",
      "2021-03-25 14:12:39,874 - mmaction - INFO - resumed epoch 30, iter 1155\n",
      "2021-03-25 14:12:39,875 - mmaction - INFO - Start running, host: actrec@actrec-HP-Z4-G4-Workstation, work_dir: /home/actrec/.virtualenvs/mmaction/mmaction2/childact-checkpoints/childact-tsn-2\n",
      "2021-03-25 14:12:39,876 - mmaction - INFO - workflow: [('train', 1)], max: 40 epochs\n",
      "/home/actrec/.virtualenvs/mmaction/mmaction2/mmaction/core/evaluation/eval_hooks.py:131: UserWarning: runner.meta is None. Creating a empty one.\n",
      "  warnings.warn('runner.meta is None. Creating a empty one.')\n",
      "2021-03-25 14:15:28,124 - mmaction - INFO - Epoch [31][20/33]\tlr: 4.077e-04, eta: 0:20:19, time: 8.412, data_time: 7.526, memory: 21540, top1_acc: 0.9422, top5_acc: 1.0000, loss_cls: 0.1803, loss: 0.1803, grad_norm: 3.5818\n",
      "2021-03-25 14:20:14,028 - mmaction - INFO - Epoch [32][20/33]\tlr: 2.470e-04, eta: 0:11:56, time: 8.535, data_time: 7.659, memory: 21540, top1_acc: 0.9391, top5_acc: 1.0000, loss_cls: 0.1854, loss: 0.1854, grad_norm: 3.5993\n",
      "2021-03-25 14:22:03,638 - mmaction - INFO - Saving checkpoint at 32 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 126/126, 3.5 task/s, elapsed: 36s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-25 14:22:39,733 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-03-25 14:22:39,735 - mmaction - INFO - \n",
      "top1_acc\t0.8571\n",
      "top5_acc\t1.0000\n",
      "2021-03-25 14:22:39,736 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-03-25 14:22:39,738 - mmaction - INFO - \n",
      "mean_acc\t0.8571\n",
      "2021-03-25 14:22:40,001 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_32.pth.\n",
      "2021-03-25 14:22:40,002 - mmaction - INFO - Best top1_acc is 0.8571 at 32 epoch.\n",
      "2021-03-25 14:22:40,003 - mmaction - INFO - Epoch(val) [32][33]\ttop1_acc: 0.8571, top5_acc: 1.0000, mean_class_accuracy: 0.8571\n",
      "2021-03-25 14:25:26,258 - mmaction - INFO - Epoch [33][20/33]\tlr: 1.249e-04, eta: 0:07:44, time: 8.313, data_time: 7.438, memory: 21540, top1_acc: 0.9141, top5_acc: 1.0000, loss_cls: 0.2269, loss: 0.2269, grad_norm: 4.7388\n",
      "2021-03-25 14:30:13,231 - mmaction - INFO - Epoch [34][20/33]\tlr: 4.337e-05, eta: 0:04:21, time: 8.550, data_time: 7.676, memory: 21540, top1_acc: 0.9359, top5_acc: 0.9984, loss_cls: 0.1936, loss: 0.1936, grad_norm: 3.9073\n",
      "2021-03-25 14:32:00,443 - mmaction - INFO - Saving checkpoint at 34 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 126/126, 3.5 task/s, elapsed: 36s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-25 14:32:36,374 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-03-25 14:32:36,376 - mmaction - INFO - \n",
      "top1_acc\t0.8651\n",
      "top5_acc\t1.0000\n",
      "2021-03-25 14:32:36,376 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-03-25 14:32:36,378 - mmaction - INFO - \n",
      "mean_acc\t0.8651\n",
      "2021-03-25 14:32:36,670 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_34.pth.\n",
      "2021-03-25 14:32:36,671 - mmaction - INFO - Best top1_acc is 0.8651 at 34 epoch.\n",
      "2021-03-25 14:32:36,672 - mmaction - INFO - Epoch(val) [34][33]\ttop1_acc: 0.8651, top5_acc: 1.0000, mean_class_accuracy: 0.8651\n",
      "2021-03-25 14:35:22,905 - mmaction - INFO - Epoch [35][20/33]\tlr: 3.904e-06, eta: 0:01:12, time: 8.312, data_time: 7.438, memory: 21540, top1_acc: 0.8984, top5_acc: 0.9984, loss_cls: 0.2482, loss: 0.2482, grad_norm: 4.5644\n",
      "2021-03-25 14:40:10,661 - mmaction - INFO - Epoch [36][20/33]\tlr: 5.144e-04, eta: -1 day, 23:58:11, time: 8.546, data_time: 7.670, memory: 21540, top1_acc: 0.9000, top5_acc: 1.0000, loss_cls: 0.2288, loss: 0.2288, grad_norm: 4.6857\n",
      "2021-03-25 14:41:57,419 - mmaction - INFO - Saving checkpoint at 36 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 126/126, 3.6 task/s, elapsed: 35s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-25 14:42:33,137 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-03-25 14:42:33,139 - mmaction - INFO - \n",
      "top1_acc\t0.8810\n",
      "top5_acc\t1.0000\n",
      "2021-03-25 14:42:33,139 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-03-25 14:42:33,141 - mmaction - INFO - \n",
      "mean_acc\t0.8810\n",
      "2021-03-25 14:42:33,430 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_36.pth.\n",
      "2021-03-25 14:42:33,431 - mmaction - INFO - Best top1_acc is 0.8810 at 36 epoch.\n",
      "2021-03-25 14:42:33,431 - mmaction - INFO - Epoch(val) [36][33]\ttop1_acc: 0.8810, top5_acc: 1.0000, mean_class_accuracy: 0.8810\n",
      "2021-03-25 14:45:18,239 - mmaction - INFO - Epoch [37][20/33]\tlr: 6.068e-04, eta: -1 day, 23:55:14, time: 8.240, data_time: 7.367, memory: 21540, top1_acc: 0.9187, top5_acc: 1.0000, loss_cls: 0.2261, loss: 0.2261, grad_norm: 4.7761\n",
      "2021-03-25 14:50:03,231 - mmaction - INFO - Epoch [38][20/33]\tlr: 7.817e-04, eta: -1 day, 23:52:19, time: 8.473, data_time: 7.601, memory: 21540, top1_acc: 0.9125, top5_acc: 1.0000, loss_cls: 0.2332, loss: 0.2332, grad_norm: 4.8438\n",
      "2021-03-25 14:51:59,104 - mmaction - INFO - Saving checkpoint at 38 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 126/126, 3.6 task/s, elapsed: 35s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-25 14:52:34,766 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-03-25 14:52:34,767 - mmaction - INFO - \n",
      "top1_acc\t0.8810\n",
      "top5_acc\t1.0000\n",
      "2021-03-25 14:52:34,768 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-03-25 14:52:34,769 - mmaction - INFO - \n",
      "mean_acc\t0.8810\n",
      "2021-03-25 14:52:34,770 - mmaction - INFO - Epoch(val) [38][33]\ttop1_acc: 0.8810, top5_acc: 1.0000, mean_class_accuracy: 0.8810\n",
      "2021-03-25 14:55:23,918 - mmaction - INFO - Epoch [39][20/33]\tlr: 1.032e-03, eta: -1 day, 23:49:25, time: 8.457, data_time: 7.586, memory: 21540, top1_acc: 0.9187, top5_acc: 1.0000, loss_cls: 0.2099, loss: 0.2099, grad_norm: 5.1517\n",
      "2021-03-25 15:00:04,888 - mmaction - INFO - Epoch [40][20/33]\tlr: 1.349e-03, eta: -1 day, 23:46:33, time: 8.332, data_time: 7.458, memory: 21540, top1_acc: 0.9000, top5_acc: 1.0000, loss_cls: 0.2583, loss: 0.2583, grad_norm: 4.9323\n",
      "2021-03-25 15:02:03,446 - mmaction - INFO - Saving checkpoint at 40 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 126/126, 3.5 task/s, elapsed: 36s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-25 15:02:39,524 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-03-25 15:02:39,526 - mmaction - INFO - \n",
      "top1_acc\t0.8810\n",
      "top5_acc\t1.0000\n",
      "2021-03-25 15:02:39,526 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-03-25 15:02:39,527 - mmaction - INFO - \n",
      "mean_acc\t0.8810\n",
      "2021-03-25 15:02:39,528 - mmaction - INFO - Epoch(val) [40][33]\ttop1_acc: 0.8810, top5_acc: 1.0000, mean_class_accuracy: 0.8810\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "from mmaction.datasets import build_dataset\n",
    "from mmaction.models import build_model\n",
    "from mmaction.apis import train_model\n",
    "\n",
    "import mmcv\n",
    "\n",
    "# Build the dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# Build the recognizer\n",
    "model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.train_cfg, test_cfg=cfg.test_cfg)\n",
    "\n",
    "# Create work_dir\n",
    "mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
    "train_model(model, datasets, cfg, distributed=False, validate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f\"{cfg.work_dir}/model50e\", 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ryVoSfZVmogw",
    "tags": []
   },
   "source": [
    "## Test the trained recognizer\n",
    "\n",
    "After finetuning the recognizer, let's check the prediction results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eyY3hCMwyTct",
    "outputId": "200e37c7-0da4-421f-98da-41418c3110ca",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 126/126, 2.2 task/s, elapsed: 56s, ETA:     0s\n",
      "Evaluating top_k_accuracy ...\n",
      "\n",
      "top1_acc\t0.9444\n",
      "top5_acc\t1.0000\n",
      "\n",
      "Evaluating mean_class_accuracy ...\n",
      "\n",
      "mean_acc\t0.9444\n",
      "top1_acc: 0.9444\n",
      "top5_acc: 1.0000\n",
      "mean_class_accuracy: 0.9444\n"
     ]
    }
   ],
   "source": [
    "from mmaction.apis import single_gpu_test\n",
    "from mmaction.datasets import build_dataloader\n",
    "from mmcv.parallel import MMDataParallel\n",
    "\n",
    "# Build a test dataloader\n",
    "dataset = build_dataset(cfg.data.test, dict(test_mode=True))\n",
    "data_loader = build_dataloader(\n",
    "        dataset,\n",
    "        videos_per_gpu=16,\n",
    "        workers_per_gpu=cfg.data.workers_per_gpu,\n",
    "        dist=False,\n",
    "        shuffle=False)\n",
    "model = MMDataParallel(model, device_ids=[0])\n",
    "outputs = single_gpu_test(model, data_loader)\n",
    "\n",
    "eval_config = cfg.evaluation\n",
    "eval_config.pop('interval')\n",
    "eval_res = dataset.evaluate(outputs, **eval_config)\n",
    "for name, val in eval_res.items():\n",
    "    print(f'{name}: {val:.04f}')\n",
    "    \n",
    "output_config = cfg.output_config\n",
    "dataset.dump_results(outputs, **output_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVoAAAD8CAYAAAA2Y2wxAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAApi0lEQVR4nO3deZwU5bn28d/VLOICqKDiAEdQTIJxjYDmIAmLiokgRGXUiMG4jBgVOEmA5D0oxxyNnkSMqEEloKCJBpQYFTBqcAFcGQQVhyUiBmHAJZEoizrM3O8fVYzDhKGre3qpbu9vPvWhu7q76kr1eM8zTz1Vj8wM55xz2ZPIdwDnnCt2Xmidcy7LvNA651yWeaF1zrks80LrnHNZ5oXWOeeyzAutc841QNLdkt6XtKzOumMlvSRpqaRyST2SbccLrXPONWwacFq9db8CrjWzY4Frwue75YXWOecaYGbzgX/WXw20Ch+3BiqTbadphnP9m6oP347lpWd7lvTKdwTnXD3bP1+vxm4jlZrT/IDDLgPK6qyabGaTk3xsFPCEpJsIGqv/mWw/WS+0zjkXV2FRTVZY67sc+C8zmyWpFJgKnLy7D3jXgXOuuNRUR1/SMwz4U/j4QSDpyTBv0Trnikv19mzvoRL4NvAs0Bf4W7IPeKF1zhUVs5qMbUvSA0BvoK2kdcB44FJgoqSmwKfs3Me7S15onXPFpSZzhdbMzmvgpeNT2Y4XWudccclgizZTvNA654pL+ie5ssYLrXOuuHiL1jnnssuyP+ogZV5onXPFJYMnwzLFC61zrrjEsOsgtleGjfvlzXzr9HMZPHR47boVq1bz/UtHcdawKyi9aARvVKzMY8JA/1N78+ay+ayoWMiY0VfkO06tuOaC+GbzXKmJa64cXBmWstgW2sHfPYU7b75up3UTJk3l8ovOZ9b033LlJUOZMGlqntIFEokEt068ngEDh3LUMX0455zBdO16eF4zxTkXxDeb5yqOXEDQoo265EhsC223Y4+idauWO62TxOYtWwHYvGUrB7Ztk49otXp0P47Vq99hzZq1VFVVMXPmI5wxsH9eM8U5F8Q3m+cqjlxAcAlu1CVHIhVaSUfsYl3vTIdJZuzIy5gwaSr9vncBN90+hVHDL8x1hJ2UtG/Hu+u+uBXluvUbKClpl8dEgbjmgvhm81ypiWsuIDgZFnXJkagt2pmSxiqwp6TbgBsaerOksnCKh/Ip9z6QmaTAjIfnMPaqMuY9fB9jRpRxzQ23ZGzbzrniYFYdecmVqIX2BKAj8AKwiODuNT0berOZTTazbmbW7ZIfNHSpcOoeffyvnNw72G3/vr3yfjKscv1GOnYoqX3eof3BVFZuzGOiQFxzQXyzea7UxDUXUNB9tFXANmBPoAWwxjJ5i5yIDmjbhkVL3gDg5cVLOaRj+1xH2Mmi8qV06dKZTp060qxZM0pLB/HY7CfzminOuSC+2TxXceQCYtl1EHUc7SLgEaA70Ba4U9JZZjYkW8FGj7+RRUteZ9Omj+k3eCg/uvgCrh07ghsn3sX26mr2aN6c8WNGZGv3kVRXVzNy1DjmzrmfJokE06bPoKJiVV4zxTkXxDeb5yqOXEAsx9HKLPn0OpK6mVl5vXUXmNl9yT7rc4Y556LKxJxhn77yYOSa06LHkEbvL4qoLdrXJI0AvhU+fxa4KyuJnHOuMTLYJSDpbmAA8L6ZHVln/VXAFUA1MMfMxuxuO1EL7R1AM2BS+PyC8PGlKeZ2zrnsymzXwTTgduDeHSsk9QEGAceY2WeSDky2kaiFtruZHVPn+dOSXkshrHPO5UZmZ1iYL6lTvdWXAzea2Wfhe95Ptp2oow6qJR2244mkQwmazM45Fy/ZH3XwFaCXpJclPSepe7IPRG3RjgaekfR2+LwT8MP0MjrnXPZYdVXk90oqY+fJFSeb2eQkH2sK7A+cSDASa6akQ203IwuiFtrnCU5+9QM2AU8AL0b8rHPO5U4KfbRhUU1WWOtbB/wpLKyvSKohGPb6QUMfiNp1cC/QGfhf4DbgUCDp0C7nnMu57Hcd/BnoAyDpK0Bz4MPdfSBqi/ZIM6t7Y5lnJFWkk9A557Iqg6MOJD0A9AbaSloHjAfuBu6WtAz4HBi2u24DiF5oX5V0opm9FO78BKA8yWeccy73MjvqoKGbtQxNZTu7LbSS3gCMYAztC5LWhs8PAVaksiPnnMuJGF6Cm6xFO6CxO4jrpa7bKhfkO8IuxfV4OVcwthfYLLhm9vdcBXHOuYwowBatc84VFp9u3DnnssxbtM45l2XeonXOuSzzFq1zzmVZoY06cM65ghNh1phc80LrnCsu3kfrnHNZ5oXWOeeyzE+GOedcllXHb/KXqPejzbv+p/bmzWXzWVGxkDGjr8hbjnG/vJlvnX4ug4cOr123YtVqvn/pKM4adgWlF43gjYqVecu3Q1yO167ENZvnSk1cc+XgfrQpK4hCm0gkuHXi9QwYOJSjjunDOecMpmvXw/OSZfB3T+HOm6/bad2ESVO5/KLzmTX9t1x5yVAmTJqal2w7xOl41RfXbJ6rOHIBXmjT1aP7caxe/Q5r1qylqqqKmTMf4YyB/fOSpduxR9G6Vcud1kli85atAGzespUD27bJR7RacTpe9cU1m+cqjlxA0EcbdcmRyIVWUnNJR0s6SlLzbIaqr6R9O95dV1n7fN36DZSUtMtlhN0aO/IyJkyaSr/vXcBNt09h1PAL85onzscrrtk8V2rimgvAaizykoykuyW9H86mUP+1n0gySW2TbSdSoZV0OrAauBW4HXhL0nd28/4ySeWSymtqtkTZRUGb8fAcxl5VxryH72PMiDKuueGWfEdy7ssrs10H04DT6q+U1BE4FVgbZSNRW7QTgD5m1tvMvk0wMdlvGnqzmU02s25m1i2R2DviLhpWuX4jHTuU1D7v0P5gKis3Nnq7mfLo43/l5N49Aejft1feT4bF+XjFNZvnSk1ccwHBqIOoSxJmNh/45y5e+g0whmDGmaSiFtpPzOytOs/fBj6J+NlGW1S+lC5dOtOpU0eaNWtGaekgHpv9ZK52n9QBbduwaMkbALy8eCmHdGyf1zxxPl5xzea5iiMXkFKLtu5f3+FSlmzzkgYB683staiRoo6jLZc0F5hJUMGHAIsknQlgZn+KusN0VFdXM3LUOObOuZ8miQTTps+gomJVNnfZoNHjb2TRktfZtOlj+g0eyo8uvoBrx47gxol3sb26mj2aN2f8mBF5ybZDnI5XfXHN5rmKIxeQ0mgCM5sMTI76fkl7Af+PoNsgMiWZJXfHxu/ZzctmZhc19GLT5u3jd4cHfM4w5+Jo++fr1dhtbL3lssg1Z69RdyXdn6ROwGwzO1LSUcA8YGv4cgegEuhhZg32nURq0ZrZD6O8zznn8i6L42PN7A3gwB3PJb0DdDOzD3f3uUiFVlIL4GLg60CLOjttsCXrnHN5EWHYVlSSHgB6A20lrQPGm1nKVyRF7aO9D1gB9Ad+AZwPLE91Z845l3UZvNeBmZ2X5PVOUbYTddRBFzO7GthiZtOB04ETIn7WOedyxmpqIi+5ErVFWxX+u0nSkcBG6vRTOOdcbGSw6yBTohbayZL2A64GHgX2Aa7JWirnnEtXod6P1symhA+fAw7NXhznnGukQmvRSvrx7l43s5szG8c55xppe/xu/J2sRbvjfoAG1B/YG79fG845V2hdB2Z2LYCk6cBIM9sUPt+P4EYzzjkXL4XWdVDH0TuKLICZfSTpuOxEyo24XurqlwY71zi5HLYVVdRCm5C0n5l9BCBp/xQ+65xzuVPALdoJwIuSHgyfDwGuz04k55xrhEIttGZ2r6RyoG+46kwzq8heLOecS1MMpxuP/Od/WFi9uDrnYi3KXGC55v2szrni4oXWOeeyrIBHHTjnXGGIYYs26m0SnXOuMNRY9CUJSXdLel/Ssjrrfi1phaTXJT0sad9k2/FC65wrKlZdE3mJYBpwWr11TwFHmtnRwCrg58k24oXWOVdcMtiiNbP5wD/rrXvSzLaHT18imKBxt7zQOueKitVY5EVSmaTyOktZiru7CHg82ZsKptD2P7U3by6bz4qKhYwZfUW+49SKU65xv7yZb51+LoOHDq9dt2LVar5/6SjOGnYFpReN4I2KlXlMGIjTMavLc6UmrrlSadGa2WQz61ZnmRx1N5L+G9gO/CHZewui0CYSCW6deD0DBg7lqGP6cM45g+na9fB8x4pdrsHfPYU7b75up3UTJk3l8ovOZ9b033LlJUOZMCnlCTwzKm7HzHMVVy4AalJY0iTpQmAAcL6ZJe2DKIhC26P7caxe/Q5r1qylqqqKmTMf4YyB/fMdK3a5uh17FK1btdxpnSQ2b9kKwOYtWzmwbZt8RKsVt2PmuYorF4Btr4m8pEPSacAY4Awz2xrlM5EKraTWkn5Tpx9jgqTWaaVMQ0n7dry7rrL2+br1GygpaZer3TcorrnqGjvyMiZMmkq/713ATbdPYdTwC/OaJ67HzHOlJq65gIy2aCU9ALwIfFXSOkkXA7cTTIrwlKSlku5Mtp2oFyzcDSwDSsPnFwD3AGc2EK4MKANQk9YkEntH3I3LtBkPz2HsVWWc0uck/jJvPtfccAtTJt6Q71jOZU0m73VgZuftYnXK/W9Ruw4OM7PxZvZ2uFzLbiZprNvBnIkiW7l+Ix07lNQ+79D+YCorNzZ6u40V11x1Pfr4Xzm5d08A+vftlfeTYXE9Zp4rNXHNBeSkjzZVUQvtNkkn7XgiqSewLTuR/t2i8qV06dKZTp060qxZM0pLB/HY7CdztfuCy1XXAW3bsGjJGwC8vHgph3Rsn9c8cT1mnqs4ckFqw7tyJWrXwXDg3jr9sh8Bw7IT6d9VV1czctQ45s65nyaJBNOmz6CiYlWudl8wuUaPv5FFS15n06aP6Td4KD+6+AKuHTuCGyfexfbqavZo3pzxY0bkLR/E75h5ruLKBeS0pRqVIoxMqDvt+D7hv5uBfwGLzWzp7j7btHn7+N3hIcZ8zjD3Zbb98/X1Z9tO2T9O/3bkmtNmznON3l8UUbsOuhG0alsBrYHLCK7//Z2kMVnK5pxzKbOa6EuuRO066AB8w8w2A0gaD8wBvgUsBn6VnXjOOZeiGHYdRC20BwKf1XleBRxkZtskfdbAZ5xzLudy2VKNKmqh/QPwsqRHwucDgfsl7Y3PI+aci5GCLbRm9r+SHgd6hquGm1l5+Pj8rCRzzrk0WHVOzm+lJJVZcMuB8qRvdM65PCrYFq1zzhUKqyngFq1zzhUCb9E651yWmXmL1jnnsspbtC6puF7quuW13+c7QoP+48TL8x1hl/6x7ZN8R/hSqonhqIOCmGHBOeeishpFXpKRdLek9yUtq7Nuf0lPSfpb+O9+ybbjhdY5V1QyWWiBaQT3danrZ8A8MzscmBc+3y0vtM65omIWfUm+LZsP/LPe6kHA9PDxdGBwsu14H61zrqikMo627rRbockRphw/yMw2hI83Agcl248XWudcUUlleFdYVJMV1t193iQlbRt7oXXOFZXq7I86eE/SwWa2QdLBwPvJPuB9tM65omKmyEuaHuWLqbyGAY/s5r2At2idc0Umk/c6kPQA0BtoK2kdMB64EZgp6WLg70Bpsu14oXXOFZUoowmib8vOa+Clfqlsxwutc66o+N27nHMuy6pr4nfqKX6JGtD/1N68uWw+KyoWMmb0FfmOU8tzJXfNbffy7WGj+d6IX9SuG/3r3zFk1HUMGXUdp136/xgy6ro8JoRbbr+eN996nudefDSvOXYlTt9lXXHNlckLFjKlIAptIpHg1onXM2DgUI46pg/nnDOYrl0Pz3cszxXRGX2/yR3XXLXTul+PvpQHbxnHg7eM4+RvfoN+3zwuT+kCf7z/Yc4969K8ZtiVuH2Xcc8FUGOKvORK0kIr6cxdLP0kHZiLgAA9uh/H6tXvsGbNWqqqqpg58xHOGNg/V7v3XI3U7euH03qfvXb5mpnxxPOL+U6vbjlOtbOXXihn00f/ymuGXYnbdxn3XJCT4V0pi9KivRiYQjAJ4/nA74CxwPOSLshitlol7dvx7rrK2ufr1m+gpKRdLna9W56r8RZXvEWbfVtySEnSqxi/lOL6XcY1F8Sz6yDKybCmQFczew9A0kHAvcAJwHzgvvofqHv9sJq0JpHYO2OBXXF5fMEivtOre75juCKSyy6BqKK0aDvuKLKh98N1/wSqdvUBM5tsZt3MrFsmimzl+o107FBS+7xD+4OprNzY6O02ludqnO3V1cx7cQn9T8pvt0GcxfW7jGsuCEYdRF1yJcqenpU0W9IwScMILj97VtLewKaspgstKl9Kly6d6dSpI82aNaO0dBCPzX4yF7v2XFn00msr6NyhHe3aJr1v8pdWXL/LuOYCsBSWXInSdXAFcCZwUvh8OjDLzAzok61gdVVXVzNy1DjmzrmfJokE06bPoKJiVS527bkyYMyEKZQvW8Wmjzdz8sU/40fnDuTMU3rylxh1G9w5dQL/eVJ39m+zH0sqnuXXN9zG/ffNynes2H2Xcc8F8ew6kEXoEQ77ZXsQ/BJ4xcyS3q1mh6bN2+fyF4fLEp8zLHU+Z1jqtn++vtFV8vl2Z0euOT03PpSTqhxleFcp8ApwNsHNE16WdHa2gznnXDpqUlhyJUrXwX8D3Xe0YiUdAPwVeCibwZxzLh1G/LoOohTaRL2ugn9QIFeUOee+fLbHsI82SqH9i6QngAfC5+cCj2cvknPOpa8gW7RmNlrSmUDPcNWdZvbnrKZyzrk0ZbLvVdJ/AZcQDAR4A/ihmX2a6nYaLLSSFprZSZI+CXey49dEmaQagil4f21mk1JO75xzWZKpFq2k9sAI4Agz2yZpJsFf9NNS3VaDhdbMTgr/bdlAiDbAC4AXWudcbGR4NEFTYE9JVcBeQGWS9+9S2ie1zOwfBHPpOOdcbFSjyIukMknldZayHdsxs/XATcBaYAPwLzNL6/K3Rs2wYGYbGvN555zLtFRmsjGzycDkXb0maT9gENCZ4HYDD0oaamYpX73jw7Scc0WlBkVekjgZWGNmH5hZFfAn4D/TyeRzhrlI+p9yfb4jNOidqUPzHWGXWn7/jnxH+FLK4DX/a4ETJe0FbCOY+bY8nQ15oXXOFZVMnQwzs5clPQS8CmwHltBAN0MyXmidc0WlRpm7YMHMxgPjG7sdL7TOuaJSne8Au+CF1jlXVFIZdZArXmidc0UlwmiCnPNC65wrKnGcacALrXOuqHjXgXPOZVkuZ06Iygutc66oVHuL1jnnsstbtM45l2VxLLQFc1OZ/qf25s1l81lRsZAxo6/Id5xanis1zfdoxh2zb2fKk3dxz7wpXPiTH+Qty/hZL9DnlzM5a+KjtevumPcap9z4EKW3zab0ttksWLk+b/l2iOt3GddcpuhLrhREizaRSHDrxOs57bvnsW7dBl56cS6PzX6S5cv/5rkKKBfA559V8ePSn7Jt66c0adqE2x6+hVeeWUTFq8tznuWMbxzGuSd+lXEPPb/T+qE9uzKs19dznmdX4vpdxjUXeIs2bT26H8fq1e+wZs1aqqqqmDnzEc4Y2D/fsTxXmrZtDaZcatq0KU2bNsUsPyMfj+98EK322iMv+44qrt9lXHNBcAlu1CVXCqLQlrRvx7vrvphBYt36DZSUtMtjooDnSk8ikWDKE3fy59ceonzBYpYvWZHvSDv540srGXLrY4yf9QIfb/ssr1ni+l3GNRcE42ijLrkSqdBKOlPS3yT9S9LHkj6R9PFu3l87PURNzZbMpXVFoaamhkv6D2dI93PpeuzX6PzVTvmOVKv0hK8w+yeDmXHlANq23JMJcxfnO5JLUU0KS65EbdH+CjjDzFqbWSsza2lmrRp6s5lNNrNuZtYtkdi70SEr12+kY4eS2ucd2h9MZeXGRm+3sTxX42z+eAtLXlhKj97d8x2lVpt99qRJIkEiIc7sfjjL1n2Y1zxx/S7jmgsKu9C+Z2a5P1sRWlS+lC5dOtOpU0eaNWtGaekgHpud1hxpnivPWu/fmn1aBb98m7doTrdex7P2rbV5TvWFDz7eWvv46Yq1dDlo3/yFIb7fZVxzQXCvg6hLMpL2lfSQpBWSlkv6ZjqZoo46KJc0A/gzUNtpZWZ/SmenqaqurmbkqHHMnXM/TRIJpk2fQUXFqlzs2nNlWJuD9ufnvxlLokmChMQzs5/jxXkv5yXLz2YsoPzt99i09VNO/b9ZXN7vaMrXvMfKDR8hoGS/fRg36IS8ZNshrt9lXHNBxvteJwJ/MbOzJTUnmHI8ZYpyxlfSPbtYbWZ2UbLPNm3ePo4303EpOunArvmO0KDHb+md7wi75HOGpW775+sbXSZvOGRo5Jrz87//vsH9SWoNLAUOtUYOjYnUojWzHzZmJ845lys1KdwoUVIZUFZn1eRwCnIIphn/ALhH0jHAYmCkmaV8hj9SoQ1btP+WPkqL1jnncimVk1xhUW1owsWmwDeAq8KJGicCPwOuTjVT1D7a2XUetwC+B1Q28F7nnMubDPZVrgPWmdmOkwgPERTalEXtOphV97mkB4CF6ezQOeeyKYPTjW+U9K6kr5rZSqAfUJHOttK918HhwIFpftY557JmuzJ6/v0q4A/hiIO3gbTOVyUttJJEcFnw5jqrNwJj09mhc85lUybLrJktBbo1djtJC62ZmaQKMzuysTtzzrlsK+S7dy2WFJ/rJJ1zrgE1WOQlV6L20Z4AnC/p78AWQASN3aOzlsw559IQxyukohbaeNxo0jnnkohj10HU4V1/z3YQF28L318e28tw43qp6yf3X57vCLsU1+OVKdUxbNMWxFQ2Lv/iWmSdq69gW7TOOVcozFu0zjmXXd6idc65LMvlsK2ovNA654pK/MqsF1rnXJHZHsNS64XWOVdU/GSYc85lmZ8Mc865LPMWrXPOZZm3aJ1zLsuqGzdh7b+R1AQoB9ab2YB0thH1Nol51//U3ry5bD4rKhYyZvQV+Y5Ty3Olpvkezbhj9u1MefIu7pk3hQt/8oN8R6oVl2M2ftYL9PnlTM6a+GjtujvmvcYpNz5E6W2zKb1tNgtWrs9bvh3icrzqy8JtEkcCyxuTqSBatIlEglsnXs9p3z2Pdes28NKLc3ls9pMsX/43z1VAuQA+/6yKH5f+lG1bP6VJ0ybc9vAtvPLMIipebdTPcaPF6Zid8Y3DOPfErzLuoed3Wj+0Z1eG9fp6zvPsSpyOV32Z7KOV1AE4Hbge+HG62ymIFm2P7sexevU7rFmzlqqqKmbOfIQzBub/zo2eKz3btn4KQNOmTWnatCmW4T/10hGnY3Z854Notdceedl3VHE6XvXVpLBIKpNUXmcpq7e5W4AxNLLrN1KhlfSdXawb3pgdp6KkfTveXffF7Obr1m+gpKRdrnbfIM+VnkQiwZQn7uTPrz1E+YLFLF+yIt+RYn/MAP740kqG3PoY42e9wMfbPstrljgfr1S6Dsxsspl1q7NM3rEdSQOA981scWMzRW3RXi2pb50AY4BBDb257m+Jmpotjc3oikxNTQ2X9B/OkO7n0vXYr9H5q53yHSn2Sk/4CrN/MpgZVw6gbcs9mTC30f/tFy1L4X9J9ATOkPQO8Eegr6Tfp5MpaqE9A/ilpF6SrieY2qbBQlv3t0QisXc6uXZSuX4jHTuU1D7v0P5gKis3Nnq7jeW5Gmfzx1tY8sJSevTO/3R0cT9mbfbZkyaJBImEOLP74Sxb92Fe88T5eFWbRV52x8x+bmYdzKwTcC7wtJkNTSdTpEJrZh8SFNvfAiXA2Wb2eTo7TMei8qV06dKZTp060qxZM0pLB/HY7CdztXvPlUGt92/NPq2CX77NWzSnW6/jWfvW2jynivcxA/jg4621j5+uWEuXg/bNXxjifbwKbnJGSZ8Q3AxH4b/NgUOBsyWZmbXKfkSorq5m5KhxzJ1zP00SCaZNn0FFxapc7NpzZVibg/bn578ZS6JJgoTEM7Of48V5L+c7VqyO2c9mLKD87ffYtPVTTv2/WVze72jK17zHyg0fIaBkv30YN+iEvGTbIU7Hq75sXLBgZs8Cz6b7eWX7jG/T5u3zf0rZNVqcp7JZ+H5+h4Y1xOcMS932z9ersdsY8B+nR645s9fOafT+okjWov3G7l43s1czG8c55xqnEG/8PWE3rxnQdzevO+dczsVhXHZ9uy20ZtYnV0Gccy4TCnq6cUlHAkcALXasM7N7sxHKOefSVYhdBwBIGg/0Jii0c4HvAAsBL7TOuViJY9dB1AsWzgb6ARvN7IfAMUDrrKVyzrk0Fdw42jo+NbMaSdsltQLeBzpmMZdzzqWlkGdYWCRpX+B3wGJgM/BitkI551y6Mn3j70yIWmhbAUMIroz4C9DKzF7PVijnnEtXwZ4MA6YCvYDbgMOAJZLmm9nErCVzzrk0xLHQRr4EN5w3pzvQBxgObDOzryX7nF+C61y8bKtckO8IDWrW9tBGXxJ7YknvyDXnpcpn838J7g6S5gF7E/TLLgC6m9n72QzmnHPpiGOLNurwrteBz4EjgaOBIyXtmbVUzjmXpgze+DtjIrVozey/ACS1BC4E7gHaAfGe2Mg596VTbdm4UWLjRO06uJLgZNjxwDvA3QRdCM45FyuZujJMUkeCq18PIriJ1uR0BwBEHXXQArgZWGxm29PZkXPO5UIG+2i3Az8xs1fDv+YXS3rKzCpS3VDUroObUt2wc87lQ6b6Xs1sA7AhfPyJpOVAeyA7hdY55wpFTRauDJPUCTgOSGvepaijDpxzriCkMupAUpmk8jpLWf3tSdoHmAWMMrOP08nkLVrnXFFJZdSBmU0GJjf0uqRmBEX2D2b2p3QzeaF1zhWVTHUdSBLB7QeWm9nNjdmWdx0454pKBi9Y6AlcAPSVtDRcvptOpoIptP1P7c2by+azomIhY0Zfke84tTxX6uKazXMlN+6XN/Ot089l8NDhtetWrFrN9y8dxVnDrqD0ohG8UbEyjwmDFm3UZXfMbKGZycyONrNjw2VuOpkKotAmEglunXg9AwYO5ahj+nDOOYPp2vXwfMfyXGmIazbPFc3g757CnTdft9O6CZOmcvlF5zNr+m+58pKhTJg0NU/pAnG8BLcgCm2P7sexevU7rFmzlqqqKmbOfIQzBvbPdyzPlYa4ZvNc0XQ79ihat2q50zpJbN6yFYDNW7ZyYNs2+YhWq9qqIy+5EqnQStpL0tWSfhc+P1zSgOxG+0JJ+3a8u66y9vm69RsoKWmXq903yHOlLq7ZPFf6xo68jAmTptLvexdw0+1TGDX8wrzmMbPIS65EbdHeA3wGfDN8vh64rqE31x2bVlOzpZERnXNxNuPhOYy9qox5D9/HmBFlXHPDLXnNE8fJGaMW2sPM7FdAFYCZbQUavGGumU02s25m1i2R2LvRISvXb6Rjh5La5x3aH0xl5cZGb7exPFfq4prNc6Xv0cf/ysm9ewLQv2+vvJ8MK+QW7efh/WcNQNJhBC3cnFhUvpQuXTrTqVNHmjVrRmnpIB6b/WSudu+5Miiu2TxX+g5o24ZFS94A4OXFSzmkY/u85snUqINMinrBwv8QTMrYUdIfCMaXXZilTP+murqakaPGMXfO/TRJJJg2fQYVFatytXvPlUFxzea5ohk9/kYWLXmdTZs+pt/gofzo4gu4duwIbpx4F9urq9mjeXPGjxmRt3wQz+nGU5kzrA1wIkGXwUtm9mGUz/mcYc7FS7HPGXZA669Grjkf/GtlrOYMewy4H3jUzPzslnMutnLZ9xpV1D7amwhmWKiQ9JCksyW1yGIu55xLS8H20ZrZc8Bz4ZTjfYFLCaazaZXFbM45l7I4tmgj370rHHUwEDgH+AYwPVuhnHMuXXGcbjxqH+1MoAfByIPbgefMYjjVpHPuS6+QW7RTgfPMcnhxsHPOpaFgpxs3syckHSnpCIIZcXesvzdryZxzLg25PMkVVdSug/FAb+AIYC7wHWAhwZznzjkXG3HsOog6vOtsoB+w0cx+CBwDtM5aKuecS1Mm70cr6TRJKyW9Jeln6WaKWmg/DU9+bZfUCngf6JjuTp1zLlsydVOZcDjrbwn+gj8COC/sPk1Z1JNhiyTtC/wOWAxsBl5MZ4fOOZdNGeyj7QG8ZWZvA0j6IzAIqEh1Q1ELbStgCPAswRCvVmb2epQPbv98fcauJZZUFk4PHDtxzea5UhPXXBDfbHHLlUrNkVQGlNVZNbnO/5f2wLt1XlsHnJBOpqhdB1OBg4HbgKeB8ZJGprPDRipL/pa8iWs2z5WauOaC+GaLa66k6t47O1yy8gsj6vCuZyTNB7oDfYDhwNeBidkI5ZxzMbCenc9FdQjXpSzq8K55wN4E/bILgO5m9n46O3TOuQKxCDhcUmeCAnsu8P10NhS16+B14HPgSOBo4Mjw3ge5Fpt+oF2IazbPlZq45oL4ZotrrkYxs+3AlcATwHJgppm9mc62It/4G0BSS4KZFX4KtDOzPdLZqXPOfZlE7Tq4kuB+tMcD7xDcIjG+t2l3zrkYiTq8qwVwM7A4bE4755yLKFIfrZndZGYvZ7vISuokaVk295EJkv5H0k/znaMQSHoh3xmKkaRnJXULH2/Odx63e1FPhjmXFjP7z3xn2B0F/L8Dl1Vx/AFrKukPkpaH85PtJamfpCWS3pB0t6Q9JHWX9LqkFpL2lvSmpCOzEUjSD8J9vSbpvnqvXSppUfjaLEl7heunSbpTUrmkVZIGZCNbvSxXhzfAWCjpAUk/lXSspJfC/A9L2i/bOepl2hwWs19LWhZ+h+eEryUkTZK0QtJTkuZKOjsHmTqFx+leYBlQXee1syVNCx9Pk3SrpBckvZ2NbJJGSxoRPv6NpKfDx33D/w7uCH+G3pR0bZJttZX0oqTTc5knvPHKg3W20VvS7PDxqWGmVyU9KGmfdLMVtFRuwJDtBegEGNAzfH43MI7gMrivhOvuBUaFj68jmDjyt8DPs5Tp68AqoG34fH/gf4Cfhs/b1HnvdcBV4eNpBJcrJ4DDCS7fa5HFY9cdWErQn94S+BvB6JDXgW+H7/kFcEuOv9PNwFnAU0AT4CBgLcGVhmcT3HYzAbQDPgLOztHPWQ1w4o6MdV47G5hW5zt8MMx3BMF175nOciLwYPh4AfAK0AwYD1wG7B++1oTgEvijw+fPAt3qHOODgJeBU3Kdh+Bcz1pg7/C1O4ChQFtgfp31Y4FrcvnzF5clji3ad83s+fDx7wluz7jGzFaF66YD3wof/wI4BegG/CpLefoS/OB9CGBm/6z3+pGSFkh6AzifoDDvMNPMaszsb8DbwNeylBGgJ/CImX1qZp8AjxFcZLKvBZNrws7HLpdOAh4ws2ozew94juAXw0kEx7bGzDYCz+Qw09/N7KUI7/tzmK+CoJhl2mLgeAV3xfuM4KKgbgSjfBYApZJeBZYQ/Gzt6u5RzYB5wBgzeyrXeSw4d/MXYKCkpsDpwCMERfsI4HlJS4FhwCGNzFeQIk/OmEP1B/ZuAto08N42wD4EP2gtgC3Zi9WgacBgM3tN0oUEN0jfof7/l/jdkfjLq+7PSt3vpUW9931W53HGbpBUu2OzKklrCManv0DwF0gfoAuwjeCvku5m9lHYpVE/H8B2ggLZn+CXWD7y/JFgcP8/gXIz+0SSgKfM7LzGZCoGcWzR/oekb4aPvw+UA50kdQnXXcAXP0x3AVcDfwD+L0t5ngaGSGoDIGn/eq+3BDZIakbQoq1rSNgPeRhwKLAySxkBnidoUbQI+8EGEBSTjyT1Ct9T99jl0gLgHElNJB1A0Kp+Jcx8VniMDmLnX1K59J6krgpOin0vD/tfQFDA5oePhxO0GFsRfIf/Co/Pdxr4vAEXAV+TNDZPeZ4jmB37UoKiC/AS0HPHf7vhuZSvZCBfwYlji3YlcIWkuwnu+ziC4At7MPyzZBFwp6QfAFVmdr+CG/S+IKmvmT2dyTBm9qak64HnJFUT/MC9U+ctVxP0jX0Q/tuyzmtrCQpKK2C4mX2ayWz1ci6S9ChBC+Q94A3gXwR/rt0ZnqR7G/hhtjI0FA14GPgm8Fr4fIyZbZQ0i6BrqIKgH/7VMHOu/QyYTfAdlhP8lZRLC4D/Bl40sy2SPgUWhH8lLQFWEByf5xvagJlVSzoPeFTSJ2Y2KZd5wv3PJmgJDwvXfRD+lfeApB1XkY4jOOfxpZLSJbguuvDPqtlm9lAO97mPmW0Oi+p8oMzMXs3V/neRpw3wqpk12C9XJ3Mbgl9KPcP+WueKRhxbtC59k/XFTMXT81xkSwjOSt+U5K2zFcze0Rz4Xy+yrhh5i9Y557IsjifDnHOuqHihdc65LPNC65xzWeaF1jnnsswLrXPOZdn/By29O7x208H8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from mmaction.core import confusion_matrix \n",
    "\n",
    "gt_labels = [ann['label'] for ann in dataset.load_annotations()]\n",
    "pred = np.argmax(outputs, axis=1)\n",
    "cf_mat = confusion_matrix(pred, gt_labels).astype(float)\n",
    "\n",
    "sns.heatmap(cf_mat, annot=True, xticklabels = ['box', 'clap', 'go', 'jog', 'run', 'walk', 'wave'], yticklabels = ['box', 'clap', 'go', 'jog', 'run', 'walk', 'wave'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "# SlowFast 91.27%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "64CW6d_AaT-Q",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "3b284fd8-4ee7-4a34-90d7-5023cd123a04",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-03-22 18:16:35--  https://download.openmmlab.com/mmaction/recognition/slowfast/slowfast_r50_video_4x16x1_256e_kinetics400_rgb/slowfast_r50_video_4x16x1_256e_kinetics400_rgb_20200826-f85b90c5.pth\n",
      "Resolving download.openmmlab.com (download.openmmlab.com)... 47.254.186.225\n",
      "Connecting to download.openmmlab.com (download.openmmlab.com)|47.254.186.225|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 138274276 (132M) [application/octet-stream]\n",
      "Saving to: ‘checkpoints/slowfast_r50_video_4x16x1_256e_kinetics400_rgb_20200826-f85b90c5.pth’\n",
      "\n",
      "checkpoints/slowfas 100%[===================>] 131,87M  6,72MB/s    in 17s     \n",
      "\n",
      "2021-03-22 18:16:55 (7,79 MB/s) - ‘checkpoints/slowfast_r50_video_4x16x1_256e_kinetics400_rgb_20200826-f85b90c5.pth’ saved [138274276/138274276]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !mkdir checkpoints\n",
    "!wget -c https://download.openmmlab.com/mmaction/recognition/slowfast/slowfast_r50_video_4x16x1_256e_kinetics400_rgb/slowfast_r50_video_4x16x1_256e_kinetics400_rgb_20200826-f85b90c5.pth \\\n",
    "      -O checkpoints/slowfast_r50_video_4x16x1_256e_kinetics400_rgb_20200826-f85b90c5.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "LjCcmCKOjktc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mmcv import Config\n",
    "cfg = Config.fromfile('./configs/recognition/slowfast/slowfast_r50_video_4x16x1_256e_kinetics400_rgb.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "tlhu9byjjt-K",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "13d1bb01-f351-48c0-9efb-0bdf2c125d29",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "model = dict(\n",
      "    type='Recognizer3D',\n",
      "    backbone=dict(\n",
      "        type='ResNet3dSlowFast',\n",
      "        pretrained=None,\n",
      "        resample_rate=8,\n",
      "        speed_ratio=8,\n",
      "        channel_ratio=8,\n",
      "        slow_pathway=dict(\n",
      "            type='resnet3d',\n",
      "            depth=50,\n",
      "            pretrained=None,\n",
      "            lateral=True,\n",
      "            conv1_kernel=(1, 7, 7),\n",
      "            dilations=(1, 1, 1, 1),\n",
      "            conv1_stride_t=1,\n",
      "            pool1_stride_t=1,\n",
      "            inflate=(0, 0, 1, 1),\n",
      "            norm_eval=False),\n",
      "        fast_pathway=dict(\n",
      "            type='resnet3d',\n",
      "            depth=50,\n",
      "            pretrained=None,\n",
      "            lateral=False,\n",
      "            base_channels=8,\n",
      "            conv1_kernel=(5, 7, 7),\n",
      "            conv1_stride_t=1,\n",
      "            pool1_stride_t=1,\n",
      "            norm_eval=False)),\n",
      "    cls_head=dict(\n",
      "        type='SlowFastHead',\n",
      "        in_channels=2304,\n",
      "        num_classes=7,\n",
      "        spatial_type='avg',\n",
      "        dropout_ratio=0.5),\n",
      "    train_cfg=None,\n",
      "    test_cfg=dict(average_clips='prob'))\n",
      "checkpoint_config = dict(interval=2)\n",
      "log_config = dict(interval=40, hooks=[dict(type='TextLoggerHook')])\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = './childact-checkpoints/childact-slowfast3/best_top1_acc_epoch_5.pth'\n",
      "resume_from = './childact-checkpoints/childact-slowfast3/best_top1_acc_epoch_5.pth'\n",
      "workflow = [('train', 1)]\n",
      "dataset_type = 'VideoDataset'\n",
      "data_root = 'data/childact_split/train/'\n",
      "data_root_val = 'data/childact_split/val/'\n",
      "ann_file_train = 'data/childact_split/childact_train_video.txt'\n",
      "ann_file_val = 'data/childact_split/childact_val_video.txt'\n",
      "ann_file_test = 'data/childact_split/childact_test_video.txt'\n",
      "img_norm_cfg = dict(\n",
      "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_bgr=False)\n",
      "train_pipeline = [\n",
      "    dict(type='DecordInit'),\n",
      "    dict(type='SampleFrames', clip_len=32, frame_interval=2, num_clips=1),\n",
      "    dict(type='DecordDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='RandomResizedCrop'),\n",
      "    dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "    dict(type='Flip', flip_ratio=0.5),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "]\n",
      "val_pipeline = [\n",
      "    dict(type='DecordInit'),\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=32,\n",
      "        frame_interval=2,\n",
      "        num_clips=1,\n",
      "        test_mode=True),\n",
      "    dict(type='DecordDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='CenterCrop', crop_size=224),\n",
      "    dict(type='Flip', flip_ratio=0),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(type='DecordInit'),\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=32,\n",
      "        frame_interval=2,\n",
      "        num_clips=10,\n",
      "        test_mode=True),\n",
      "    dict(type='DecordDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='ThreeCrop', crop_size=256),\n",
      "    dict(type='Flip', flip_ratio=0),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "data = dict(\n",
      "    videos_per_gpu=16,\n",
      "    workers_per_gpu=4,\n",
      "    train=dict(\n",
      "        type='VideoDataset',\n",
      "        ann_file='data/childact_split/childact_train_video.txt',\n",
      "        data_prefix='data/childact_split/train/',\n",
      "        pipeline=[\n",
      "            dict(type='DecordInit'),\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=1),\n",
      "            dict(type='DecordDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='RandomResizedCrop'),\n",
      "            dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "            dict(type='Flip', flip_ratio=0.5),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "        ]),\n",
      "    val=dict(\n",
      "        type='VideoDataset',\n",
      "        ann_file='data/childact_split/childact_val_video.txt',\n",
      "        data_prefix='data/childact_split/val/',\n",
      "        pipeline=[\n",
      "            dict(type='DecordInit'),\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=1,\n",
      "                test_mode=True),\n",
      "            dict(type='DecordDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='CenterCrop', crop_size=224),\n",
      "            dict(type='Flip', flip_ratio=0),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ]),\n",
      "    test=dict(\n",
      "        type='VideoDataset',\n",
      "        ann_file='data/childact_split/childact_test_video.txt',\n",
      "        data_prefix='data/childact_split/test/',\n",
      "        pipeline=[\n",
      "            dict(type='DecordInit'),\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=10,\n",
      "                test_mode=True),\n",
      "            dict(type='DecordDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='ThreeCrop', crop_size=256),\n",
      "            dict(type='Flip', flip_ratio=0),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ]))\n",
      "evaluation = dict(\n",
      "    interval=1, metrics=['top_k_accuracy', 'mean_class_accuracy'])\n",
      "optimizer = dict(type='SGD', lr=0.001, momentum=0.9, weight_decay=0.0001)\n",
      "optimizer_config = dict(grad_clip=dict(max_norm=40, norm_type=2))\n",
      "lr_config = dict(\n",
      "    policy='CosineAnnealing',\n",
      "    min_lr=0,\n",
      "    warmup='linear',\n",
      "    warmup_by_epoch=True,\n",
      "    warmup_iters=34)\n",
      "total_epochs = 11\n",
      "work_dir = './childact-checkpoints/childact-slowfast4/'\n",
      "find_unused_parameters = False\n",
      "omnisource = False\n",
      "seed = 42\n",
      "gpu_ids = range(0, 1)\n",
      "output_config = dict(\n",
      "    out='./childact-checkpoints/childact-slowfast4//results.json')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mmcv.runner import set_random_seed\n",
    "\n",
    "# Modify dataset type and path\n",
    "cfg.dataset_type = 'VideoDataset'\n",
    "cfg.data_root = 'data/childact_split/train/'\n",
    "cfg.data_root_val = 'data/childact_split/val/'\n",
    "cfg.ann_file_train = 'data/childact_split/childact_train_video.txt'\n",
    "cfg.ann_file_val = 'data/childact_split/childact_val_video.txt'\n",
    "cfg.ann_file_test = 'data/childact_split/childact_test_video.txt'\n",
    "\n",
    "cfg.data.test.type = 'VideoDataset'\n",
    "cfg.data.test.ann_file = 'data/childact_split/childact_test_video.txt'\n",
    "cfg.data.test.data_prefix = 'data/childact_split/test/'\n",
    "\n",
    "cfg.data.train.type = 'VideoDataset'\n",
    "cfg.data.train.ann_file = 'data/childact_split/childact_train_video.txt'\n",
    "cfg.data.train.data_prefix = 'data/childact_split/train/'\n",
    "\n",
    "cfg.data.val.type = 'VideoDataset'\n",
    "cfg.data.val.ann_file = 'data/childact_split/childact_val_video.txt'\n",
    "cfg.data.val.data_prefix = 'data/childact_split/val/'\n",
    "\n",
    "# The flag is used to determine whether it is omnisource training\n",
    "cfg.setdefault('omnisource', False)\n",
    "# Modify num classes of the model in cls_head\n",
    "cfg.model.cls_head.num_classes = 7\n",
    "# We can use the pre-trained TSN model\n",
    "cfg.load_from = './childact-checkpoints/childact-slowfast3/best_top1_acc_epoch_5.pth'\n",
    "cfg.resume_from = './childact-checkpoints/childact-slowfast3/best_top1_acc_epoch_5.pth'\n",
    "\n",
    "# Set up working dir to save files and logs.\n",
    "cfg.work_dir = './childact-checkpoints/childact-slowfast4/'\n",
    "\n",
    "# The original learning rate (LR) is set for 8-GPU training.\n",
    "# We divide it by 8 since we only use one GPU.\n",
    "cfg.data.videos_per_gpu = 16\n",
    "# cfg.data.workers_per_gpu = 4\n",
    "# cfg.optimizer.type = 'Adam'\n",
    "# cfg.optimizer.weight_decay=0.0001\n",
    "\n",
    "# cfg.optimizer_config.grad_clip=None\n",
    "cfg.optimizer.lr = 0.001\n",
    "\n",
    "# cfg.lr_config = dict(\n",
    "#     policy='cyclic',\n",
    "#     target_ratio=(10, 1e-4),\n",
    "#     cyclic_times=1,\n",
    "#     step_ratio_up=0.4,\n",
    "# )\n",
    "cfg.total_epochs = 11\n",
    "cfg.evaluation.interval = 1\n",
    "# We can set the checkpoint saving interval to reduce the storage cost\n",
    "cfg.checkpoint_config.interval = 2\n",
    "# We can set the log print interval to reduce the the times of printing log\n",
    "cfg.log_config.interval = 40\n",
    "\n",
    "# Set seed thus the results are more reproducible\n",
    "cfg.seed = 42\n",
    "set_random_seed(42, deterministic=False)\n",
    "cfg.gpu_ids = range(1)\n",
    "cfg.output_config = dict(out=f'{cfg.work_dir}/results.json')\n",
    "# We can initialize the logger for training and have a look\n",
    "# at the final config used for training\n",
    "print(f'Config:\\n{cfg.pretty_text}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tES-qnZ3k38Z",
    "tags": []
   },
   "source": [
    "### Train a new recognizer\n",
    "\n",
    "Finally, lets initialize the dataset and recognizer, then train a new recognizer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "dDBWkdDRk6oz",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "85a52ef3-7b5c-4c52-8fef-00322a8c65e6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-25 12:40:03,663 - mmaction - INFO - load checkpoint from ./childact-checkpoints/childact-slowfast3/best_top1_acc_epoch_5.pth\n",
      "2021-03-25 12:40:03,664 - mmaction - INFO - Use load_from_local loader\n",
      "2021-03-25 12:40:03,874 - mmaction - INFO - resumed epoch 5, iter 330\n",
      "2021-03-25 12:40:03,876 - mmaction - INFO - Start running, host: actrec@actrec-HP-Z4-G4-Workstation, work_dir: /home/actrec/.virtualenvs/mmaction/mmaction2/childact-checkpoints/childact-slowfast4\n",
      "2021-03-25 12:40:03,877 - mmaction - INFO - workflow: [('train', 1)], max: 11 epochs\n",
      "2021-03-25 12:41:36,333 - mmaction - INFO - Epoch [6][40/66]\tlr: 1.416e-02, eta: 0:13:42, time: 2.311, data_time: 1.426, memory: 12207, top1_acc: 0.7500, top5_acc: 0.9844, loss_cls: 0.6193, loss: 0.6193, grad_norm: 0.9436\n",
      "2021-03-25 12:42:28,631 - mmaction - INFO - Saving checkpoint at 6 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 126/126, 6.4 task/s, elapsed: 20s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-25 12:42:48,742 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-03-25 12:42:48,743 - mmaction - INFO - \n",
      "top1_acc\t0.8254\n",
      "top5_acc\t1.0000\n",
      "2021-03-25 12:42:48,744 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-03-25 12:42:48,744 - mmaction - INFO - \n",
      "mean_acc\t0.8254\n",
      "2021-03-25 12:42:49,135 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_6.pth.\n",
      "2021-03-25 12:42:49,136 - mmaction - INFO - Best top1_acc is 0.8254 at 6 epoch.\n",
      "2021-03-25 12:42:49,137 - mmaction - INFO - Epoch(val) [6][66]\ttop1_acc: 0.8254, top5_acc: 1.0000, mean_class_accuracy: 0.8254\n",
      "2021-03-25 12:44:22,536 - mmaction - INFO - Epoch [7][40/66]\tlr: 1.177e-02, eta: 0:08:28, time: 2.335, data_time: 1.443, memory: 12207, top1_acc: 0.6984, top5_acc: 0.9906, loss_cls: 0.7002, loss: 0.7002, grad_norm: 1.1397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 126/126, 6.4 task/s, elapsed: 20s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-25 12:45:38,793 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-03-25 12:45:38,795 - mmaction - INFO - \n",
      "top1_acc\t0.7937\n",
      "top5_acc\t1.0000\n",
      "2021-03-25 12:45:38,796 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-03-25 12:45:38,797 - mmaction - INFO - \n",
      "mean_acc\t0.7937\n",
      "2021-03-25 12:45:38,798 - mmaction - INFO - Epoch(val) [7][66]\ttop1_acc: 0.7937, top5_acc: 1.0000, mean_class_accuracy: 0.7937\n",
      "2021-03-25 12:47:09,756 - mmaction - INFO - Epoch [8][40/66]\tlr: 8.796e-03, eta: 0:06:00, time: 2.274, data_time: 1.378, memory: 12207, top1_acc: 0.7109, top5_acc: 0.9891, loss_cls: 0.7145, loss: 0.7145, grad_norm: 1.0675\n",
      "2021-03-25 12:48:02,808 - mmaction - INFO - Saving checkpoint at 8 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 126/126, 6.5 task/s, elapsed: 20s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-25 12:48:22,782 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-03-25 12:48:22,783 - mmaction - INFO - \n",
      "top1_acc\t0.8492\n",
      "top5_acc\t1.0000\n",
      "2021-03-25 12:48:22,784 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-03-25 12:48:22,784 - mmaction - INFO - \n",
      "mean_acc\t0.8492\n",
      "2021-03-25 12:48:23,196 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_8.pth.\n",
      "2021-03-25 12:48:23,197 - mmaction - INFO - Best top1_acc is 0.8492 at 8 epoch.\n",
      "2021-03-25 12:48:23,198 - mmaction - INFO - Epoch(val) [8][66]\ttop1_acc: 0.8492, top5_acc: 1.0000, mean_class_accuracy: 0.8492\n",
      "2021-03-25 12:49:56,477 - mmaction - INFO - Epoch [9][40/66]\tlr: 5.650e-03, eta: 0:04:05, time: 2.332, data_time: 1.436, memory: 12207, top1_acc: 0.7219, top5_acc: 0.9875, loss_cls: 0.7207, loss: 0.7207, grad_norm: 1.1247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 126/126, 6.4 task/s, elapsed: 20s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-25 12:51:07,151 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-03-25 12:51:07,154 - mmaction - INFO - \n",
      "top1_acc\t0.8254\n",
      "top5_acc\t1.0000\n",
      "2021-03-25 12:51:07,155 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-03-25 12:51:07,157 - mmaction - INFO - \n",
      "mean_acc\t0.8254\n",
      "2021-03-25 12:51:07,158 - mmaction - INFO - Epoch(val) [9][66]\ttop1_acc: 0.8254, top5_acc: 1.0000, mean_class_accuracy: 0.8254\n",
      "2021-03-25 12:52:37,352 - mmaction - INFO - Epoch [10][40/66]\tlr: 2.809e-03, eta: 0:02:19, time: 2.255, data_time: 1.360, memory: 12207, top1_acc: 0.7141, top5_acc: 0.9922, loss_cls: 0.6923, loss: 0.6923, grad_norm: 1.0608\n",
      "2021-03-25 12:53:32,091 - mmaction - INFO - Saving checkpoint at 10 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 126/126, 6.5 task/s, elapsed: 19s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-25 12:53:52,057 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-03-25 12:53:52,059 - mmaction - INFO - \n",
      "top1_acc\t0.8492\n",
      "top5_acc\t1.0000\n",
      "2021-03-25 12:53:52,059 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-03-25 12:53:52,061 - mmaction - INFO - \n",
      "mean_acc\t0.8492\n",
      "2021-03-25 12:53:52,062 - mmaction - INFO - Epoch(val) [10][66]\ttop1_acc: 0.8492, top5_acc: 1.0000, mean_class_accuracy: 0.8492\n",
      "2021-03-25 12:55:24,466 - mmaction - INFO - Epoch [11][40/66]\tlr: 7.703e-04, eta: 0:00:38, time: 2.310, data_time: 1.412, memory: 12207, top1_acc: 0.6969, top5_acc: 0.9859, loss_cls: 0.7326, loss: 0.7326, grad_norm: 1.0574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 126/126, 6.4 task/s, elapsed: 20s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-25 12:56:38,924 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-03-25 12:56:38,926 - mmaction - INFO - \n",
      "top1_acc\t0.8333\n",
      "top5_acc\t1.0000\n",
      "2021-03-25 12:56:38,926 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-03-25 12:56:38,928 - mmaction - INFO - \n",
      "mean_acc\t0.8333\n",
      "2021-03-25 12:56:38,928 - mmaction - INFO - Epoch(val) [11][66]\ttop1_acc: 0.8333, top5_acc: 1.0000, mean_class_accuracy: 0.8333\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "from mmaction.datasets import build_dataset\n",
    "from mmaction.models import build_model\n",
    "from mmaction.apis import train_model\n",
    "\n",
    "import mmcv\n",
    "\n",
    "# Build the dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# Build the recognizer\n",
    "model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.train_cfg, test_cfg=cfg.test_cfg)\n",
    "\n",
    "# Create work_dir\n",
    "mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
    "train_model(model, datasets, cfg, distributed=False, validate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.ipc_collect()\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "# gc.enable()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f\"{cfg.work_dir}/model50e\", 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ryVoSfZVmogw"
   },
   "source": [
    "## Test the trained recognizer\n",
    "\n",
    "After finetuning the recognizer, let's check the prediction results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eyY3hCMwyTct",
    "outputId": "200e37c7-0da4-421f-98da-41418c3110ca",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 126/126, 0.0 task/s, elapsed: 2773s, ETA:     0s\n",
      "Evaluating top_k_accuracy ...\n",
      "\n",
      "top1_acc\t0.9365\n",
      "top5_acc\t1.0000\n",
      "\n",
      "Evaluating mean_class_accuracy ...\n",
      "\n",
      "mean_acc\t0.9365\n",
      "top1_acc: 0.9365\n",
      "top5_acc: 1.0000\n",
      "mean_class_accuracy: 0.9365\n"
     ]
    }
   ],
   "source": [
    "from mmaction.apis import single_gpu_test\n",
    "from mmaction.datasets import build_dataloader\n",
    "from mmcv.parallel import MMDataParallel\n",
    "from mmaction.models import build_model\n",
    "from mmaction.datasets import build_dataset\n",
    "\n",
    "# checkpoint = './childact-checkpoints/childact-slowfast/best_top1_acc_epoch_50.pth'\n",
    "# modelt = init_recognizer(cfg, checkpoint, device='cuda:0')\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# Build a test dataloader\n",
    "dataset = build_dataset(cfg.data.test, dict(test_mode=True))\n",
    "data_loader = build_dataloader(\n",
    "        dataset,\n",
    "        videos_per_gpu=2,\n",
    "        workers_per_gpu=1,\n",
    "        dist=False,\n",
    "        shuffle=False)\n",
    "model = MMDataParallel(model, device_ids=[0])\n",
    "outputs = single_gpu_test(model, data_loader)\n",
    "\n",
    "eval_config = cfg.evaluation\n",
    "eval_config.pop('interval')\n",
    "eval_res = dataset.evaluate(outputs, **eval_config)\n",
    "for name, val in eval_res.items():\n",
    "    print(f'{name}: {val:.04f}')\n",
    "    \n",
    "output_config = cfg.output_config\n",
    "dataset.dump_results(outputs, **output_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from mmaction.core import confusion_matrix \n",
    "\n",
    "gt_labels = [ann['label'] for ann in dataset.load_annotations()]\n",
    "pred = np.argmax(outputs, axis=1)\n",
    "cf_mat = confusion_matrix(pred, gt_labels).astype(float)\n",
    "\n",
    "sns.heatmap(cf_mat, annot=True, xticklabels = ['box', 'clap', 'go', 'jog', 'run', 'walk', 'wave'], yticklabels = ['box', 'clap', 'go', 'jog', 'run', 'walk', 'wave'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "# R(2+1)D 59.52%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "64CW6d_AaT-Q",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "3b284fd8-4ee7-4a34-90d7-5023cd123a04",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-03-24 13:44:27--  https://download.openmmlab.com/mmaction/recognition/r2plus1d/r2plus1d_r34_video_8x8x1_180e_kinetics400_rgb/r2plus1d_r34_video_8x8x1_180e_kinetics400_rgb_20200826-ab35a529.pth\n",
      "Resolving download.openmmlab.com (download.openmmlab.com)... 47.252.96.35\n",
      "Connecting to download.openmmlab.com (download.openmmlab.com)|47.252.96.35|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 255320099 (243M) [application/octet-stream]\n",
      "Saving to: ‘checkpoints/r2plus1d_r34_video_8x8x1_180e_kinetics400_rgb_20200826-ab35a529.pth’\n",
      "\n",
      "checkpoints/r2plus1 100%[===================>] 243,49M  6,98MB/s    in 32s     \n",
      "\n",
      "2021-03-24 13:45:05 (7,72 MB/s) - ‘checkpoints/r2plus1d_r34_video_8x8x1_180e_kinetics400_rgb_20200826-ab35a529.pth’ saved [255320099/255320099]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !mkdir checkpoints\n",
    "!wget -c https://download.openmmlab.com/mmaction/recognition/r2plus1d/r2plus1d_r34_video_8x8x1_180e_kinetics400_rgb/r2plus1d_r34_video_8x8x1_180e_kinetics400_rgb_20200826-ab35a529.pth \\\n",
    "      -O checkpoints/r2plus1d_r34_video_8x8x1_180e_kinetics400_rgb_20200826-ab35a529.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "LjCcmCKOjktc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mmcv import Config\n",
    "cfg = Config.fromfile('./configs/recognition/r2plus1d/r2plus1d_r34_video_8x8x1_180e_kinetics400_rgb.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "tlhu9byjjt-K",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "13d1bb01-f351-48c0-9efb-0bdf2c125d29",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "model = dict(\n",
      "    type='Recognizer3D',\n",
      "    backbone=dict(\n",
      "        type='ResNet2Plus1d',\n",
      "        depth=34,\n",
      "        pretrained=None,\n",
      "        pretrained2d=False,\n",
      "        norm_eval=False,\n",
      "        conv_cfg=dict(type='Conv2plus1d'),\n",
      "        conv1_kernel=(3, 7, 7),\n",
      "        conv1_stride_t=1,\n",
      "        pool1_stride_t=1,\n",
      "        inflate=(1, 1, 1, 1),\n",
      "        spatial_strides=(1, 2, 2, 2),\n",
      "        temporal_strides=(1, 2, 2, 2),\n",
      "        zero_init_residual=False,\n",
      "        act_cfg=dict(type='ReLU')),\n",
      "    cls_head=dict(\n",
      "        type='I3DHead',\n",
      "        num_classes=7,\n",
      "        in_channels=512,\n",
      "        spatial_type='avg',\n",
      "        dropout_ratio=0.5,\n",
      "        init_std=0.01),\n",
      "    train_cfg=None,\n",
      "    test_cfg=dict(average_clips='prob'))\n",
      "checkpoint_config = dict(interval=4)\n",
      "log_config = dict(interval=10, hooks=[dict(type='TextLoggerHook')])\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = './checkpoints/r2plus1d_r34_video_8x8x1_180e_kinetics400_rgb_20200826-ab35a529.pth'\n",
      "resume_from = None\n",
      "workflow = [('train', 1)]\n",
      "dataset_type = 'VideoDataset'\n",
      "data_root = 'data/childact_split/train/'\n",
      "data_root_val = 'data/childact_split/val/'\n",
      "ann_file_train = 'data/childact_split/childact_train_video.txt'\n",
      "ann_file_val = 'data/childact_split/childact_val_video.txt'\n",
      "ann_file_test = 'data/childact_split/childact_test_video.txt'\n",
      "img_norm_cfg = dict(\n",
      "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_bgr=False)\n",
      "train_pipeline = [\n",
      "    dict(type='DecordInit'),\n",
      "    dict(type='SampleFrames', clip_len=8, frame_interval=8, num_clips=1),\n",
      "    dict(type='DecordDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='RandomResizedCrop'),\n",
      "    dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "    dict(type='Flip', flip_ratio=0.5),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "]\n",
      "val_pipeline = [\n",
      "    dict(type='DecordInit'),\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=8,\n",
      "        frame_interval=8,\n",
      "        num_clips=1,\n",
      "        test_mode=True),\n",
      "    dict(type='DecordDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='CenterCrop', crop_size=224),\n",
      "    dict(type='Flip', flip_ratio=0),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(type='DecordInit'),\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=8,\n",
      "        frame_interval=8,\n",
      "        num_clips=10,\n",
      "        test_mode=True),\n",
      "    dict(type='DecordDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='ThreeCrop', crop_size=256),\n",
      "    dict(type='Flip', flip_ratio=0),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "data = dict(\n",
      "    videos_per_gpu=32,\n",
      "    workers_per_gpu=4,\n",
      "    train=dict(\n",
      "        type='VideoDataset',\n",
      "        ann_file='data/childact_split/childact_train_video.txt',\n",
      "        data_prefix='data/childact_split/train/',\n",
      "        pipeline=[\n",
      "            dict(type='DecordInit'),\n",
      "            dict(\n",
      "                type='SampleFrames', clip_len=8, frame_interval=8,\n",
      "                num_clips=1),\n",
      "            dict(type='DecordDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='RandomResizedCrop'),\n",
      "            dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "            dict(type='Flip', flip_ratio=0.5),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "        ]),\n",
      "    val=dict(\n",
      "        type='VideoDataset',\n",
      "        ann_file='data/childact_split/childact_val_video.txt',\n",
      "        data_prefix='data/childact_split/val/',\n",
      "        pipeline=[\n",
      "            dict(type='DecordInit'),\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=8,\n",
      "                frame_interval=8,\n",
      "                num_clips=1,\n",
      "                test_mode=True),\n",
      "            dict(type='DecordDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='CenterCrop', crop_size=224),\n",
      "            dict(type='Flip', flip_ratio=0),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ],\n",
      "        test_mode=True),\n",
      "    test=dict(\n",
      "        type='VideoDataset',\n",
      "        ann_file='data/childact_split/childact_test_video.txt',\n",
      "        data_prefix='data/childact_split/test/',\n",
      "        pipeline=[\n",
      "            dict(type='DecordInit'),\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=8,\n",
      "                frame_interval=8,\n",
      "                num_clips=10,\n",
      "                test_mode=True),\n",
      "            dict(type='DecordDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='ThreeCrop', crop_size=256),\n",
      "            dict(type='Flip', flip_ratio=0),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ],\n",
      "        test_mode=True))\n",
      "evaluation = dict(\n",
      "    interval=5, metrics=['top_k_accuracy', 'mean_class_accuracy'])\n",
      "optimizer = dict(type='SGD', lr=0.2, momentum=0.9, weight_decay=0.0001)\n",
      "optimizer_config = dict(grad_clip=dict(max_norm=40, norm_type=2))\n",
      "lr_config = dict(\n",
      "    policy='cyclic',\n",
      "    target_ratio=(10, 0.0001),\n",
      "    cyclic_times=1,\n",
      "    step_ratio_up=0.4)\n",
      "total_epochs = 51\n",
      "work_dir = './childact-checkpoints/childact-r2plus1d-3'\n",
      "find_unused_parameters = False\n",
      "omnisource = False\n",
      "momentum_config = dict(\n",
      "    policy='cyclic',\n",
      "    target_ratio=(0.8947368421052632, 1),\n",
      "    cyclic_times=1,\n",
      "    step_ratio_up=0.4)\n",
      "seed = 42\n",
      "gpu_ids = range(0, 1)\n",
      "output_config = dict(\n",
      "    out='./childact-checkpoints/childact-r2plus1d-3/results.json')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mmcv.runner import set_random_seed\n",
    "\n",
    "# Modify dataset type and path\n",
    "cfg.dataset_type = 'VideoDataset'\n",
    "cfg.data_root = 'data/childact_split/train/'\n",
    "cfg.data_root_val = 'data/childact_split/val/'\n",
    "cfg.ann_file_train = 'data/childact_split/childact_train_video.txt'\n",
    "cfg.ann_file_val = 'data/childact_split/childact_val_video.txt'\n",
    "cfg.ann_file_test = 'data/childact_split/childact_test_video.txt'\n",
    "\n",
    "cfg.data.test.type = 'VideoDataset'\n",
    "cfg.data.test.ann_file = 'data/childact_split/childact_test_video.txt'\n",
    "cfg.data.test.data_prefix = 'data/childact_split/test/'\n",
    "\n",
    "cfg.data.train.type = 'VideoDataset'\n",
    "cfg.data.train.ann_file = 'data/childact_split/childact_train_video.txt'\n",
    "cfg.data.train.data_prefix = 'data/childact_split/train/'\n",
    "\n",
    "cfg.data.val.type = 'VideoDataset'\n",
    "cfg.data.val.ann_file = 'data/childact_split/childact_val_video.txt'\n",
    "cfg.data.val.data_prefix = 'data/childact_split/val/'\n",
    "\n",
    "# The flag is used to determine whether it is omnisource training\n",
    "cfg.setdefault('omnisource', False)\n",
    "# Modify num classes of the model in cls_head\n",
    "cfg.model.cls_head.num_classes = 7\n",
    "# We can use the pre-trained TSN model\n",
    "cfg.load_from = './checkpoints/r2plus1d_r34_video_8x8x1_180e_kinetics400_rgb_20200826-ab35a529.pth'\n",
    "# cfg.resume_from = './childact-checkpoints/childact-r2plus1d/latest.pth'\n",
    "\n",
    "# Set up working dir to save files and logs.\n",
    "cfg.work_dir = './childact-checkpoints/childact-r2plus1d-3'\n",
    "\n",
    "# The original learning rate (LR) is set for 8-GPU training.\n",
    "# We divide it by 8 since we only use one GPU.\n",
    "cfg.data.videos_per_gpu = 32\n",
    "# cfg.optimizer.type = 'Adam'\n",
    "# cfg.optimizer.weight_decay=0.0001\n",
    "\n",
    "# cfg.optimizer_config.grad_clip=None\n",
    "# cfg.optimizer.lr = 0.01\n",
    "\n",
    "cfg.lr_config = dict(\n",
    "    policy='cyclic',\n",
    "    target_ratio=(10, 1e-4),\n",
    "    cyclic_times=1,\n",
    "    step_ratio_up=0.4,\n",
    ")\n",
    "\n",
    "cfg.total_epochs = 51\n",
    "\n",
    "cfg.momentum_config = dict(\n",
    "    policy='cyclic',\n",
    "    target_ratio=(0.85 / 0.95, 1),\n",
    "    cyclic_times=1,\n",
    "    step_ratio_up=0.4,\n",
    ")\n",
    "\n",
    "# We can set the checkpoint saving interval to reduce the storage cost\n",
    "cfg.checkpoint_config.interval = 4\n",
    "# We can set the log print interval to reduce the the times of printing log\n",
    "cfg.log_config.interval = 10\n",
    "\n",
    "# Set seed thus the results are more reproducible\n",
    "cfg.seed = 42\n",
    "set_random_seed(42, deterministic=False)\n",
    "cfg.gpu_ids = range(1)\n",
    "\n",
    "cfg.output_config = dict(out=f'{cfg.work_dir}/results.json')\n",
    "# We can initialize the logger for training and have a look\n",
    "# at the final config used for training\n",
    "# del cfg.optimizer['momentum']\n",
    "del cfg.model.backbone['norm_cfg']\n",
    "# cfg.model.cls_head.type = \"TSNHead\"\n",
    "print(f'Config:\\n{cfg.pretty_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "dDBWkdDRk6oz",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "85a52ef3-7b5c-4c52-8fef-00322a8c65e6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-25 15:30:20,791 - mmaction - INFO - load checkpoint from ./checkpoints/r2plus1d_r34_video_8x8x1_180e_kinetics400_rgb_20200826-ab35a529.pth\n",
      "2021-03-25 15:30:20,792 - mmaction - INFO - Use load_from_local loader\n",
      "2021-03-25 15:30:20,949 - mmaction - WARNING - The model and loaded state dict do not match exactly\n",
      "\n",
      "size mismatch for cls_head.fc_cls.weight: copying a param with shape torch.Size([400, 512]) from checkpoint, the shape in current model is torch.Size([7, 512]).\n",
      "size mismatch for cls_head.fc_cls.bias: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([7]).\n",
      "2021-03-25 15:30:20,956 - mmaction - INFO - Start running, host: actrec@actrec-HP-Z4-G4-Workstation, work_dir: /home/actrec/.virtualenvs/mmaction/mmaction2/childact-checkpoints/childact-r2plus1d-3\n",
      "2021-03-25 15:30:20,956 - mmaction - INFO - workflow: [('train', 1)], max: 51 epochs\n",
      "/home/actrec/.virtualenvs/mmaction/mmaction2/mmaction/core/evaluation/eval_hooks.py:131: UserWarning: runner.meta is None. Creating a empty one.\n",
      "  warnings.warn('runner.meta is None. Creating a empty one.')\n",
      "2021-03-25 15:31:02,501 - mmaction - INFO - Epoch [1][10/33]\tlr: 2.008e-01, eta: 1:55:49, time: 4.154, data_time: 2.889, memory: 21540, top1_acc: 0.2844, top5_acc: 0.8156, loss_cls: 2.1705, loss: 2.1705, grad_norm: 7.4289\n",
      "2021-03-25 15:31:31,298 - mmaction - INFO - Epoch [1][20/33]\tlr: 2.035e-01, eta: 1:37:28, time: 2.880, data_time: 1.614, memory: 21540, top1_acc: 0.2469, top5_acc: 0.9125, loss_cls: 2.0148, loss: 2.0148, grad_norm: 3.1453\n",
      "2021-03-25 15:32:05,559 - mmaction - INFO - Epoch [1][30/33]\tlr: 2.082e-01, eta: 1:36:03, time: 3.426, data_time: 2.157, memory: 21540, top1_acc: 0.2000, top5_acc: 0.9062, loss_cls: 2.6605, loss: 2.6605, grad_norm: 2.8469\n",
      "2021-03-25 15:32:53,722 - mmaction - INFO - Epoch [2][10/33]\tlr: 2.172e-01, eta: 1:33:10, time: 4.199, data_time: 2.919, memory: 21540, top1_acc: 0.2281, top5_acc: 0.9000, loss_cls: 1.8570, loss: 1.8570, grad_norm: 1.2776\n",
      "2021-03-25 15:33:23,062 - mmaction - INFO - Epoch [2][20/33]\tlr: 2.264e-01, eta: 1:30:10, time: 2.934, data_time: 1.633, memory: 21540, top1_acc: 0.2531, top5_acc: 0.9250, loss_cls: 1.8182, loss: 1.8182, grad_norm: 0.8250\n",
      "2021-03-25 15:33:55,783 - mmaction - INFO - Epoch [2][30/33]\tlr: 2.374e-01, eta: 1:29:25, time: 3.272, data_time: 1.989, memory: 21540, top1_acc: 0.2375, top5_acc: 0.9281, loss_cls: 1.8561, loss: 1.8561, grad_norm: 0.9048\n",
      "2021-03-25 15:34:41,763 - mmaction - INFO - Epoch [3][10/33]\tlr: 2.546e-01, eta: 1:27:36, time: 3.995, data_time: 2.713, memory: 21540, top1_acc: 0.2656, top5_acc: 0.9531, loss_cls: 1.7201, loss: 1.7201, grad_norm: 0.6051\n",
      "2021-03-25 15:35:11,548 - mmaction - INFO - Epoch [3][20/33]\tlr: 2.699e-01, eta: 1:26:09, time: 2.979, data_time: 1.666, memory: 21540, top1_acc: 0.3063, top5_acc: 0.9688, loss_cls: 1.5800, loss: 1.5800, grad_norm: 0.5706\n",
      "2021-03-25 15:35:46,000 - mmaction - INFO - Epoch [3][30/33]\tlr: 2.871e-01, eta: 1:26:11, time: 3.445, data_time: 2.146, memory: 21540, top1_acc: 0.2938, top5_acc: 0.9688, loss_cls: 1.5985, loss: 1.5985, grad_norm: 0.6491\n",
      "2021-03-25 15:36:35,684 - mmaction - INFO - Epoch [4][10/33]\tlr: 3.120e-01, eta: 1:25:19, time: 4.171, data_time: 2.896, memory: 21540, top1_acc: 0.3563, top5_acc: 0.9531, loss_cls: 1.5086, loss: 1.5086, grad_norm: 0.6201\n",
      "2021-03-25 15:37:03,335 - mmaction - INFO - Epoch [4][20/33]\tlr: 3.331e-01, eta: 1:23:43, time: 2.765, data_time: 1.476, memory: 21540, top1_acc: 0.3219, top5_acc: 0.9437, loss_cls: 1.7628, loss: 1.7628, grad_norm: 0.7288\n",
      "2021-03-25 15:37:40,292 - mmaction - INFO - Epoch [4][30/33]\tlr: 3.559e-01, eta: 1:24:09, time: 3.696, data_time: 2.415, memory: 21540, top1_acc: 0.3344, top5_acc: 0.9531, loss_cls: 1.5966, loss: 1.5966, grad_norm: 0.5774\n",
      "2021-03-25 15:37:48,214 - mmaction - INFO - Saving checkpoint at 4 epochs\n",
      "2021-03-25 15:38:30,725 - mmaction - INFO - Epoch [5][10/33]\tlr: 3.880e-01, eta: 1:23:20, time: 4.167, data_time: 2.890, memory: 21540, top1_acc: 0.3094, top5_acc: 0.9500, loss_cls: 1.5623, loss: 1.5623, grad_norm: 0.4973\n",
      "2021-03-25 15:38:59,313 - mmaction - INFO - Epoch [5][20/33]\tlr: 4.145e-01, eta: 1:22:09, time: 2.859, data_time: 1.580, memory: 21540, top1_acc: 0.3406, top5_acc: 0.9469, loss_cls: 1.5454, loss: 1.5454, grad_norm: 0.6928\n",
      "2021-03-25 15:39:32,827 - mmaction - INFO - Epoch [5][30/33]\tlr: 4.424e-01, eta: 1:21:49, time: 3.351, data_time: 2.062, memory: 21540, top1_acc: 0.2687, top5_acc: 0.9531, loss_cls: 1.6012, loss: 1.6012, grad_norm: 0.6742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 126/126, 7.8 task/s, elapsed: 16s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-25 15:39:57,043 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-03-25 15:39:57,045 - mmaction - INFO - \n",
      "top1_acc\t0.3651\n",
      "top5_acc\t1.0000\n",
      "2021-03-25 15:39:57,046 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-03-25 15:39:57,047 - mmaction - INFO - \n",
      "mean_acc\t0.3651\n",
      "2021-03-25 15:39:57,852 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_5.pth.\n",
      "2021-03-25 15:39:57,854 - mmaction - INFO - Best top1_acc is 0.3651 at 5 epoch.\n",
      "2021-03-25 15:39:57,855 - mmaction - INFO - Epoch(val) [5][33]\ttop1_acc: 0.3651, top5_acc: 1.0000, mean_class_accuracy: 0.3651\n",
      "2021-03-25 15:40:40,465 - mmaction - INFO - Epoch [6][10/33]\tlr: 4.809e-01, eta: 1:21:13, time: 4.260, data_time: 2.969, memory: 21540, top1_acc: 0.3094, top5_acc: 0.9656, loss_cls: 1.5239, loss: 1.5239, grad_norm: 0.5280\n",
      "2021-03-25 15:41:09,933 - mmaction - INFO - Epoch [6][20/33]\tlr: 5.121e-01, eta: 1:20:17, time: 2.947, data_time: 1.667, memory: 21540, top1_acc: 0.3312, top5_acc: 0.9750, loss_cls: 1.5019, loss: 1.5019, grad_norm: 0.6209\n",
      "2021-03-25 15:41:43,015 - mmaction - INFO - Epoch [6][30/33]\tlr: 5.445e-01, eta: 1:19:52, time: 3.308, data_time: 2.006, memory: 21540, top1_acc: 0.3500, top5_acc: 0.9688, loss_cls: 1.4669, loss: 1.4669, grad_norm: 0.6785\n",
      "2021-03-25 15:42:32,873 - mmaction - INFO - Epoch [7][10/33]\tlr: 5.885e-01, eta: 1:19:19, time: 4.316, data_time: 3.034, memory: 21540, top1_acc: 0.3187, top5_acc: 0.9625, loss_cls: 1.4733, loss: 1.4733, grad_norm: 0.5135\n",
      "2021-03-25 15:42:59,557 - mmaction - INFO - Epoch [7][20/33]\tlr: 6.236e-01, eta: 1:18:10, time: 2.668, data_time: 1.382, memory: 21540, top1_acc: 0.2750, top5_acc: 0.9531, loss_cls: 1.6565, loss: 1.6565, grad_norm: 0.7216\n",
      "2021-03-25 15:43:33,497 - mmaction - INFO - Epoch [7][30/33]\tlr: 6.597e-01, eta: 1:17:50, time: 3.394, data_time: 2.101, memory: 21540, top1_acc: 0.2875, top5_acc: 0.9531, loss_cls: 1.6605, loss: 1.6605, grad_norm: 0.4748\n",
      "2021-03-25 15:44:20,169 - mmaction - INFO - Epoch [8][10/33]\tlr: 7.082e-01, eta: 1:17:09, time: 4.178, data_time: 2.898, memory: 21540, top1_acc: 0.2812, top5_acc: 0.9594, loss_cls: 1.5988, loss: 1.5988, grad_norm: 0.4877\n",
      "2021-03-25 15:44:48,118 - mmaction - INFO - Epoch [8][20/33]\tlr: 7.464e-01, eta: 1:16:13, time: 2.795, data_time: 1.508, memory: 21540, top1_acc: 0.3063, top5_acc: 0.9531, loss_cls: 1.5732, loss: 1.5732, grad_norm: 0.5366\n",
      "2021-03-25 15:45:21,504 - mmaction - INFO - Epoch [8][30/33]\tlr: 7.854e-01, eta: 1:15:49, time: 3.339, data_time: 2.050, memory: 21540, top1_acc: 0.3094, top5_acc: 0.9625, loss_cls: 1.6534, loss: 1.6534, grad_norm: 0.5668\n",
      "2021-03-25 15:45:27,422 - mmaction - INFO - Saving checkpoint at 8 epochs\n",
      "2021-03-25 15:46:09,156 - mmaction - INFO - Epoch [9][10/33]\tlr: 8.371e-01, eta: 1:15:04, time: 4.092, data_time: 2.812, memory: 21540, top1_acc: 0.3156, top5_acc: 0.9594, loss_cls: 1.5707, loss: 1.5707, grad_norm: 0.4655\n",
      "2021-03-25 15:46:37,676 - mmaction - INFO - Epoch [9][20/33]\tlr: 8.776e-01, eta: 1:14:15, time: 2.852, data_time: 1.568, memory: 21540, top1_acc: 0.2875, top5_acc: 0.9625, loss_cls: 1.5536, loss: 1.5536, grad_norm: 0.4027\n",
      "2021-03-25 15:47:11,993 - mmaction - INFO - Epoch [9][30/33]\tlr: 9.185e-01, eta: 1:13:55, time: 3.432, data_time: 2.142, memory: 21540, top1_acc: 0.3438, top5_acc: 0.9656, loss_cls: 1.5327, loss: 1.5327, grad_norm: 0.4450\n",
      "2021-03-25 15:48:03,821 - mmaction - INFO - Epoch [10][10/33]\tlr: 9.723e-01, eta: 1:13:31, time: 4.556, data_time: 3.277, memory: 21540, top1_acc: 0.3219, top5_acc: 0.9344, loss_cls: 1.6519, loss: 1.6519, grad_norm: 0.4373\n",
      "2021-03-25 15:48:30,898 - mmaction - INFO - Epoch [10][20/33]\tlr: 1.014e+00, eta: 1:12:38, time: 2.708, data_time: 1.429, memory: 21540, top1_acc: 0.3344, top5_acc: 0.9594, loss_cls: 1.5237, loss: 1.5237, grad_norm: 0.4727\n",
      "2021-03-25 15:49:05,581 - mmaction - INFO - Epoch [10][30/33]\tlr: 1.056e+00, eta: 1:12:17, time: 3.468, data_time: 2.188, memory: 21540, top1_acc: 0.2969, top5_acc: 0.9531, loss_cls: 1.6167, loss: 1.6167, grad_norm: 0.4456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 126/126, 7.8 task/s, elapsed: 16s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-25 15:49:29,166 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-03-25 15:49:29,167 - mmaction - INFO - \n",
      "top1_acc\t0.3175\n",
      "top5_acc\t0.9762\n",
      "2021-03-25 15:49:29,168 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-03-25 15:49:29,168 - mmaction - INFO - \n",
      "mean_acc\t0.3175\n",
      "2021-03-25 15:49:29,169 - mmaction - INFO - Epoch(val) [10][33]\ttop1_acc: 0.3175, top5_acc: 0.9762, mean_class_accuracy: 0.3175\n",
      "2021-03-25 15:50:15,748 - mmaction - INFO - Epoch [11][10/33]\tlr: 1.111e+00, eta: 1:11:55, time: 4.658, data_time: 3.382, memory: 21540, top1_acc: 0.3438, top5_acc: 0.9625, loss_cls: 1.5662, loss: 1.5662, grad_norm: 0.5038\n",
      "2021-03-25 15:50:44,778 - mmaction - INFO - Epoch [11][20/33]\tlr: 1.152e+00, eta: 1:11:11, time: 2.903, data_time: 1.629, memory: 21540, top1_acc: 0.3219, top5_acc: 0.9563, loss_cls: 1.5483, loss: 1.5483, grad_norm: 0.5952\n",
      "2021-03-25 15:51:14,411 - mmaction - INFO - Epoch [11][30/33]\tlr: 1.194e+00, eta: 1:10:31, time: 2.963, data_time: 1.683, memory: 21540, top1_acc: 0.2844, top5_acc: 0.9125, loss_cls: 1.7821, loss: 1.7821, grad_norm: 0.5007\n",
      "2021-03-25 15:52:04,710 - mmaction - INFO - Epoch [12][10/33]\tlr: 1.248e+00, eta: 1:09:55, time: 4.323, data_time: 3.048, memory: 21540, top1_acc: 0.3156, top5_acc: 0.9500, loss_cls: 1.6321, loss: 1.6321, grad_norm: 0.4643\n",
      "2021-03-25 15:52:31,314 - mmaction - INFO - Epoch [12][20/33]\tlr: 1.290e+00, eta: 1:09:04, time: 2.660, data_time: 1.385, memory: 21540, top1_acc: 0.3469, top5_acc: 0.9469, loss_cls: 1.5154, loss: 1.5154, grad_norm: 0.4713\n",
      "2021-03-25 15:53:06,273 - mmaction - INFO - Epoch [12][30/33]\tlr: 1.331e+00, eta: 1:08:43, time: 3.496, data_time: 2.220, memory: 21540, top1_acc: 0.2687, top5_acc: 0.9531, loss_cls: 1.6201, loss: 1.6201, grad_norm: 0.5333\n",
      "2021-03-25 15:53:11,182 - mmaction - INFO - Saving checkpoint at 12 epochs\n",
      "2021-03-25 15:53:54,373 - mmaction - INFO - Epoch [13][10/33]\tlr: 1.383e+00, eta: 1:08:04, time: 4.238, data_time: 2.959, memory: 21540, top1_acc: 0.2844, top5_acc: 0.9594, loss_cls: 1.6167, loss: 1.6167, grad_norm: 0.4886\n",
      "2021-03-25 15:54:21,622 - mmaction - INFO - Epoch [13][20/33]\tlr: 1.422e+00, eta: 1:07:17, time: 2.725, data_time: 1.443, memory: 21540, top1_acc: 0.3031, top5_acc: 0.9531, loss_cls: 1.6399, loss: 1.6399, grad_norm: 0.5734\n",
      "2021-03-25 15:54:56,608 - mmaction - INFO - Epoch [13][30/33]\tlr: 1.461e+00, eta: 1:06:55, time: 3.499, data_time: 2.206, memory: 21540, top1_acc: 0.3281, top5_acc: 0.9531, loss_cls: 1.6068, loss: 1.6068, grad_norm: 0.5718\n",
      "2021-03-25 15:55:44,090 - mmaction - INFO - Epoch [14][10/33]\tlr: 1.511e+00, eta: 1:06:19, time: 4.360, data_time: 3.075, memory: 21540, top1_acc: 0.2875, top5_acc: 0.9469, loss_cls: 1.6569, loss: 1.6569, grad_norm: 0.4681\n",
      "2021-03-25 15:56:09,282 - mmaction - INFO - Epoch [14][20/33]\tlr: 1.548e+00, eta: 1:05:28, time: 2.519, data_time: 1.239, memory: 21540, top1_acc: 0.3125, top5_acc: 0.9406, loss_cls: 1.6067, loss: 1.6067, grad_norm: 0.4975\n",
      "2021-03-25 15:56:44,415 - mmaction - INFO - Epoch [14][30/33]\tlr: 1.584e+00, eta: 1:05:05, time: 3.513, data_time: 2.233, memory: 21540, top1_acc: 0.3094, top5_acc: 0.9469, loss_cls: 1.7103, loss: 1.7103, grad_norm: 0.6207\n",
      "2021-03-25 15:57:35,040 - mmaction - INFO - Epoch [15][10/33]\tlr: 1.629e+00, eta: 1:04:32, time: 4.481, data_time: 3.195, memory: 21540, top1_acc: 0.3031, top5_acc: 0.9563, loss_cls: 1.6136, loss: 1.6136, grad_norm: 0.4809\n",
      "2021-03-25 15:58:03,388 - mmaction - INFO - Epoch [15][20/33]\tlr: 1.662e+00, eta: 1:03:51, time: 2.835, data_time: 1.556, memory: 21540, top1_acc: 0.2812, top5_acc: 0.9375, loss_cls: 1.6945, loss: 1.6945, grad_norm: 0.5643\n",
      "2021-03-25 15:58:36,623 - mmaction - INFO - Epoch [15][30/33]\tlr: 1.694e+00, eta: 1:03:23, time: 3.323, data_time: 2.042, memory: 21540, top1_acc: 0.2500, top5_acc: 0.9469, loss_cls: 1.7621, loss: 1.7621, grad_norm: 0.5227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 126/126, 7.8 task/s, elapsed: 16s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-25 15:58:57,064 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-03-25 15:58:57,066 - mmaction - INFO - \n",
      "top1_acc\t0.1984\n",
      "top5_acc\t0.7460\n",
      "2021-03-25 15:58:57,066 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-03-25 15:58:57,067 - mmaction - INFO - \n",
      "mean_acc\t0.1984\n",
      "2021-03-25 15:58:57,068 - mmaction - INFO - Epoch(val) [15][33]\ttop1_acc: 0.1984, top5_acc: 0.7460, mean_class_accuracy: 0.1984\n",
      "2021-03-25 15:59:40,086 - mmaction - INFO - Epoch [16][10/33]\tlr: 1.734e+00, eta: 1:02:45, time: 4.301, data_time: 3.018, memory: 21540, top1_acc: 0.2938, top5_acc: 0.9437, loss_cls: 1.6775, loss: 1.6775, grad_norm: 0.4904\n",
      "2021-03-25 16:00:07,571 - mmaction - INFO - Epoch [16][20/33]\tlr: 1.763e+00, eta: 1:02:02, time: 2.748, data_time: 1.469, memory: 21540, top1_acc: 0.3063, top5_acc: 0.9719, loss_cls: 1.6790, loss: 1.6790, grad_norm: 0.4985\n",
      "2021-03-25 16:00:40,896 - mmaction - INFO - Epoch [16][30/33]\tlr: 1.791e+00, eta: 1:01:34, time: 3.332, data_time: 2.050, memory: 21540, top1_acc: 0.3031, top5_acc: 0.9500, loss_cls: 1.6984, loss: 1.6984, grad_norm: 0.5027\n",
      "2021-03-25 16:00:44,909 - mmaction - INFO - Saving checkpoint at 16 epochs\n",
      "2021-03-25 16:01:27,873 - mmaction - INFO - Epoch [17][10/33]\tlr: 1.825e+00, eta: 1:00:54, time: 4.216, data_time: 2.942, memory: 21540, top1_acc: 0.2812, top5_acc: 0.9437, loss_cls: 1.7198, loss: 1.7198, grad_norm: 0.4784\n",
      "2021-03-25 16:01:55,806 - mmaction - INFO - Epoch [17][20/33]\tlr: 1.849e+00, eta: 1:00:14, time: 2.793, data_time: 1.517, memory: 21540, top1_acc: 0.2812, top5_acc: 0.9281, loss_cls: 1.7077, loss: 1.7077, grad_norm: 0.5080\n",
      "2021-03-25 16:02:30,456 - mmaction - INFO - Epoch [17][30/33]\tlr: 1.871e+00, eta: 0:59:47, time: 3.465, data_time: 2.179, memory: 21540, top1_acc: 0.3094, top5_acc: 0.9531, loss_cls: 1.6434, loss: 1.6434, grad_norm: 0.5502\n",
      "2021-03-25 16:03:19,488 - mmaction - INFO - Epoch [18][10/33]\tlr: 1.898e+00, eta: 0:59:08, time: 4.251, data_time: 2.976, memory: 21540, top1_acc: 0.3156, top5_acc: 0.9375, loss_cls: 1.6301, loss: 1.6301, grad_norm: 0.5023\n",
      "2021-03-25 16:03:47,471 - mmaction - INFO - Epoch [18][20/33]\tlr: 1.917e+00, eta: 0:58:29, time: 2.798, data_time: 1.523, memory: 21540, top1_acc: 0.2687, top5_acc: 0.9531, loss_cls: 1.6953, loss: 1.6953, grad_norm: 0.4932\n",
      "2021-03-25 16:04:20,780 - mmaction - INFO - Epoch [18][30/33]\tlr: 1.933e+00, eta: 0:57:59, time: 3.331, data_time: 2.055, memory: 21540, top1_acc: 0.2875, top5_acc: 0.9406, loss_cls: 1.6540, loss: 1.6540, grad_norm: 0.4814\n",
      "2021-03-25 16:05:10,733 - mmaction - INFO - Epoch [19][10/33]\tlr: 1.952e+00, eta: 0:57:19, time: 4.208, data_time: 2.933, memory: 21540, top1_acc: 0.2531, top5_acc: 0.9469, loss_cls: 1.7584, loss: 1.7584, grad_norm: 0.5687\n",
      "2021-03-25 16:05:37,501 - mmaction - INFO - Epoch [19][20/33]\tlr: 1.965e+00, eta: 0:56:38, time: 2.677, data_time: 1.399, memory: 21540, top1_acc: 0.2531, top5_acc: 0.9156, loss_cls: 1.7489, loss: 1.7489, grad_norm: 0.3908\n",
      "2021-03-25 16:06:12,213 - mmaction - INFO - Epoch [19][30/33]\tlr: 1.976e+00, eta: 0:56:12, time: 3.471, data_time: 2.192, memory: 21540, top1_acc: 0.2656, top5_acc: 0.9219, loss_cls: 1.7889, loss: 1.7889, grad_norm: 0.3820\n",
      "2021-03-25 16:07:02,668 - mmaction - INFO - Epoch [20][10/33]\tlr: 1.987e+00, eta: 0:55:34, time: 4.388, data_time: 3.113, memory: 21540, top1_acc: 0.2156, top5_acc: 0.9125, loss_cls: 1.7605, loss: 1.7605, grad_norm: 0.3493\n",
      "2021-03-25 16:07:30,063 - mmaction - INFO - Epoch [20][20/33]\tlr: 1.993e+00, eta: 0:54:55, time: 2.739, data_time: 1.467, memory: 21540, top1_acc: 0.2125, top5_acc: 0.9313, loss_cls: 1.7811, loss: 1.7811, grad_norm: 0.4657\n",
      "2021-03-25 16:08:02,925 - mmaction - INFO - Epoch [20][30/33]\tlr: 1.997e+00, eta: 0:54:25, time: 3.286, data_time: 2.005, memory: 21540, top1_acc: 0.2938, top5_acc: 0.9156, loss_cls: 1.7447, loss: 1.7447, grad_norm: 0.8211\n",
      "2021-03-25 16:08:09,007 - mmaction - INFO - Saving checkpoint at 20 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 126/126, 7.8 task/s, elapsed: 16s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-25 16:08:26,069 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-03-25 16:08:26,070 - mmaction - INFO - \n",
      "top1_acc\t0.2381\n",
      "top5_acc\t1.0000\n",
      "2021-03-25 16:08:26,071 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-03-25 16:08:26,072 - mmaction - INFO - \n",
      "mean_acc\t0.2381\n",
      "2021-03-25 16:08:26,073 - mmaction - INFO - Epoch(val) [20][33]\ttop1_acc: 0.2381, top5_acc: 1.0000, mean_class_accuracy: 0.2381\n",
      "2021-03-25 16:09:08,179 - mmaction - INFO - Epoch [21][10/33]\tlr: 2.000e+00, eta: 0:53:45, time: 4.210, data_time: 2.933, memory: 21540, top1_acc: 0.2875, top5_acc: 0.9437, loss_cls: 1.7203, loss: 1.7203, grad_norm: 0.4388\n",
      "2021-03-25 16:09:35,054 - mmaction - INFO - Epoch [21][20/33]\tlr: 2.000e+00, eta: 0:53:06, time: 2.687, data_time: 1.415, memory: 21540, top1_acc: 0.3156, top5_acc: 0.9187, loss_cls: 1.6964, loss: 1.6964, grad_norm: 0.4517\n",
      "2021-03-25 16:10:09,760 - mmaction - INFO - Epoch [21][30/33]\tlr: 1.999e+00, eta: 0:52:38, time: 3.471, data_time: 2.192, memory: 21540, top1_acc: 0.2281, top5_acc: 0.9156, loss_cls: 1.7660, loss: 1.7660, grad_norm: 0.3907\n",
      "2021-03-25 16:11:00,239 - mmaction - INFO - Epoch [22][10/33]\tlr: 1.996e+00, eta: 0:52:00, time: 4.338, data_time: 3.055, memory: 21540, top1_acc: 0.2875, top5_acc: 0.9313, loss_cls: 1.7388, loss: 1.7388, grad_norm: 0.3903\n",
      "2021-03-25 16:11:29,629 - mmaction - INFO - Epoch [22][20/33]\tlr: 1.993e+00, eta: 0:51:24, time: 2.939, data_time: 1.668, memory: 21540, top1_acc: 0.2531, top5_acc: 0.9688, loss_cls: 1.6935, loss: 1.6935, grad_norm: 0.3204\n",
      "2021-03-25 16:12:04,397 - mmaction - INFO - Epoch [22][30/33]\tlr: 1.988e+00, eta: 0:50:57, time: 3.477, data_time: 2.202, memory: 21540, top1_acc: 0.2969, top5_acc: 0.9437, loss_cls: 1.6150, loss: 1.6150, grad_norm: 0.3260\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "from mmaction.datasets import build_dataset\n",
    "from mmaction.models import build_model\n",
    "from mmaction.apis import train_model\n",
    "\n",
    "import mmcv\n",
    "\n",
    "# Build the dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# Build the recognizer\n",
    "model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.train_cfg, test_cfg=cfg.test_cfg)\n",
    "\n",
    "# Create work_dir\n",
    "mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
    "train_model(model, datasets, cfg, distributed=False, validate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f\"{cfg.work_dir}/model50e\", 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eyY3hCMwyTct",
    "outputId": "200e37c7-0da4-421f-98da-41418c3110ca",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 126/126, 0.7 task/s, elapsed: 180s, ETA:     0s\n",
      "Evaluating top_k_accuracy ...\n",
      "\n",
      "top1_acc\t0.5952\n",
      "top5_acc\t1.0000\n",
      "\n",
      "Evaluating mean_class_accuracy ...\n",
      "\n",
      "mean_acc\t0.5952\n",
      "top1_acc: 0.5952\n",
      "top5_acc: 1.0000\n",
      "mean_class_accuracy: 0.5952\n"
     ]
    }
   ],
   "source": [
    "from mmaction.apis import single_gpu_test\n",
    "from mmaction.datasets import build_dataloader, load_annotations, build_dataset\n",
    "from mmcv.parallel import MMDataParallel\n",
    "from mmaction.models import build_model\n",
    "\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# Build a test dataloader\n",
    "dataset = build_dataset(cfg.data.test, dict(test_mode=True))\n",
    "data_loader = build_dataloader(\n",
    "        dataset,\n",
    "        videos_per_gpu=4,\n",
    "        workers_per_gpu=1,\n",
    "        dist=False,\n",
    "        shuffle=False)\n",
    "model = MMDataParallel(model, device_ids=[0])\n",
    "outputs = single_gpu_test(model, data_loader)\n",
    "\n",
    "eval_config = cfg.evaluation\n",
    "eval_config.pop('interval')\n",
    "eval_res = dataset.evaluate(outputs, **eval_config)\n",
    "for name, val in eval_res.items():\n",
    "    print(f'{name}: {val:.04f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from mmaction.core import confusion_matrix \n",
    "\n",
    "gt_labels = [ann['label'] for ann in dataset.load_annotations()]\n",
    "pred = np.argmax(outputs, axis=1)\n",
    "cf_mat = confusion_matrix(pred, gt_labels).astype(float)\n",
    "\n",
    "sns.heatmap(cf_mat, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "# TSM 93.65%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "64CW6d_AaT-Q",
    "outputId": "3b284fd8-4ee7-4a34-90d7-5023cd123a04",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-03-24 19:15:04--  https://download.openmmlab.com/mmaction/recognition/tsm/tsm_r50_video_1x1x8_100e_kinetics400_rgb/tsm_r50_video_1x1x8_100e_kinetics400_rgb_20200702-a77f4328.pth\n",
      "Resolving download.openmmlab.com (download.openmmlab.com)... 47.75.20.25\n",
      "Connecting to download.openmmlab.com (download.openmmlab.com)|47.75.20.25|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 97579687 (93M) [application/octet-stream]\n",
      "Saving to: ‘checkpoints/tsm_r50_video_1x1x8_100e_kinetics400_rgb_20200702-a77f4328.pth’\n",
      "\n",
      "checkpoints/tsm_r50 100%[===================>]  93,06M  8,72MB/s    in 12s     \n",
      "\n",
      "2021-03-24 19:15:20 (7,71 MB/s) - ‘checkpoints/tsm_r50_video_1x1x8_100e_kinetics400_rgb_20200702-a77f4328.pth’ saved [97579687/97579687]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !mkdir checkpoints\n",
    "!wget -c https://download.openmmlab.com/mmaction/recognition/tsm/tsm_r50_video_1x1x8_100e_kinetics400_rgb/tsm_r50_video_1x1x8_100e_kinetics400_rgb_20200702-a77f4328.pth \\\n",
    "      -O checkpoints/tsm_r50_video_1x1x8_100e_kinetics400_rgb_20200702-a77f4328.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "LjCcmCKOjktc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mmcv import Config\n",
    "cfg = Config.fromfile('./configs/recognition/tsm/tsm_r50_video_1x1x8_50e_kinetics400_rgb.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "tlhu9byjjt-K",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "13d1bb01-f351-48c0-9efb-0bdf2c125d29",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "model = dict(\n",
      "    type='Recognizer2D',\n",
      "    backbone=dict(\n",
      "        type='ResNetTSM',\n",
      "        pretrained='torchvision://resnet50',\n",
      "        depth=50,\n",
      "        norm_eval=False,\n",
      "        shift_div=8),\n",
      "    cls_head=dict(\n",
      "        type='TSMHead',\n",
      "        num_classes=7,\n",
      "        in_channels=2048,\n",
      "        spatial_type='avg',\n",
      "        consensus=dict(type='AvgConsensus', dim=1),\n",
      "        dropout_ratio=0.5,\n",
      "        init_std=0.001,\n",
      "        is_shift=True),\n",
      "    train_cfg=None,\n",
      "    test_cfg=dict(average_clips='prob'))\n",
      "optimizer = dict(\n",
      "    type='SGD',\n",
      "    constructor='TSMOptimizerConstructor',\n",
      "    paramwise_cfg=dict(fc_lr5=True),\n",
      "    lr=0.02,\n",
      "    weight_decay=0.0001)\n",
      "optimizer_config = dict(grad_clip=dict(max_norm=20, norm_type=2))\n",
      "lr_config = dict(\n",
      "    policy='cyclic',\n",
      "    target_ratio=(10, 0.0001),\n",
      "    cyclic_times=1,\n",
      "    step_ratio_up=0.4)\n",
      "total_epochs = 51\n",
      "checkpoint_config = dict(interval=4)\n",
      "log_config = dict(interval=25, hooks=[dict(type='TextLoggerHook')])\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = 'checkpoints/tsm_r50_video_1x1x8_100e_kinetics400_rgb_20200702-a77f4328.pth'\n",
      "resume_from = None\n",
      "workflow = [('train', 1)]\n",
      "dataset_type = 'VideoDataset'\n",
      "data_root = 'data/childact_split/train/'\n",
      "data_root_val = 'data/childact_split/val/'\n",
      "ann_file_train = 'data/childact_split/childact_train_video.txt'\n",
      "ann_file_val = 'data/childact_split/childact_val_video.txt'\n",
      "ann_file_test = 'data/childact_split/childact_test_video.txt'\n",
      "img_norm_cfg = dict(\n",
      "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_bgr=False)\n",
      "train_pipeline = [\n",
      "    dict(type='DecordInit'),\n",
      "    dict(type='SampleFrames', clip_len=1, frame_interval=1, num_clips=8),\n",
      "    dict(type='DecordDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(\n",
      "        type='MultiScaleCrop',\n",
      "        input_size=224,\n",
      "        scales=(1, 0.875, 0.75, 0.66),\n",
      "        random_crop=False,\n",
      "        max_wh_scale_gap=1,\n",
      "        num_fixed_crops=13),\n",
      "    dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "    dict(type='Flip', flip_ratio=0.5),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "]\n",
      "val_pipeline = [\n",
      "    dict(type='DecordInit'),\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=1,\n",
      "        frame_interval=1,\n",
      "        num_clips=8,\n",
      "        test_mode=True),\n",
      "    dict(type='DecordDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='CenterCrop', crop_size=224),\n",
      "    dict(type='Flip', flip_ratio=0),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(type='DecordInit'),\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=1,\n",
      "        frame_interval=1,\n",
      "        num_clips=8,\n",
      "        test_mode=True),\n",
      "    dict(type='DecordDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='CenterCrop', crop_size=224),\n",
      "    dict(type='Flip', flip_ratio=0),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "data = dict(\n",
      "    videos_per_gpu=24,\n",
      "    workers_per_gpu=4,\n",
      "    train=dict(\n",
      "        type='VideoDataset',\n",
      "        ann_file='data/childact_split/childact_train_video.txt',\n",
      "        data_prefix='data/childact_split/train/',\n",
      "        pipeline=[\n",
      "            dict(type='DecordInit'),\n",
      "            dict(\n",
      "                type='SampleFrames', clip_len=1, frame_interval=1,\n",
      "                num_clips=8),\n",
      "            dict(type='DecordDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(\n",
      "                type='MultiScaleCrop',\n",
      "                input_size=224,\n",
      "                scales=(1, 0.875, 0.75, 0.66),\n",
      "                random_crop=False,\n",
      "                max_wh_scale_gap=1,\n",
      "                num_fixed_crops=13),\n",
      "            dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "            dict(type='Flip', flip_ratio=0.5),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "        ]),\n",
      "    val=dict(\n",
      "        type='VideoDataset',\n",
      "        ann_file='data/childact_split/childact_val_video.txt',\n",
      "        data_prefix='data/childact_split/val/',\n",
      "        pipeline=[\n",
      "            dict(type='DecordInit'),\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=1,\n",
      "                frame_interval=1,\n",
      "                num_clips=8,\n",
      "                test_mode=True),\n",
      "            dict(type='DecordDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='CenterCrop', crop_size=224),\n",
      "            dict(type='Flip', flip_ratio=0),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ]),\n",
      "    test=dict(\n",
      "        type='VideoDataset',\n",
      "        ann_file='data/childact_split/childact_test_video.txt',\n",
      "        data_prefix='data/childact_split/test/',\n",
      "        pipeline=[\n",
      "            dict(type='DecordInit'),\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=1,\n",
      "                frame_interval=1,\n",
      "                num_clips=8,\n",
      "                test_mode=True),\n",
      "            dict(type='DecordDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='CenterCrop', crop_size=224),\n",
      "            dict(type='Flip', flip_ratio=0),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ]))\n",
      "evaluation = dict(\n",
      "    interval=5, metrics=['top_k_accuracy', 'mean_class_accuracy'])\n",
      "work_dir = './childact-checkpoints/childact-tsm'\n",
      "omnisource = False\n",
      "seed = 42\n",
      "gpu_ids = range(0, 1)\n",
      "output_config = dict(out='./childact-checkpoints/childact-tsm/results.json')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mmcv.runner import set_random_seed\n",
    "\n",
    "# Modify dataset type and path\n",
    "cfg.dataset_type = 'VideoDataset'\n",
    "cfg.data_root = 'data/childact_split/train/'\n",
    "cfg.data_root_val = 'data/childact_split/val/'\n",
    "cfg.ann_file_train = 'data/childact_split/childact_train_video.txt'\n",
    "cfg.ann_file_val = 'data/childact_split/childact_val_video.txt'\n",
    "cfg.ann_file_test = 'data/childact_split/childact_test_video.txt'\n",
    "\n",
    "cfg.data.test.type = 'VideoDataset'\n",
    "cfg.data.test.ann_file = 'data/childact_split/childact_test_video.txt'\n",
    "cfg.data.test.data_prefix = 'data/childact_split/test/'\n",
    "\n",
    "cfg.data.train.type = 'VideoDataset'\n",
    "cfg.data.train.ann_file = 'data/childact_split/childact_train_video.txt'\n",
    "cfg.data.train.data_prefix = 'data/childact_split/train/'\n",
    "\n",
    "cfg.data.val.type = 'VideoDataset'\n",
    "cfg.data.val.ann_file = 'data/childact_split/childact_val_video.txt'\n",
    "cfg.data.val.data_prefix = 'data/childact_split/val/'\n",
    "\n",
    "# The flag is used to determine whether it is omnisource training\n",
    "cfg.setdefault('omnisource', False)\n",
    "# Modify num classes of the model in cls_head\n",
    "cfg.model.cls_head.num_classes = 7\n",
    "# We can use the pre-trained TSN model\n",
    "cfg.load_from = 'checkpoints/tsm_r50_video_1x1x8_100e_kinetics400_rgb_20200702-a77f4328.pth'\n",
    "# cfg.resume_from = './childact-mm/latest.pth'\n",
    "\n",
    "# Set up working dir to save files and logs.\n",
    "cfg.work_dir = './childact-checkpoints/childact-tsm'\n",
    "\n",
    "# The original learning rate (LR) is set for 8-GPU training.\n",
    "# We divide it by 8 since we only use one GPU.\n",
    "cfg.data.videos_per_gpu = 24\n",
    "# cfg.optimizer.type = 'Adam'\n",
    "# cfg.optimizer.weight_decay=0.0001\n",
    "\n",
    "# cfg.optimizer_config.grad_clip=None\n",
    "# cfg.optimizer.lr = 0.01\n",
    "\n",
    "cfg.lr_config = dict(\n",
    "    policy='cyclic',\n",
    "    target_ratio=(10, 1e-4),\n",
    "    cyclic_times=1,\n",
    "    step_ratio_up=0.4,\n",
    ")\n",
    "\n",
    "cfg.total_epochs = 51\n",
    "\n",
    "cfg.momentum_config = dict(\n",
    "    policy='cyclic',\n",
    "    target_ratio=(0.85 / 0.95, 1),\n",
    "    cyclic_times=1,\n",
    "    step_ratio_up=0.4,\n",
    ")\n",
    "\n",
    "# We can set the checkpoint saving interval to reduce the storage cost\n",
    "cfg.checkpoint_config.interval = 4\n",
    "# We can set the log print interval to reduce the the times of printing log\n",
    "cfg.log_config.interval = 25\n",
    "\n",
    "# Set seed thus the results are more reproducible\n",
    "cfg.seed = 42\n",
    "set_random_seed(42, deterministic=False)\n",
    "cfg.gpu_ids = range(1)\n",
    "\n",
    "cfg.output_config = dict(out=f'{cfg.work_dir}/results.json')\n",
    "# We can initialize the logger for training and have a look\n",
    "# at the final config used for training\n",
    "del cfg.optimizer['momentum']\n",
    "print(f'Config:\\n{cfg.pretty_text}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "dDBWkdDRk6oz",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "85a52ef3-7b5c-4c52-8fef-00322a8c65e6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-24 21:03:31,550 - mmaction - INFO - These parameters in pretrained checkpoint are not loaded: {'fc.weight', 'fc.bias'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use load_from_torchvision loader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-24 21:03:33,980 - mmaction - INFO - load checkpoint from checkpoints/tsm_r50_video_1x1x8_100e_kinetics400_rgb_20200702-a77f4328.pth\n",
      "2021-03-24 21:03:33,982 - mmaction - INFO - Use load_from_local loader\n",
      "2021-03-24 21:03:34,081 - mmaction - WARNING - The model and loaded state dict do not match exactly\n",
      "\n",
      "size mismatch for cls_head.fc_cls.weight: copying a param with shape torch.Size([400, 2048]) from checkpoint, the shape in current model is torch.Size([7, 2048]).\n",
      "size mismatch for cls_head.fc_cls.bias: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([7]).\n",
      "2021-03-24 21:03:34,086 - mmaction - INFO - Start running, host: actrec@actrec-HP-Z4-G4-Workstation, work_dir: /home/actrec/.virtualenvs/mmaction/mmaction2/childact-checkpoints/childact-tsm\n",
      "2021-03-24 21:03:34,087 - mmaction - INFO - workflow: [('train', 1)], max: 51 epochs\n",
      "/home/actrec/.virtualenvs/mmaction/mmaction2/mmaction/core/evaluation/eval_hooks.py:131: UserWarning: runner.meta is None. Creating a empty one.\n",
      "  warnings.warn('runner.meta is None. Creating a empty one.')\n",
      "2021-03-24 21:06:25,694 - mmaction - INFO - Epoch [1][25/44]\tlr: 2.032e-02, eta: 4:13:51, time: 6.864, data_time: 5.883, memory: 20204, top1_acc: 0.4633, top5_acc: 0.9567, loss_cls: 1.3184, loss: 1.3184, grad_norm: 3.1978\n",
      "2021-03-24 21:11:00,838 - mmaction - INFO - Epoch [2][25/44]\tlr: 2.254e-02, eta: 3:01:16, time: 6.938, data_time: 5.963, memory: 20204, top1_acc: 0.7883, top5_acc: 0.9983, loss_cls: 0.5617, loss: 0.5617, grad_norm: 4.1688\n",
      "2021-03-24 21:15:26,642 - mmaction - INFO - Epoch [3][25/44]\tlr: 2.684e-02, eta: 2:42:07, time: 6.832, data_time: 5.857, memory: 20204, top1_acc: 0.8417, top5_acc: 1.0000, loss_cls: 0.4150, loss: 0.4150, grad_norm: 4.0095\n",
      "2021-03-24 21:19:46,715 - mmaction - INFO - Epoch [4][25/44]\tlr: 3.310e-02, eta: 2:30:22, time: 6.516, data_time: 5.540, memory: 20204, top1_acc: 0.8383, top5_acc: 0.9967, loss_cls: 0.4149, loss: 0.4149, grad_norm: 4.5450\n",
      "2021-03-24 21:21:35,996 - mmaction - INFO - Saving checkpoint at 4 epochs\n",
      "2021-03-24 21:24:21,918 - mmaction - INFO - Epoch [5][25/44]\tlr: 4.119e-02, eta: 2:23:03, time: 6.630, data_time: 5.657, memory: 20204, top1_acc: 0.8517, top5_acc: 0.9983, loss_cls: 0.3703, loss: 0.3703, grad_norm: 4.2365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 126/126, 2.6 task/s, elapsed: 48s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-24 21:26:56,186 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-03-24 21:26:56,188 - mmaction - INFO - \n",
      "top1_acc\t0.8016\n",
      "top5_acc\t1.0000\n",
      "2021-03-24 21:26:56,188 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-03-24 21:26:56,190 - mmaction - INFO - \n",
      "mean_acc\t0.8016\n",
      "2021-03-24 21:26:56,341 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_5.pth.\n",
      "2021-03-24 21:26:56,342 - mmaction - INFO - Best top1_acc is 0.8016 at 5 epoch.\n",
      "2021-03-24 21:26:56,343 - mmaction - INFO - Epoch(val) [5][44]\ttop1_acc: 0.8016, top5_acc: 1.0000, mean_class_accuracy: 0.8016\n",
      "2021-03-24 21:29:50,484 - mmaction - INFO - Epoch [6][25/44]\tlr: 5.091e-02, eta: 2:18:31, time: 6.965, data_time: 5.991, memory: 20204, top1_acc: 0.8633, top5_acc: 0.9983, loss_cls: 0.3578, loss: 0.3578, grad_norm: 4.3697\n",
      "2021-03-24 21:34:28,680 - mmaction - INFO - Epoch [7][25/44]\tlr: 6.203e-02, eta: 2:15:02, time: 7.168, data_time: 6.192, memory: 20204, top1_acc: 0.8700, top5_acc: 0.9983, loss_cls: 0.3594, loss: 0.3594, grad_norm: 3.4862\n",
      "2021-03-24 21:38:51,003 - mmaction - INFO - Epoch [8][25/44]\tlr: 7.429e-02, eta: 2:10:30, time: 6.668, data_time: 5.691, memory: 20204, top1_acc: 0.8433, top5_acc: 0.9983, loss_cls: 0.4086, loss: 0.4086, grad_norm: 3.8250\n",
      "2021-03-24 21:40:36,587 - mmaction - INFO - Saving checkpoint at 8 epochs\n",
      "2021-03-24 21:43:27,767 - mmaction - INFO - Epoch [9][25/44]\tlr: 8.739e-02, eta: 2:06:44, time: 6.841, data_time: 5.864, memory: 20204, top1_acc: 0.8467, top5_acc: 0.9983, loss_cls: 0.3808, loss: 0.3808, grad_norm: 3.2192\n",
      "2021-03-24 21:47:55,665 - mmaction - INFO - Epoch [10][25/44]\tlr: 1.010e-01, eta: 2:03:01, time: 6.763, data_time: 5.787, memory: 20204, top1_acc: 0.8433, top5_acc: 1.0000, loss_cls: 0.3620, loss: 0.3620, grad_norm: 3.3093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 126/126, 2.6 task/s, elapsed: 48s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-24 21:50:22,735 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-03-24 21:50:22,737 - mmaction - INFO - \n",
      "top1_acc\t0.8492\n",
      "top5_acc\t1.0000\n",
      "2021-03-24 21:50:22,737 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-03-24 21:50:22,739 - mmaction - INFO - \n",
      "mean_acc\t0.8492\n",
      "2021-03-24 21:50:22,912 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_10.pth.\n",
      "2021-03-24 21:50:22,912 - mmaction - INFO - Best top1_acc is 0.8492 at 10 epoch.\n",
      "2021-03-24 21:50:22,913 - mmaction - INFO - Epoch(val) [10][44]\ttop1_acc: 0.8492, top5_acc: 1.0000, mean_class_accuracy: 0.8492\n",
      "2021-03-24 21:53:16,024 - mmaction - INFO - Epoch [11][25/44]\tlr: 1.149e-01, eta: 1:59:43, time: 6.924, data_time: 5.948, memory: 20204, top1_acc: 0.8817, top5_acc: 1.0000, loss_cls: 0.3227, loss: 0.3227, grad_norm: 2.9154\n",
      "2021-03-24 21:57:42,847 - mmaction - INFO - Epoch [12][25/44]\tlr: 1.286e-01, eta: 1:56:20, time: 6.806, data_time: 5.830, memory: 20204, top1_acc: 0.8550, top5_acc: 0.9983, loss_cls: 0.3954, loss: 0.3954, grad_norm: 3.6179\n",
      "2021-03-24 21:59:27,183 - mmaction - INFO - Saving checkpoint at 12 epochs\n",
      "2021-03-24 22:02:22,179 - mmaction - INFO - Epoch [13][25/44]\tlr: 1.419e-01, eta: 1:53:16, time: 6.994, data_time: 6.020, memory: 20204, top1_acc: 0.8467, top5_acc: 0.9967, loss_cls: 0.4247, loss: 0.4247, grad_norm: 2.8231\n",
      "2021-03-24 22:06:52,903 - mmaction - INFO - Epoch [14][25/44]\tlr: 1.545e-01, eta: 1:50:05, time: 6.867, data_time: 5.891, memory: 20204, top1_acc: 0.8617, top5_acc: 1.0000, loss_cls: 0.3434, loss: 0.3434, grad_norm: 2.8790\n",
      "2021-03-24 22:11:20,068 - mmaction - INFO - Epoch [15][25/44]\tlr: 1.659e-01, eta: 1:46:49, time: 6.736, data_time: 5.761, memory: 20204, top1_acc: 0.8483, top5_acc: 1.0000, loss_cls: 0.3807, loss: 0.3807, grad_norm: 3.1518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 126/126, 2.6 task/s, elapsed: 48s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-24 22:13:51,448 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-03-24 22:13:51,450 - mmaction - INFO - \n",
      "top1_acc\t0.8254\n",
      "top5_acc\t1.0000\n",
      "2021-03-24 22:13:51,451 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-03-24 22:13:51,452 - mmaction - INFO - \n",
      "mean_acc\t0.8254\n",
      "2021-03-24 22:13:51,453 - mmaction - INFO - Epoch(val) [15][44]\ttop1_acc: 0.8254, top5_acc: 1.0000, mean_class_accuracy: 0.8254\n",
      "2021-03-24 22:16:43,943 - mmaction - INFO - Epoch [16][25/44]\tlr: 1.761e-01, eta: 1:43:45, time: 6.899, data_time: 5.905, memory: 20204, top1_acc: 0.8417, top5_acc: 1.0000, loss_cls: 0.3540, loss: 0.3540, grad_norm: 2.6687\n",
      "2021-03-24 22:18:21,773 - mmaction - INFO - Saving checkpoint at 16 epochs\n",
      "2021-03-24 22:21:12,732 - mmaction - INFO - Epoch [17][25/44]\tlr: 1.847e-01, eta: 1:40:39, time: 6.831, data_time: 5.806, memory: 20204, top1_acc: 0.8600, top5_acc: 1.0000, loss_cls: 0.3870, loss: 0.3870, grad_norm: 3.2032\n",
      "2021-03-24 22:25:49,296 - mmaction - INFO - Epoch [18][25/44]\tlr: 1.915e-01, eta: 1:37:45, time: 7.039, data_time: 6.065, memory: 20204, top1_acc: 0.8467, top5_acc: 1.0000, loss_cls: 0.3955, loss: 0.3955, grad_norm: 2.7949\n",
      "2021-03-24 22:30:21,211 - mmaction - INFO - Epoch [19][25/44]\tlr: 1.964e-01, eta: 1:34:43, time: 6.867, data_time: 5.877, memory: 20204, top1_acc: 0.8483, top5_acc: 0.9967, loss_cls: 0.4092, loss: 0.4092, grad_norm: 2.6388\n",
      "2021-03-24 22:34:51,821 - mmaction - INFO - Epoch [20][25/44]\tlr: 1.992e-01, eta: 1:31:41, time: 6.859, data_time: 5.879, memory: 20204, top1_acc: 0.8667, top5_acc: 1.0000, loss_cls: 0.3351, loss: 0.3351, grad_norm: 2.3010\n",
      "2021-03-24 22:36:29,026 - mmaction - INFO - Saving checkpoint at 20 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 126/126, 2.6 task/s, elapsed: 48s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-24 22:37:17,321 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-03-24 22:37:17,323 - mmaction - INFO - \n",
      "top1_acc\t0.8254\n",
      "top5_acc\t1.0000\n",
      "2021-03-24 22:37:17,323 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-03-24 22:37:17,325 - mmaction - INFO - \n",
      "mean_acc\t0.8254\n",
      "2021-03-24 22:37:17,326 - mmaction - INFO - Epoch(val) [20][44]\ttop1_acc: 0.8254, top5_acc: 1.0000, mean_class_accuracy: 0.8254\n",
      "2021-03-24 22:40:03,624 - mmaction - INFO - Epoch [21][25/44]\tlr: 2.000e-01, eta: 1:28:33, time: 6.652, data_time: 5.674, memory: 20204, top1_acc: 0.8833, top5_acc: 1.0000, loss_cls: 0.2851, loss: 0.2851, grad_norm: 1.9111\n",
      "2021-03-24 22:44:38,747 - mmaction - INFO - Epoch [22][25/44]\tlr: 1.993e-01, eta: 1:25:35, time: 6.870, data_time: 5.893, memory: 20204, top1_acc: 0.8867, top5_acc: 1.0000, loss_cls: 0.2601, loss: 0.2601, grad_norm: 2.1638\n",
      "2021-03-24 22:49:22,392 - mmaction - INFO - Epoch [23][25/44]\tlr: 1.976e-01, eta: 1:22:50, time: 7.289, data_time: 6.287, memory: 20204, top1_acc: 0.8250, top5_acc: 1.0000, loss_cls: 0.5038, loss: 0.5038, grad_norm: 3.5063\n",
      "2021-03-24 22:53:49,042 - mmaction - INFO - Epoch [24][25/44]\tlr: 1.948e-01, eta: 1:19:52, time: 6.871, data_time: 5.877, memory: 20204, top1_acc: 0.8867, top5_acc: 0.9983, loss_cls: 0.2872, loss: 0.2872, grad_norm: 2.0569\n",
      "2021-03-24 22:55:28,710 - mmaction - INFO - Saving checkpoint at 24 epochs\n",
      "2021-03-24 22:58:30,402 - mmaction - INFO - Epoch [25][25/44]\tlr: 1.910e-01, eta: 1:17:04, time: 7.261, data_time: 6.271, memory: 20204, top1_acc: 0.8983, top5_acc: 1.0000, loss_cls: 0.2459, loss: 0.2459, grad_norm: 2.1627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 126/126, 2.6 task/s, elapsed: 48s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-24 23:00:59,644 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-03-24 23:00:59,647 - mmaction - INFO - \n",
      "top1_acc\t0.8175\n",
      "top5_acc\t1.0000\n",
      "2021-03-24 23:00:59,648 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-03-24 23:00:59,651 - mmaction - INFO - \n",
      "mean_acc\t0.8175\n",
      "2021-03-24 23:00:59,652 - mmaction - INFO - Epoch(val) [25][44]\ttop1_acc: 0.8175, top5_acc: 1.0000, mean_class_accuracy: 0.8175\n",
      "2021-03-24 23:03:51,401 - mmaction - INFO - Epoch [26][25/44]\tlr: 1.863e-01, eta: 1:14:06, time: 6.870, data_time: 5.887, memory: 20204, top1_acc: 0.8933, top5_acc: 1.0000, loss_cls: 0.2764, loss: 0.2764, grad_norm: 2.1380\n",
      "2021-03-24 23:08:27,656 - mmaction - INFO - Epoch [27][25/44]\tlr: 1.807e-01, eta: 1:11:07, time: 6.820, data_time: 5.837, memory: 20204, top1_acc: 0.8933, top5_acc: 0.9983, loss_cls: 0.2690, loss: 0.2690, grad_norm: 2.0275\n",
      "2021-03-24 23:12:53,090 - mmaction - INFO - Epoch [28][25/44]\tlr: 1.742e-01, eta: 1:08:04, time: 6.593, data_time: 5.600, memory: 20204, top1_acc: 0.9167, top5_acc: 1.0000, loss_cls: 0.2153, loss: 0.2153, grad_norm: 1.7606\n",
      "2021-03-24 23:14:46,727 - mmaction - INFO - Saving checkpoint at 28 epochs\n",
      "2021-03-24 23:17:38,587 - mmaction - INFO - Epoch [29][25/44]\tlr: 1.669e-01, eta: 1:05:08, time: 6.868, data_time: 5.887, memory: 20204, top1_acc: 0.8950, top5_acc: 1.0000, loss_cls: 0.2431, loss: 0.2431, grad_norm: 1.8443\n",
      "2021-03-24 23:22:12,771 - mmaction - INFO - Epoch [30][25/44]\tlr: 1.590e-01, eta: 1:02:12, time: 6.900, data_time: 5.912, memory: 20204, top1_acc: 0.9117, top5_acc: 1.0000, loss_cls: 0.2420, loss: 0.2420, grad_norm: 2.0150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 126/126, 2.6 task/s, elapsed: 49s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-24 23:24:46,975 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-03-24 23:24:46,977 - mmaction - INFO - \n",
      "top1_acc\t0.8730\n",
      "top5_acc\t1.0000\n",
      "2021-03-24 23:24:46,977 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-03-24 23:24:46,979 - mmaction - INFO - \n",
      "mean_acc\t0.8730\n",
      "2021-03-24 23:24:47,130 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_30.pth.\n",
      "2021-03-24 23:24:47,131 - mmaction - INFO - Best top1_acc is 0.8730 at 30 epoch.\n",
      "2021-03-24 23:24:47,132 - mmaction - INFO - Epoch(val) [30][44]\ttop1_acc: 0.8730, top5_acc: 1.0000, mean_class_accuracy: 0.8730\n",
      "2021-03-24 23:27:36,817 - mmaction - INFO - Epoch [31][25/44]\tlr: 1.504e-01, eta: 0:59:15, time: 6.787, data_time: 5.808, memory: 20204, top1_acc: 0.9150, top5_acc: 0.9983, loss_cls: 0.2196, loss: 0.2196, grad_norm: 1.8701\n",
      "2021-03-24 23:32:14,860 - mmaction - INFO - Epoch [32][25/44]\tlr: 1.413e-01, eta: 0:56:22, time: 7.025, data_time: 6.044, memory: 20204, top1_acc: 0.9217, top5_acc: 1.0000, loss_cls: 0.1901, loss: 0.1901, grad_norm: 1.7479\n",
      "2021-03-24 23:33:52,908 - mmaction - INFO - Saving checkpoint at 32 epochs\n",
      "2021-03-24 23:36:38,792 - mmaction - INFO - Epoch [33][25/44]\tlr: 1.317e-01, eta: 0:53:23, time: 6.629, data_time: 5.654, memory: 20204, top1_acc: 0.9033, top5_acc: 0.9983, loss_cls: 0.2275, loss: 0.2275, grad_norm: 1.9888\n",
      "2021-03-24 23:41:10,659 - mmaction - INFO - Epoch [34][25/44]\tlr: 1.219e-01, eta: 0:50:26, time: 6.672, data_time: 5.694, memory: 20204, top1_acc: 0.9117, top5_acc: 1.0000, loss_cls: 0.2229, loss: 0.2229, grad_norm: 1.9745\n",
      "2021-03-24 23:45:50,226 - mmaction - INFO - Epoch [35][25/44]\tlr: 1.118e-01, eta: 0:47:31, time: 6.810, data_time: 5.810, memory: 20204, top1_acc: 0.9350, top5_acc: 1.0000, loss_cls: 0.1698, loss: 0.1698, grad_norm: 1.7408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 126/126, 2.6 task/s, elapsed: 49s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-24 23:48:22,662 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-03-24 23:48:22,664 - mmaction - INFO - \n",
      "top1_acc\t0.8254\n",
      "top5_acc\t1.0000\n",
      "2021-03-24 23:48:22,665 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-03-24 23:48:22,667 - mmaction - INFO - \n",
      "mean_acc\t0.8254\n",
      "2021-03-24 23:48:22,668 - mmaction - INFO - Epoch(val) [35][44]\ttop1_acc: 0.8254, top5_acc: 1.0000, mean_class_accuracy: 0.8254\n",
      "2021-03-24 23:51:14,376 - mmaction - INFO - Epoch [36][25/44]\tlr: 1.015e-01, eta: 0:44:36, time: 6.868, data_time: 5.877, memory: 20204, top1_acc: 0.9167, top5_acc: 1.0000, loss_cls: 0.1841, loss: 0.1841, grad_norm: 2.0693\n",
      "2021-03-24 23:52:51,813 - mmaction - INFO - Saving checkpoint at 36 epochs\n",
      "2021-03-24 23:55:40,270 - mmaction - INFO - Epoch [37][25/44]\tlr: 9.127e-02, eta: 0:41:41, time: 6.732, data_time: 5.754, memory: 20204, top1_acc: 0.9533, top5_acc: 1.0000, loss_cls: 0.1247, loss: 0.1247, grad_norm: 1.3474\n",
      "2021-03-25 00:00:14,754 - mmaction - INFO - Epoch [38][25/44]\tlr: 8.111e-02, eta: 0:38:47, time: 6.818, data_time: 5.801, memory: 20204, top1_acc: 0.9250, top5_acc: 1.0000, loss_cls: 0.1797, loss: 0.1797, grad_norm: 2.0569\n",
      "2021-03-25 00:04:50,878 - mmaction - INFO - Epoch [39][25/44]\tlr: 7.115e-02, eta: 0:35:52, time: 6.759, data_time: 5.761, memory: 20204, top1_acc: 0.9433, top5_acc: 1.0000, loss_cls: 0.1380, loss: 0.1380, grad_norm: 1.4081\n",
      "2021-03-25 00:09:30,047 - mmaction - INFO - Epoch [40][25/44]\tlr: 6.149e-02, eta: 0:32:59, time: 6.947, data_time: 5.930, memory: 20204, top1_acc: 0.9483, top5_acc: 1.0000, loss_cls: 0.1448, loss: 0.1448, grad_norm: 1.5499\n",
      "2021-03-25 00:11:11,701 - mmaction - INFO - Saving checkpoint at 40 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 126/126, 2.6 task/s, elapsed: 48s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-25 00:11:59,894 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-03-25 00:11:59,896 - mmaction - INFO - \n",
      "top1_acc\t0.9206\n",
      "top5_acc\t1.0000\n",
      "2021-03-25 00:11:59,897 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-03-25 00:11:59,898 - mmaction - INFO - \n",
      "mean_acc\t0.9206\n",
      "2021-03-25 00:12:00,066 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_40.pth.\n",
      "2021-03-25 00:12:00,067 - mmaction - INFO - Best top1_acc is 0.9206 at 40 epoch.\n",
      "2021-03-25 00:12:00,068 - mmaction - INFO - Epoch(val) [40][44]\ttop1_acc: 0.9206, top5_acc: 1.0000, mean_class_accuracy: 0.9206\n",
      "2021-03-25 00:15:02,371 - mmaction - INFO - Epoch [41][25/44]\tlr: 5.224e-02, eta: 0:30:08, time: 7.292, data_time: 6.300, memory: 20204, top1_acc: 0.9417, top5_acc: 1.0000, loss_cls: 0.1471, loss: 0.1471, grad_norm: 1.4060\n",
      "2021-03-25 00:19:40,726 - mmaction - INFO - Epoch [42][25/44]\tlr: 4.349e-02, eta: 0:27:16, time: 7.081, data_time: 6.099, memory: 20204, top1_acc: 0.9417, top5_acc: 0.9983, loss_cls: 0.1504, loss: 0.1504, grad_norm: 1.7364\n",
      "2021-03-25 00:24:19,106 - mmaction - INFO - Epoch [43][25/44]\tlr: 3.534e-02, eta: 0:24:23, time: 7.103, data_time: 6.101, memory: 20204, top1_acc: 0.9533, top5_acc: 0.9983, loss_cls: 0.1330, loss: 0.1330, grad_norm: 1.5412\n",
      "2021-03-25 00:29:08,938 - mmaction - INFO - Epoch [44][25/44]\tlr: 2.786e-02, eta: 0:21:32, time: 7.522, data_time: 6.529, memory: 20204, top1_acc: 0.9633, top5_acc: 1.0000, loss_cls: 0.1166, loss: 0.1166, grad_norm: 1.6169\n",
      "2021-03-25 00:31:25,146 - mmaction - INFO - Saving checkpoint at 44 epochs\n",
      "2021-03-25 00:35:51,517 - mmaction - INFO - Epoch [45][25/44]\tlr: 2.115e-02, eta: 0:18:51, time: 10.648, data_time: 9.640, memory: 20204, top1_acc: 0.9550, top5_acc: 1.0000, loss_cls: 0.1182, loss: 0.1182, grad_norm: 1.4543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 126/126, 2.6 task/s, elapsed: 49s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-25 00:39:22,798 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-03-25 00:39:22,800 - mmaction - INFO - \n",
      "top1_acc\t0.9048\n",
      "top5_acc\t1.0000\n",
      "2021-03-25 00:39:22,800 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-03-25 00:39:22,802 - mmaction - INFO - \n",
      "mean_acc\t0.9048\n",
      "2021-03-25 00:39:22,803 - mmaction - INFO - Epoch(val) [45][44]\ttop1_acc: 0.9048, top5_acc: 1.0000, mean_class_accuracy: 0.9048\n",
      "2021-03-25 00:42:20,754 - mmaction - INFO - Epoch [46][25/44]\tlr: 1.526e-02, eta: 0:15:56, time: 7.118, data_time: 6.136, memory: 20204, top1_acc: 0.9550, top5_acc: 1.0000, loss_cls: 0.1165, loss: 0.1165, grad_norm: 1.5267\n",
      "2021-03-25 00:46:57,192 - mmaction - INFO - Epoch [47][25/44]\tlr: 1.027e-02, eta: 0:12:59, time: 7.019, data_time: 6.046, memory: 20204, top1_acc: 0.9633, top5_acc: 1.0000, loss_cls: 0.0944, loss: 0.0944, grad_norm: 1.2650\n",
      "2021-03-25 00:51:21,230 - mmaction - INFO - Epoch [48][25/44]\tlr: 6.220e-03, eta: 0:10:03, time: 6.794, data_time: 5.818, memory: 20204, top1_acc: 0.9600, top5_acc: 1.0000, loss_cls: 0.0976, loss: 0.0976, grad_norm: 1.2563\n",
      "2021-03-25 00:53:03,163 - mmaction - INFO - Saving checkpoint at 48 epochs\n",
      "2021-03-25 00:55:47,302 - mmaction - INFO - Epoch [49][25/44]\tlr: 3.158e-03, eta: 0:07:07, time: 6.559, data_time: 5.584, memory: 20204, top1_acc: 0.9550, top5_acc: 1.0000, loss_cls: 0.1207, loss: 0.1207, grad_norm: 1.5171\n",
      "2021-03-25 01:00:24,345 - mmaction - INFO - Epoch [50][25/44]\tlr: 1.114e-03, eta: 0:04:11, time: 6.680, data_time: 5.703, memory: 20204, top1_acc: 0.9500, top5_acc: 1.0000, loss_cls: 0.1192, loss: 0.1192, grad_norm: 1.5985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 126/126, 1.3 task/s, elapsed: 98s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-25 01:03:51,321 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-03-25 01:03:51,323 - mmaction - INFO - \n",
      "top1_acc\t0.9127\n",
      "top5_acc\t1.0000\n",
      "2021-03-25 01:03:51,324 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-03-25 01:03:51,326 - mmaction - INFO - \n",
      "mean_acc\t0.9127\n",
      "2021-03-25 01:03:51,327 - mmaction - INFO - Epoch(val) [50][44]\ttop1_acc: 0.9127, top5_acc: 1.0000, mean_class_accuracy: 0.9127\n",
      "2021-03-25 01:06:43,848 - mmaction - INFO - Epoch [51][25/44]\tlr: 1.108e-04, eta: 0:01:15, time: 6.901, data_time: 5.915, memory: 20204, top1_acc: 0.9583, top5_acc: 1.0000, loss_cls: 0.0970, loss: 0.0970, grad_norm: 1.2868\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "from mmaction.datasets import build_dataset\n",
    "from mmaction.models import build_model\n",
    "from mmaction.apis import train_model\n",
    "\n",
    "import mmcv\n",
    "\n",
    "# Build the dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# Build the recognizer\n",
    "model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.train_cfg, test_cfg=cfg.test_cfg)\n",
    "\n",
    "# Create work_dir\n",
    "mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
    "train_model(model, datasets, cfg, distributed=False, validate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f\"{cfg.work_dir}/model50e\", 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eyY3hCMwyTct",
    "outputId": "200e37c7-0da4-421f-98da-41418c3110ca",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 126/126, 1.1 task/s, elapsed: 120s, ETA:     0s\n",
      "Evaluating top_k_accuracy ...\n",
      "\n",
      "top1_acc\t0.9365\n",
      "top5_acc\t1.0000\n",
      "\n",
      "Evaluating mean_class_accuracy ...\n",
      "\n",
      "mean_acc\t0.9365\n",
      "top1_acc: 0.9365\n",
      "top5_acc: 1.0000\n",
      "mean_class_accuracy: 0.9365\n"
     ]
    }
   ],
   "source": [
    "from mmaction.apis import single_gpu_test\n",
    "from mmaction.datasets import build_dataloader\n",
    "from mmcv.parallel import MMDataParallel\n",
    "from mmaction.models import build_model\n",
    "from mmaction.datasets import build_dataset\n",
    "\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# Build a test dataloader\n",
    "dataset = build_dataset(cfg.data.test, dict(test_mode=True))\n",
    "data_loader = build_dataloader(\n",
    "        dataset,\n",
    "        videos_per_gpu=4,\n",
    "        workers_per_gpu=1,\n",
    "        dist=False,\n",
    "        shuffle=False)\n",
    "model = MMDataParallel(model, device_ids=[0])\n",
    "outputs = single_gpu_test(model, data_loader)\n",
    "\n",
    "eval_config = cfg.evaluation\n",
    "eval_config.pop('interval')\n",
    "eval_res = dataset.evaluate(outputs, **eval_config)\n",
    "for name, val in eval_res.items():\n",
    "    print(f'{name}: {val:.04f}')\n",
    "    \n",
    "output_config = cfg.output_config\n",
    "dataset.dump_results(outputs, **output_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from mmaction.core import confusion_matrix \n",
    "\n",
    "gt_labels = [ann['label'] for ann in dataset.load_annotations()]\n",
    "pred = np.argmax(outputs, axis=1)\n",
    "cf_mat = confusion_matrix(pred, gt_labels).astype(float)\n",
    "\n",
    "sns.heatmap(cf_mat, annot=True, xticklabels = ['box', 'clap', 'go', 'jog', 'run', 'walk', 'wave'], yticklabels = ['box', 'clap', 'go', 'jog', 'run', 'walk', 'wave'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "# CSN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "64CW6d_AaT-Q",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "3b284fd8-4ee7-4a34-90d7-5023cd123a04",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-03-28 21:54:04--  https://download.openmmlab.com/mmaction/recognition/csn/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth\n",
      "Resolving download.openmmlab.com (download.openmmlab.com)... 47.254.186.225\n",
      "Connecting to download.openmmlab.com (download.openmmlab.com)|47.254.186.225|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 119580180 (114M) [application/octet-stream]\n",
      "Saving to: ‘checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth’\n",
      "\n",
      "checkpoints/ircsn_i 100%[===================>] 114,04M  7,32MB/s    in 18s     \n",
      "\n",
      "2021-03-28 21:54:27 (6,46 MB/s) - ‘checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth’ saved [119580180/119580180]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !mkdir checkpoints\n",
    "!wget -c https://download.openmmlab.com/mmaction/recognition/csn/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth \\\n",
    "      -O checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "LjCcmCKOjktc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mmcv import Config\n",
    "cfg = Config.fromfile('./configs/recognition/csn/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "tlhu9byjjt-K",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "13d1bb01-f351-48c0-9efb-0bdf2c125d29",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "model = dict(\n",
      "    type='Recognizer3D',\n",
      "    backbone=dict(\n",
      "        type='ResNet3dCSN',\n",
      "        pretrained2d=False,\n",
      "        pretrained=\n",
      "        'https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth',\n",
      "        depth=152,\n",
      "        with_pool2=False,\n",
      "        bottleneck_mode='ir',\n",
      "        norm_eval=True,\n",
      "        zero_init_residual=False,\n",
      "        bn_frozen=True),\n",
      "    cls_head=dict(\n",
      "        type='I3DHead',\n",
      "        num_classes=7,\n",
      "        in_channels=2048,\n",
      "        spatial_type='avg',\n",
      "        dropout_ratio=0.5,\n",
      "        init_std=0.01),\n",
      "    train_cfg=None,\n",
      "    test_cfg=dict(average_clips='prob'))\n",
      "dataset_type = 'RawframeDataset'\n",
      "data_root = 'data/childact_rawframe/train/'\n",
      "data_root_val = 'data/childact_rawframe/val/'\n",
      "ann_file_train = 'data/childact_rawframe/childact_train_rawframe.txt'\n",
      "ann_file_val = 'data/childact_rawframe/childact_val_rawframe.txt'\n",
      "ann_file_test = 'data/childact_rawframe/childact_test_rawframe.txt'\n",
      "img_norm_cfg = dict(\n",
      "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_bgr=False)\n",
      "train_pipeline = [\n",
      "    dict(type='SampleFrames', clip_len=32, frame_interval=2, num_clips=1),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='RandomResizedCrop'),\n",
      "    dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "    dict(type='Flip', flip_ratio=0.5),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCHW_Flow'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "]\n",
      "val_pipeline = [\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=32,\n",
      "        frame_interval=2,\n",
      "        num_clips=1,\n",
      "        test_mode=True),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='CenterCrop', crop_size=224),\n",
      "    dict(type='Flip', flip_ratio=0),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCHW_Flow'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=32,\n",
      "        frame_interval=2,\n",
      "        num_clips=10,\n",
      "        test_mode=True),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='ThreeCrop', crop_size=256),\n",
      "    dict(type='Flip', flip_ratio=0),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCHW_Flow'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "data = dict(\n",
      "    videos_per_gpu=3,\n",
      "    workers_per_gpu=4,\n",
      "    train=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file='data/childact_rawframe/childact_train_rawframe.txt',\n",
      "        data_prefix='data/childact_rawframe/train/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=1),\n",
      "            dict(type='FrameSelector'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='RandomResizedCrop'),\n",
      "            dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "            dict(type='Flip', flip_ratio=0.5),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "        ],\n",
      "        modality='Flow',\n",
      "        start_index=0,\n",
      "        filename_tmpl='flow_{}_{:05d}.jpg'),\n",
      "    val=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file='data/childact_rawframe/childact_val_rawframe.txt',\n",
      "        data_prefix='data/childact_rawframe/val/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=1,\n",
      "                test_mode=True),\n",
      "            dict(type='FrameSelector'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='CenterCrop', crop_size=224),\n",
      "            dict(type='Flip', flip_ratio=0),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ],\n",
      "        modality='Flow',\n",
      "        start_index=0,\n",
      "        filename_tmpl='flow_{}_{:05d}.jpg'),\n",
      "    test=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file='data/childact_rawframe/childact_test_rawframe.txt',\n",
      "        data_prefix='data/childact_rawframe/test/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=10,\n",
      "                test_mode=True),\n",
      "            dict(type='FrameSelector'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='ThreeCrop', crop_size=256),\n",
      "            dict(type='Flip', flip_ratio=0),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ],\n",
      "        modality='Flow',\n",
      "        start_index=0,\n",
      "        filename_tmpl='flow_{}_{:05d}.jpg'))\n",
      "optimizer = dict(type='SGD', lr=0.000125, momentum=0.9, weight_decay=0.0001)\n",
      "optimizer_config = dict(grad_clip=dict(max_norm=40, norm_type=2))\n",
      "lr_config = dict(\n",
      "    policy='cyclic',\n",
      "    target_ratio=(10, 1e-05),\n",
      "    cyclic_times=1,\n",
      "    step_ratio_up=0.4)\n",
      "total_epochs = 51\n",
      "checkpoint_config = dict(interval=12)\n",
      "evaluation = dict(\n",
      "    interval=5, metrics=['top_k_accuracy', 'mean_class_accuracy'])\n",
      "log_config = dict(\n",
      "    interval=25,\n",
      "    hooks=[dict(type='TextLoggerHook'),\n",
      "           dict(type='TensorboardLoggerHook')])\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "work_dir = './childact-checkpoints/childact-csn'\n",
      "load_from = 'checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth'\n",
      "resume_from = None\n",
      "workflow = [('train', 1)]\n",
      "find_unused_parameters = True\n",
      "omnisource = False\n",
      "momentum_config = dict(\n",
      "    policy='cyclic',\n",
      "    target_ratio=(0.8947368421052632, 1),\n",
      "    cyclic_times=1,\n",
      "    step_ratio_up=0.4)\n",
      "seed = 42\n",
      "gpu_ids = range(0, 1)\n",
      "output_config = dict(out='./childact-checkpoints/childact-csn/results.json')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mmcv.runner import set_random_seed\n",
    "\n",
    "# Modify dataset type and path\n",
    "# cfg.dataset_type = 'VideoDataset'\n",
    "cfg.data_root = 'data/childact_rawframe/train/'\n",
    "cfg.data_root_val = 'data/childact_rawframe/val/'\n",
    "cfg.ann_file_train = 'data/childact_rawframe/childact_train_rawframe.txt'\n",
    "cfg.ann_file_val = 'data/childact_rawframe/childact_val_rawframe.txt'\n",
    "cfg.ann_file_test = 'data/childact_rawframe/childact_test_rawframe.txt'\n",
    "\n",
    "# cfg.data.test.type = 'VideoDataset'\n",
    "cfg.data.test.ann_file = 'data/childact_rawframe/childact_test_rawframe.txt'\n",
    "cfg.data.test.data_prefix = 'data/childact_rawframe/test/'\n",
    "\n",
    "# cfg.data.train.type = 'VideoDataset'\n",
    "cfg.data.train.ann_file = 'data/childact_rawframe/childact_train_rawframe.txt'\n",
    "cfg.data.train.data_prefix = 'data/childact_rawframe/train/'\n",
    "\n",
    "# cfg.data.val.type = 'VideoDataset'\n",
    "cfg.data.val.ann_file = 'data/childact_rawframe/childact_val_rawframe.txt'\n",
    "cfg.data.val.data_prefix = 'data/childact_rawframe/val/'\n",
    "\n",
    "cfg.data.test.modality = 'Flow'\n",
    "cfg.data.val.modality = 'Flow'\n",
    "cfg.data.train.modality = 'Flow'\n",
    "\n",
    "cfg.data.train.start_index = 0\n",
    "cfg.data.test.start_index = 0\n",
    "cfg.data.val.start_index = 0\n",
    "\n",
    "cfg.data.test.filename_tmpl = 'flow_{}_{:05d}.jpg'\n",
    "cfg.data.train.filename_tmpl = 'flow_{}_{:05d}.jpg'\n",
    "cfg.data.val.filename_tmpl = 'flow_{}_{:05d}.jpg'\n",
    "# The flag is used to determine whether it is omnisource training\n",
    "cfg.setdefault('omnisource', False)\n",
    "# Modify num classes of the model in cls_head\n",
    "cfg.model.cls_head.num_classes = 7\n",
    "# We can use the pre-trained TSN model\n",
    "cfg.load_from = 'checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth'\n",
    "# cfg.resume_from = './childact-mm/latest.pth'\n",
    "\n",
    "# Set up working dir to save files and logs.\n",
    "cfg.work_dir = './childact-checkpoints/childact-csn'\n",
    "\n",
    "cfg.train_pipeline[1] = dict(type='RawFrameDecode')\n",
    "cfg.test_pipeline[1] = dict(type='RawFrameDecode')\n",
    "cfg.val_pipeline[1] = dict(type='RawFrameDecode')\n",
    "\n",
    "cfg.train_pipeline[7].input_format = 'NCHW_Flow'\n",
    "cfg.test_pipeline[6].input_format = 'NCHW_Flow'\n",
    "cfg.val_pipeline[6].input_format = 'NCHW_Flow'\n",
    "\n",
    "# The original learning rate (LR) is set for 8-GPU training.\n",
    "# We divide it by 8 since we only use one GPU.\n",
    "# cfg.data.videos_per_gpu = 24\n",
    "# cfg.optimizer.type = 'Adam'\n",
    "# cfg.optimizer.weight_decay=0.0001\n",
    "\n",
    "# cfg.optimizer_config.grad_clip=None\n",
    "# cfg.optimizer.lr = 0.01\n",
    "\n",
    "cfg.lr_config = dict(\n",
    "    policy='cyclic',\n",
    "    target_ratio=(10, 1e-5),\n",
    "    cyclic_times=1,\n",
    "    step_ratio_up=0.4,\n",
    ")\n",
    "\n",
    "cfg.total_epochs = 51\n",
    "\n",
    "cfg.momentum_config = dict(\n",
    "    policy='cyclic',\n",
    "    target_ratio=(0.85 / 0.95, 1),\n",
    "    cyclic_times=1,\n",
    "    step_ratio_up=0.4,\n",
    ")\n",
    "\n",
    "# We can set the checkpoint saving interval to reduce the storage cost\n",
    "cfg.checkpoint_config.interval = 12\n",
    "# We can set the log print interval to reduce the the times of printing log\n",
    "cfg.log_config.interval = 25\n",
    "\n",
    "# Set seed thus the results are more reproducible\n",
    "cfg.seed = 42\n",
    "set_random_seed(42, deterministic=False)\n",
    "cfg.gpu_ids = range(1)\n",
    "\n",
    "cfg.output_config = dict(out=f'{cfg.work_dir}/results.json')\n",
    "# We can initialize the logger for training and have a look\n",
    "# at the final config used for training\n",
    "# del cfg.optimizer['momentum']\n",
    "print(f'Config:\\n{cfg.pretty_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "dDBWkdDRk6oz",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "85a52ef3-7b5c-4c52-8fef-00322a8c65e6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-29 00:08:15,510 - mmaction - INFO - load model from: https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth\n",
      "2021-03-29 00:08:15,511 - mmaction - INFO - Use load_from_http loader\n",
      "2021-03-29 00:08:15,830 - mmaction - INFO - load checkpoint from checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth\n",
      "2021-03-29 00:08:15,831 - mmaction - INFO - Use load_from_local loader\n",
      "2021-03-29 00:08:15,981 - mmaction - WARNING - The model and loaded state dict do not match exactly\n",
      "\n",
      "size mismatch for cls_head.fc_cls.weight: copying a param with shape torch.Size([400, 2048]) from checkpoint, the shape in current model is torch.Size([7, 2048]).\n",
      "size mismatch for cls_head.fc_cls.bias: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([7]).\n",
      "2021-03-29 00:08:15,983 - mmaction - INFO - Start running, host: actrec@actrec-HP-Z4-G4-Workstation, work_dir: /home/actrec/.virtualenvs/mmaction/mmaction2/childact-checkpoints/childact-csn\n",
      "2021-03-29 00:08:15,984 - mmaction - INFO - workflow: [('train', 1)], max: 51 epochs\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Caught AssertionError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/actrec/.virtualenvs/mmaction/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py\", line 198, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/actrec/.virtualenvs/mmaction/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/actrec/.virtualenvs/mmaction/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/actrec/.virtualenvs/mmaction/mmaction2/mmaction/datasets/base.py\", line 280, in __getitem__\n    return self.prepare_train_frames(idx)\n  File \"/home/actrec/.virtualenvs/mmaction/mmaction2/mmaction/datasets/rawframe_dataset.py\", line 170, in prepare_train_frames\n    return self.pipeline(results)\n  File \"/home/actrec/.virtualenvs/mmaction/mmaction2/mmaction/datasets/pipelines/compose.py\", line 41, in __call__\n    data = t(data)\n  File \"/home/actrec/.virtualenvs/mmaction/mmaction2/mmaction/datasets/pipelines/augmentations.py\", line 1243, in __call__\n    assert self.mean.shape[0] == 2\nAssertionError\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-be92e24cff32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Create work_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mmmcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir_or_exist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mosp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwork_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistributed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.virtualenvs/mmaction/mmaction2/mmaction/apis/train.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dataset, cfg, distributed, validate, timestamp, meta)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0momnisource\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0mrunner_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_ratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkflow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mrunner_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.virtualenvs/mmaction/lib/python3.6/site-packages/mmcv/runner/epoch_based_runner.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data_loaders, workflow, max_epochs, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_epochs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m                     \u001b[0mepoch_runner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# wait for some hooks like loggers to finish\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/mmaction/lib/python3.6/site-packages/mmcv/runner/epoch_based_runner.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_loader, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'before_train_epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Prevent possible deadlock during epoch transition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inner_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'before_train_iter'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/mmaction/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/mmaction/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1083\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1085\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/mmaction/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1111\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1112\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/mmaction/lib/python3.6/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    426\u001b[0m             \u001b[0;31m# have message field\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Caught AssertionError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/actrec/.virtualenvs/mmaction/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py\", line 198, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/actrec/.virtualenvs/mmaction/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/actrec/.virtualenvs/mmaction/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/actrec/.virtualenvs/mmaction/mmaction2/mmaction/datasets/base.py\", line 280, in __getitem__\n    return self.prepare_train_frames(idx)\n  File \"/home/actrec/.virtualenvs/mmaction/mmaction2/mmaction/datasets/rawframe_dataset.py\", line 170, in prepare_train_frames\n    return self.pipeline(results)\n  File \"/home/actrec/.virtualenvs/mmaction/mmaction2/mmaction/datasets/pipelines/compose.py\", line 41, in __call__\n    data = t(data)\n  File \"/home/actrec/.virtualenvs/mmaction/mmaction2/mmaction/datasets/pipelines/augmentations.py\", line 1243, in __call__\n    assert self.mean.shape[0] == 2\nAssertionError\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "from mmaction.datasets import build_dataset\n",
    "from mmaction.models import build_model\n",
    "from mmaction.apis import train_model\n",
    "\n",
    "import mmcv\n",
    "\n",
    "# Build the dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# Build the recognizer\n",
    "model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.train_cfg, test_cfg=cfg.test_cfg)\n",
    "\n",
    "# Create work_dir\n",
    "mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
    "train_model(model, datasets, cfg, distributed=False, validate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f\"{cfg.work_dir}/model50e\", 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eyY3hCMwyTct",
    "outputId": "200e37c7-0da4-421f-98da-41418c3110ca",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 126/126, 1.1 task/s, elapsed: 120s, ETA:     0s\n",
      "Evaluating top_k_accuracy ...\n",
      "\n",
      "top1_acc\t0.9365\n",
      "top5_acc\t1.0000\n",
      "\n",
      "Evaluating mean_class_accuracy ...\n",
      "\n",
      "mean_acc\t0.9365\n",
      "top1_acc: 0.9365\n",
      "top5_acc: 1.0000\n",
      "mean_class_accuracy: 0.9365\n"
     ]
    }
   ],
   "source": [
    "from mmaction.apis import single_gpu_test\n",
    "from mmaction.datasets import build_dataloader\n",
    "from mmcv.parallel import MMDataParallel\n",
    "from mmaction.models import build_model\n",
    "from mmaction.datasets import build_dataset\n",
    "\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# Build a test dataloader\n",
    "dataset = build_dataset(cfg.data.test, dict(test_mode=True))\n",
    "data_loader = build_dataloader(\n",
    "        dataset,\n",
    "        videos_per_gpu=4,\n",
    "        workers_per_gpu=1,\n",
    "        dist=False,\n",
    "        shuffle=False)\n",
    "model = MMDataParallel(model, device_ids=[0])\n",
    "outputs = single_gpu_test(model, data_loader)\n",
    "\n",
    "eval_config = cfg.evaluation\n",
    "eval_config.pop('interval')\n",
    "eval_res = dataset.evaluate(outputs, **eval_config)\n",
    "for name, val in eval_res.items():\n",
    "    print(f'{name}: {val:.04f}')\n",
    "    \n",
    "output_config = cfg.output_config\n",
    "dataset.dump_results(outputs, **output_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from mmaction.core import confusion_matrix \n",
    "\n",
    "gt_labels = [ann['label'] for ann in dataset.load_annotations()]\n",
    "pred = np.argmax(outputs, axis=1)\n",
    "cf_mat = confusion_matrix(pred, gt_labels).astype(float)\n",
    "\n",
    "sns.heatmap(cf_mat, annot=True, xticklabels = ['box', 'clap', 'go', 'jog', 'run', 'walk', 'wave'], yticklabels = ['box', 'clap', 'go', 'jog', 'run', 'walk', 'wave'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TANet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "64CW6d_AaT-Q",
    "outputId": "3b284fd8-4ee7-4a34-90d7-5023cd123a04",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-03-29 00:29:51--  https://download.openmmlab.com/mmaction/recognition/tanet/tanet_r50_dense_1x1x8_100e_kinetics400_rgb/tanet_r50_dense_1x1x8_100e_kinetics400_rgb_20210219-032c8e94.pth\n",
      "Resolving download.openmmlab.com (download.openmmlab.com)... 47.254.186.225\n",
      "Connecting to download.openmmlab.com (download.openmmlab.com)|47.254.186.225|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 102681749 (98M) [application/octet-stream]\n",
      "Saving to: ‘checkpoints/tanet_r50_dense_1x1x8_100e_kinetics400_rgb_20210219-032c8e94.pth’\n",
      "\n",
      "checkpoints/tanet_r 100%[===================>]  97,92M  11,1MB/s    in 9,1s    \n",
      "\n",
      "2021-03-29 00:30:03 (10,8 MB/s) - ‘checkpoints/tanet_r50_dense_1x1x8_100e_kinetics400_rgb_20210219-032c8e94.pth’ saved [102681749/102681749]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !mkdir checkpoints\n",
    "!wget -c https://download.openmmlab.com/mmaction/recognition/tanet/tanet_r50_dense_1x1x8_100e_kinetics400_rgb/tanet_r50_dense_1x1x8_100e_kinetics400_rgb_20210219-032c8e94.pth \\\n",
    "      -O checkpoints/tanet_r50_dense_1x1x8_100e_kinetics400_rgb_20210219-032c8e94.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "LjCcmCKOjktc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mmcv import Config\n",
    "cfg = Config.fromfile('./configs/recognition/tanet/tanet_r50_dense_1x1x8_100e_kinetics400_rgb.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "tlhu9byjjt-K",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "13d1bb01-f351-48c0-9efb-0bdf2c125d29",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "model = dict(\n",
      "    type='Recognizer2D',\n",
      "    backbone=dict(\n",
      "        type='TANet',\n",
      "        pretrained='torchvision://resnet50',\n",
      "        depth=50,\n",
      "        num_segments=8,\n",
      "        tam_cfg=dict()),\n",
      "    cls_head=dict(\n",
      "        type='TSMHead',\n",
      "        num_classes=7,\n",
      "        in_channels=2048,\n",
      "        spatial_type='avg',\n",
      "        consensus=dict(type='AvgConsensus', dim=1),\n",
      "        dropout_ratio=0.5,\n",
      "        init_std=0.001),\n",
      "    train_cfg=None,\n",
      "    test_cfg=dict(average_clips='prob'))\n",
      "checkpoint_config = dict(interval=12)\n",
      "log_config = dict(interval=25, hooks=[dict(type='TextLoggerHook')])\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = 'checkpoints/tanet_r50_dense_1x1x8_100e_kinetics400_rgb_20210219-032c8e94.pth'\n",
      "resume_from = None\n",
      "workflow = [('train', 1)]\n",
      "dataset_type = 'RawframeDataset'\n",
      "data_root = 'data/childact_rawframe/train/'\n",
      "data_root_val = 'data/childact_rawframe/val/'\n",
      "ann_file_train = 'data/childact_rawframe/childact_train_rawframe.txt'\n",
      "ann_file_val = 'data/childact_rawframe/childact_val_rawframe.txt'\n",
      "ann_file_test = 'data/childact_rawframe/childact_test_rawframe.txt'\n",
      "img_norm_cfg = dict(\n",
      "    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], to_bgr=False)\n",
      "train_pipeline = [\n",
      "    dict(type='DenseSampleFrames', clip_len=1, frame_interval=1, num_clips=8),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(\n",
      "        type='MultiScaleCrop',\n",
      "        input_size=224,\n",
      "        scales=(1, 0.875, 0.75, 0.66),\n",
      "        random_crop=False,\n",
      "        max_wh_scale_gap=1,\n",
      "        num_fixed_crops=13),\n",
      "    dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "    dict(type='Flip', flip_ratio=0.5),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[0.485, 0.456, 0.406],\n",
      "        std=[0.229, 0.224, 0.225],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "]\n",
      "val_pipeline = [\n",
      "    dict(\n",
      "        type='DenseSampleFrames',\n",
      "        clip_len=1,\n",
      "        frame_interval=1,\n",
      "        num_clips=8,\n",
      "        test_mode=True),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='CenterCrop', crop_size=224),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[0.485, 0.456, 0.406],\n",
      "        std=[0.229, 0.224, 0.225],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(\n",
      "        type='DenseSampleFrames',\n",
      "        clip_len=1,\n",
      "        frame_interval=1,\n",
      "        num_clips=8,\n",
      "        test_mode=True),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='ThreeCrop', crop_size=256),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[0.485, 0.456, 0.406],\n",
      "        std=[0.229, 0.224, 0.225],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "data = dict(\n",
      "    videos_per_gpu=8,\n",
      "    workers_per_gpu=4,\n",
      "    test_dataloader=dict(videos_per_gpu=2),\n",
      "    train=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file='data/childact_rawframe/childact_train_rawframe.txt',\n",
      "        data_prefix='data/childact_rawframe/train/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='DenseSampleFrames',\n",
      "                clip_len=1,\n",
      "                frame_interval=1,\n",
      "                num_clips=8),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(\n",
      "                type='MultiScaleCrop',\n",
      "                input_size=224,\n",
      "                scales=(1, 0.875, 0.75, 0.66),\n",
      "                random_crop=False,\n",
      "                max_wh_scale_gap=1,\n",
      "                num_fixed_crops=13),\n",
      "            dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "            dict(type='Flip', flip_ratio=0.5),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "        ],\n",
      "        modality='Flow',\n",
      "        start_index=0,\n",
      "        filename_tmpl='flow_{}_{:05d}.jpg'),\n",
      "    val=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file='data/childact_rawframe/childact_val_rawframe.txt',\n",
      "        data_prefix='data/childact_rawframe/val/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='DenseSampleFrames',\n",
      "                clip_len=1,\n",
      "                frame_interval=1,\n",
      "                num_clips=8,\n",
      "                test_mode=True),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='CenterCrop', crop_size=224),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ],\n",
      "        modality='Flow',\n",
      "        start_index=0,\n",
      "        filename_tmpl='flow_{}_{:05d}.jpg'),\n",
      "    test=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file='data/childact_rawframe/childact_test_rawframe.txt',\n",
      "        data_prefix='data/childact_rawframe/test/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='DenseSampleFrames',\n",
      "                clip_len=1,\n",
      "                frame_interval=1,\n",
      "                num_clips=8,\n",
      "                test_mode=True),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='ThreeCrop', crop_size=256),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ],\n",
      "        modality='Flow',\n",
      "        start_index=0,\n",
      "        filename_tmpl='flow_{}_{:05d}.jpg'))\n",
      "evaluation = dict(\n",
      "    interval=2, metrics=['top_k_accuracy', 'mean_class_accuracy'])\n",
      "optimizer = dict(\n",
      "    type='SGD',\n",
      "    constructor='TSMOptimizerConstructor',\n",
      "    paramwise_cfg=dict(fc_lr5=True),\n",
      "    lr=0.01,\n",
      "    momentum=0.9,\n",
      "    weight_decay=0.0001)\n",
      "optimizer_config = dict(grad_clip=dict(max_norm=20, norm_type=2))\n",
      "lr_config = dict(\n",
      "    policy='cyclic',\n",
      "    target_ratio=(10, 1e-05),\n",
      "    cyclic_times=1,\n",
      "    step_ratio_up=0.4)\n",
      "total_epochs = 51\n",
      "work_dir = './childact-checkpoints/childact-TANet'\n",
      "omnisource = False\n",
      "momentum_config = dict(\n",
      "    policy='cyclic',\n",
      "    target_ratio=(0.8947368421052632, 1),\n",
      "    cyclic_times=1,\n",
      "    step_ratio_up=0.4)\n",
      "seed = 42\n",
      "gpu_ids = range(0, 1)\n",
      "output_config = dict(out='./childact-checkpoints/childact-TANet/results.json')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mmcv.runner import set_random_seed\n",
    "\n",
    "# Modify dataset type and path\n",
    "# cfg.dataset_type = 'VideoDataset'\n",
    "cfg.data_root = 'data/childact_rawframe/train/'\n",
    "cfg.data_root_val = 'data/childact_rawframe/val/'\n",
    "cfg.ann_file_train = 'data/childact_rawframe/childact_train_rawframe.txt'\n",
    "cfg.ann_file_val = 'data/childact_rawframe/childact_val_rawframe.txt'\n",
    "cfg.ann_file_test = 'data/childact_rawframe/childact_test_rawframe.txt'\n",
    "\n",
    "# cfg.data.test.type = 'VideoDataset'\n",
    "cfg.data.test.ann_file = 'data/childact_rawframe/childact_test_rawframe.txt'\n",
    "cfg.data.test.data_prefix = 'data/childact_rawframe/test/'\n",
    "\n",
    "# cfg.data.train.type = 'VideoDataset'\n",
    "cfg.data.train.ann_file = 'data/childact_rawframe/childact_train_rawframe.txt'\n",
    "cfg.data.train.data_prefix = 'data/childact_rawframe/train/'\n",
    "\n",
    "# cfg.data.val.type = 'VideoDataset'\n",
    "cfg.data.val.ann_file = 'data/childact_rawframe/childact_val_rawframe.txt'\n",
    "cfg.data.val.data_prefix = 'data/childact_rawframe/val/'\n",
    "\n",
    "cfg.data.test.modality = 'Flow'\n",
    "cfg.data.val.modality = 'Flow'\n",
    "cfg.data.train.modality = 'Flow'\n",
    "\n",
    "cfg.data.train.start_index = 0\n",
    "cfg.data.test.start_index = 0\n",
    "cfg.data.val.start_index = 0\n",
    "\n",
    "cfg.data.test.filename_tmpl = 'flow_{}_{:05d}.jpg'\n",
    "cfg.data.train.filename_tmpl = 'flow_{}_{:05d}.jpg'\n",
    "cfg.data.val.filename_tmpl = 'flow_{}_{:05d}.jpg'\n",
    "# The flag is used to determine whether it is omnisource training\n",
    "cfg.setdefault('omnisource', False)\n",
    "# Modify num classes of the model in cls_head\n",
    "cfg.model.cls_head.num_classes = 7\n",
    "# We can use the pre-trained TSN model\n",
    "cfg.load_from = 'checkpoints/tanet_r50_dense_1x1x8_100e_kinetics400_rgb_20210219-032c8e94.pth'\n",
    "# cfg.resume_from = './childact-mm/latest.pth'\n",
    "\n",
    "# Set up working dir to save files and logs.\n",
    "cfg.work_dir = './childact-checkpoints/childact-TANet'\n",
    "\n",
    "# cfg.train_pipeline[1] = dict(type='RawFrameDecode')\n",
    "# cfg.test_pipeline[1] = dict(type='RawFrameDecode')\n",
    "# cfg.val_pipeline[1] = dict(type='RawFrameDecode')\n",
    "\n",
    "# cfg.train_pipeline[7].input_format = 'NCHW_Flow'\n",
    "# cfg.test_pipeline[6].input_format = 'NCHW_Flow'\n",
    "# cfg.val_pipeline[6].input_format = 'NCHW_Flow'\n",
    "\n",
    "# The original learning rate (LR) is set for 8-GPU training.\n",
    "# We divide it by 8 since we only use one GPU.\n",
    "# cfg.data.videos_per_gpu = 24\n",
    "# cfg.optimizer.type = 'Adam'\n",
    "# cfg.optimizer.weight_decay=0.0001\n",
    "\n",
    "# cfg.optimizer_config.grad_clip=None\n",
    "# cfg.optimizer.lr = 0.01\n",
    "\n",
    "cfg.lr_config = dict(\n",
    "    policy='cyclic',\n",
    "    target_ratio=(10, 1e-5),\n",
    "    cyclic_times=1,\n",
    "    step_ratio_up=0.4,\n",
    ")\n",
    "\n",
    "cfg.total_epochs = 51\n",
    "\n",
    "cfg.momentum_config = dict(\n",
    "    policy='cyclic',\n",
    "    target_ratio=(0.85 / 0.95, 1),\n",
    "    cyclic_times=1,\n",
    "    step_ratio_up=0.4,\n",
    ")\n",
    "\n",
    "# We can set the checkpoint saving interval to reduce the storage cost\n",
    "cfg.checkpoint_config.interval = 12\n",
    "# We can set the log print interval to reduce the the times of printing log\n",
    "cfg.log_config.interval = 25\n",
    "\n",
    "# Set seed thus the results are more reproducible\n",
    "cfg.seed = 42\n",
    "set_random_seed(42, deterministic=False)\n",
    "cfg.gpu_ids = range(1)\n",
    "\n",
    "cfg.img_norm_cfg.mean = [0.485, 0.456, 0.406]\n",
    "cfg.img_norm_cfg.std = [0.229, 0.224, 0.225]\n",
    "\n",
    "cfg.train_pipeline[6] = dict(type='Normalize', mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], to_bgr=False)\n",
    "cfg.test_pipeline[4] = dict(type='Normalize', mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], to_bgr=False)\n",
    "cfg.val_pipeline[4] = dict(type='Normalize', mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], to_bgr=False)\n",
    "\n",
    "cfg.output_config = dict(out=f'{cfg.work_dir}/results.json')\n",
    "# We can initialize the logger for training and have a look\n",
    "# at the final config used for training\n",
    "# del cfg.optimizer['momentum']\n",
    "print(f'Config:\\n{cfg.pretty_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dDBWkdDRk6oz",
    "outputId": "85a52ef3-7b5c-4c52-8fef-00322a8c65e6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-29 01:12:21,182 - mmaction - INFO - These parameters in pretrained checkpoint are not loaded: {'fc.bias', 'fc.weight'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use load_from_torchvision loader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-29 01:12:23,634 - mmaction - INFO - load checkpoint from checkpoints/tanet_r50_dense_1x1x8_100e_kinetics400_rgb_20210219-032c8e94.pth\n",
      "2021-03-29 01:12:23,635 - mmaction - INFO - Use load_from_local loader\n",
      "2021-03-29 01:12:23,774 - mmaction - WARNING - The model and loaded state dict do not match exactly\n",
      "\n",
      "size mismatch for cls_head.fc_cls.weight: copying a param with shape torch.Size([400, 2048]) from checkpoint, the shape in current model is torch.Size([7, 2048]).\n",
      "size mismatch for cls_head.fc_cls.bias: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([7]).\n",
      "2021-03-29 01:12:23,775 - mmaction - INFO - Start running, host: actrec@actrec-HP-Z4-G4-Workstation, work_dir: /home/actrec/.virtualenvs/mmaction/mmaction2/childact-checkpoints/childact-TANet\n",
      "2021-03-29 01:12:23,776 - mmaction - INFO - workflow: [('train', 1)], max: 51 epochs\n",
      "/home/actrec/.virtualenvs/mmaction/mmaction2/mmaction/core/evaluation/eval_hooks.py:131: UserWarning: runner.meta is None. Creating a empty one.\n",
      "  warnings.warn('runner.meta is None. Creating a empty one.')\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Caught AssertionError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/actrec/.virtualenvs/mmaction/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py\", line 198, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/actrec/.virtualenvs/mmaction/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/actrec/.virtualenvs/mmaction/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/actrec/.virtualenvs/mmaction/mmaction2/mmaction/datasets/base.py\", line 280, in __getitem__\n    return self.prepare_train_frames(idx)\n  File \"/home/actrec/.virtualenvs/mmaction/mmaction2/mmaction/datasets/rawframe_dataset.py\", line 170, in prepare_train_frames\n    return self.pipeline(results)\n  File \"/home/actrec/.virtualenvs/mmaction/mmaction2/mmaction/datasets/pipelines/compose.py\", line 41, in __call__\n    data = t(data)\n  File \"/home/actrec/.virtualenvs/mmaction/mmaction2/mmaction/datasets/pipelines/augmentations.py\", line 1243, in __call__\n    assert self.mean.shape[0] == 2\nAssertionError\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-be92e24cff32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Create work_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mmmcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir_or_exist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mosp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwork_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistributed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.virtualenvs/mmaction/mmaction2/mmaction/apis/train.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dataset, cfg, distributed, validate, timestamp, meta)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0momnisource\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0mrunner_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_ratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkflow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mrunner_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.virtualenvs/mmaction/lib/python3.6/site-packages/mmcv/runner/epoch_based_runner.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data_loaders, workflow, max_epochs, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_epochs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m                     \u001b[0mepoch_runner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# wait for some hooks like loggers to finish\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/mmaction/lib/python3.6/site-packages/mmcv/runner/epoch_based_runner.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_loader, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'before_train_epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Prevent possible deadlock during epoch transition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inner_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'before_train_iter'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/mmaction/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/mmaction/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1083\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1085\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/mmaction/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1111\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1112\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/mmaction/lib/python3.6/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    426\u001b[0m             \u001b[0;31m# have message field\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Caught AssertionError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/actrec/.virtualenvs/mmaction/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py\", line 198, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/actrec/.virtualenvs/mmaction/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/actrec/.virtualenvs/mmaction/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/actrec/.virtualenvs/mmaction/mmaction2/mmaction/datasets/base.py\", line 280, in __getitem__\n    return self.prepare_train_frames(idx)\n  File \"/home/actrec/.virtualenvs/mmaction/mmaction2/mmaction/datasets/rawframe_dataset.py\", line 170, in prepare_train_frames\n    return self.pipeline(results)\n  File \"/home/actrec/.virtualenvs/mmaction/mmaction2/mmaction/datasets/pipelines/compose.py\", line 41, in __call__\n    data = t(data)\n  File \"/home/actrec/.virtualenvs/mmaction/mmaction2/mmaction/datasets/pipelines/augmentations.py\", line 1243, in __call__\n    assert self.mean.shape[0] == 2\nAssertionError\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "from mmaction.datasets import build_dataset\n",
    "from mmaction.models import build_model\n",
    "from mmaction.apis import train_model\n",
    "\n",
    "import mmcv\n",
    "\n",
    "# Build the dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# Build the recognizer\n",
    "model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.train_cfg, test_cfg=cfg.test_cfg)\n",
    "\n",
    "# Create work_dir\n",
    "mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
    "train_model(model, datasets, cfg, distributed=False, validate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f\"{cfg.work_dir}/model50e\", 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eyY3hCMwyTct",
    "outputId": "200e37c7-0da4-421f-98da-41418c3110ca",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 126/126, 1.1 task/s, elapsed: 120s, ETA:     0s\n",
      "Evaluating top_k_accuracy ...\n",
      "\n",
      "top1_acc\t0.9365\n",
      "top5_acc\t1.0000\n",
      "\n",
      "Evaluating mean_class_accuracy ...\n",
      "\n",
      "mean_acc\t0.9365\n",
      "top1_acc: 0.9365\n",
      "top5_acc: 1.0000\n",
      "mean_class_accuracy: 0.9365\n"
     ]
    }
   ],
   "source": [
    "from mmaction.apis import single_gpu_test\n",
    "from mmaction.datasets import build_dataloader\n",
    "from mmcv.parallel import MMDataParallel\n",
    "from mmaction.models import build_model\n",
    "from mmaction.datasets import build_dataset\n",
    "\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# Build a test dataloader\n",
    "dataset = build_dataset(cfg.data.test, dict(test_mode=True))\n",
    "data_loader = build_dataloader(\n",
    "        dataset,\n",
    "        videos_per_gpu=4,\n",
    "        workers_per_gpu=1,\n",
    "        dist=False,\n",
    "        shuffle=False)\n",
    "model = MMDataParallel(model, device_ids=[0])\n",
    "outputs = single_gpu_test(model, data_loader)\n",
    "\n",
    "eval_config = cfg.evaluation\n",
    "eval_config.pop('interval')\n",
    "eval_res = dataset.evaluate(outputs, **eval_config)\n",
    "for name, val in eval_res.items():\n",
    "    print(f'{name}: {val:.04f}')\n",
    "    \n",
    "output_config = cfg.output_config\n",
    "dataset.dump_results(outputs, **output_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from mmaction.core import confusion_matrix \n",
    "\n",
    "gt_labels = [ann['label'] for ann in dataset.load_annotations()]\n",
    "pred = np.argmax(outputs, axis=1)\n",
    "cf_mat = confusion_matrix(pred, gt_labels).astype(float)\n",
    "\n",
    "sns.heatmap(cf_mat, annot=True, xticklabels = ['box', 'clap', 'go', 'jog', 'run', 'walk', 'wave'], yticklabels = ['box', 'clap', 'go', 'jog', 'run', 'walk', 'wave'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "MMAction2 Tutorial.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

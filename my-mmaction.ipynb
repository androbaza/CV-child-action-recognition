{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/open-mmlab/mmaction2/blob/master/demo/mmaction2_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VcjSRFELVbNk",
    "tags": []
   },
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bf8PpPXtVvmg",
    "outputId": "2c685a33-474b-4e71-8f98-c2533c66095e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2017 NVIDIA Corporation\n",
      "Built on Fri_Nov__3_21:07:56_CDT_2017\n",
      "Cuda compilation tools, release 9.1, V9.1.85\n",
      "gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\n",
      "Copyright (C) 2017 Free Software Foundation, Inc.\n",
      "This is free software; see the source for copying conditions.  There is NO\n",
      "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check nvcc version\n",
    "!nvcc -V\n",
    "# Check GCC version\n",
    "!gcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5PAJ4ArzV5Ry",
    "outputId": "e48dbf61-fae0-431c-e964-04c7caaee4bc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Install some optional requirements\n",
    "# !pip install -r requirements/optional.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "No_zZAFpWC-a",
    "outputId": "1d425eea-d44e-434a-991c-01eb15abaab2",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.0+cu110 True\n",
      "0.12.0\n",
      "11.0\n",
      "GCC 7.5\n"
     ]
    }
   ],
   "source": [
    "# Check Pytorch installation\n",
    "import torch, torchvision\n",
    "print(torch.__version__, torch.cuda.is_available())\n",
    "\n",
    "# Check MMAction2 installation\n",
    "import mmaction\n",
    "print(mmaction.__version__)\n",
    "\n",
    "# Check MMCV installation\n",
    "from mmcv.ops import get_compiling_cuda_version, get_compiler_version\n",
    "print(get_compiling_cuda_version())\n",
    "print(get_compiler_version())\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/actrec/.local/lib/python3.6/site-packages/decord-0.5.3-py3.6-linux-x86_64.egg')\n",
    "import decord\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/actrec/.virtualenvs/mmaction/mmaction2\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mcheckpoints\u001b[0m/  \u001b[01;34mdemo\u001b[0m/        LICENSE              README.md         setup.cfg\n",
      "\u001b[01;34mchildact-mm\u001b[0m/  \u001b[01;34mdocker\u001b[0m/      \u001b[01;34mmmaction\u001b[0m/            README_zh-CN.md   setup.py\n",
      "\u001b[01;34mconfigs\u001b[0m/      \u001b[01;34mdocs\u001b[0m/        \u001b[01;34mmmaction2.egg-info\u001b[0m/  \u001b[01;34mrequirements\u001b[0m/     \u001b[01;34mtests\u001b[0m/\n",
      "\u001b[01;34mdata\u001b[0m/         \u001b[01;34mdocs_zh_CN\u001b[0m/  my-mmaction.ipynb    requirements.txt  \u001b[01;34mtools\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "# TSN 90.48% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "LjCcmCKOjktc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mmcv import Config\n",
    "cfg = Config.fromfile('./configs/recognition/tsn/tsn_r50_video_1x1x8_100e_kinetics400_rgb.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "64CW6d_AaT-Q",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "3b284fd8-4ee7-4a34-90d7-5023cd123a04",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-03-22 01:46:07--  https://download.openmmlab.com/mmaction/recognition/tsn/tsn_r50_video_1x1x8_100e_kinetics600_rgb/tsn_r50_video_1x1x8_100e_kinetics600_rgb_20201015-4db3c461.pth\n",
      "Resolving download.openmmlab.com (download.openmmlab.com)... 47.75.20.25\n",
      "Connecting to download.openmmlab.com (download.openmmlab.com)|47.75.20.25|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 99220779 (95M) [application/octet-stream]\n",
      "Saving to: ‘checkpoints/tsn_r50_video_1x1x8_100e_kinetics600_rgb_20201015-4db3c461.pth’\n",
      "\n",
      "checkpoints/tsn_r50 100%[===================>]  94,62M  10,5MB/s    in 11s     \n",
      "\n",
      "2021-03-22 01:46:22 (8,61 MB/s) - ‘checkpoints/tsn_r50_video_1x1x8_100e_kinetics600_rgb_20201015-4db3c461.pth’ saved [99220779/99220779]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !mkdir checkpoints\n",
    "# !wget -c https://download.openmmlab.com/   .pth \\\n",
    "#       -O checkpoints/db3c461.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tlhu9byjjt-K",
    "outputId": "13d1bb01-f351-48c0-9efb-0bdf2c125d29",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "model = dict(\n",
      "    type='Recognizer2D',\n",
      "    backbone=dict(\n",
      "        type='ResNet',\n",
      "        pretrained='torchvision://resnet50',\n",
      "        depth=50,\n",
      "        norm_eval=False),\n",
      "    cls_head=dict(\n",
      "        type='TSNHead',\n",
      "        num_classes=7,\n",
      "        in_channels=2048,\n",
      "        spatial_type='avg',\n",
      "        consensus=dict(type='AvgConsensus', dim=1),\n",
      "        dropout_ratio=0.4,\n",
      "        init_std=0.01),\n",
      "    train_cfg=None,\n",
      "    test_cfg=dict(average_clips=None))\n",
      "optimizer = dict(type='SGD', lr=0.0001, momentum=0.9, weight_decay=0.0001)\n",
      "optimizer_config = dict(grad_clip=dict(max_norm=40, norm_type=2))\n",
      "lr_config = dict(policy='step', step=[40, 80])\n",
      "total_epochs = 30\n",
      "checkpoint_config = dict(interval=2)\n",
      "log_config = dict(interval=10, hooks=[dict(type='TextLoggerHook')])\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = None\n",
      "resume_from = './childact-mm/latest.pth'\n",
      "workflow = [('train', 1)]\n",
      "dataset_type = 'VideoDataset'\n",
      "data_root = 'data/childact_split/train/'\n",
      "data_root_val = 'data/childact_split/val/'\n",
      "ann_file_train = 'data/childact_split/childact_train_video.txt'\n",
      "ann_file_val = 'data/childact_split/childact_val_video.txt'\n",
      "ann_file_test = 'data/childact_split/childact_test_video.txt'\n",
      "img_norm_cfg = dict(\n",
      "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_bgr=False)\n",
      "train_pipeline = [\n",
      "    dict(type='DecordInit'),\n",
      "    dict(type='SampleFrames', clip_len=1, frame_interval=1, num_clips=8),\n",
      "    dict(type='DecordDecode'),\n",
      "    dict(\n",
      "        type='MultiScaleCrop',\n",
      "        input_size=224,\n",
      "        scales=(1, 0.875, 0.75, 0.66),\n",
      "        random_crop=False,\n",
      "        max_wh_scale_gap=1),\n",
      "    dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "    dict(type='Flip', flip_ratio=0.5),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "]\n",
      "val_pipeline = [\n",
      "    dict(type='DecordInit'),\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=1,\n",
      "        frame_interval=1,\n",
      "        num_clips=8,\n",
      "        test_mode=True),\n",
      "    dict(type='DecordDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='CenterCrop', crop_size=224),\n",
      "    dict(type='Flip', flip_ratio=0),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(type='DecordInit'),\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=1,\n",
      "        frame_interval=1,\n",
      "        num_clips=25,\n",
      "        test_mode=True),\n",
      "    dict(type='DecordDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='ThreeCrop', crop_size=256),\n",
      "    dict(type='Flip', flip_ratio=0),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "data = dict(\n",
      "    videos_per_gpu=32,\n",
      "    workers_per_gpu=4,\n",
      "    train=dict(\n",
      "        type='VideoDataset',\n",
      "        ann_file='data/childact_split/childact_train_video.txt',\n",
      "        data_prefix='data/childact_split/train/',\n",
      "        pipeline=[\n",
      "            dict(type='DecordInit'),\n",
      "            dict(\n",
      "                type='SampleFrames', clip_len=1, frame_interval=1,\n",
      "                num_clips=8),\n",
      "            dict(type='DecordDecode'),\n",
      "            dict(\n",
      "                type='MultiScaleCrop',\n",
      "                input_size=224,\n",
      "                scales=(1, 0.875, 0.75, 0.66),\n",
      "                random_crop=False,\n",
      "                max_wh_scale_gap=1),\n",
      "            dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "            dict(type='Flip', flip_ratio=0.5),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "        ]),\n",
      "    val=dict(\n",
      "        type='VideoDataset',\n",
      "        ann_file='data/childact_split/childact_val_video.txt',\n",
      "        data_prefix='data/childact_split/val/',\n",
      "        pipeline=[\n",
      "            dict(type='DecordInit'),\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=1,\n",
      "                frame_interval=1,\n",
      "                num_clips=8,\n",
      "                test_mode=True),\n",
      "            dict(type='DecordDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='CenterCrop', crop_size=224),\n",
      "            dict(type='Flip', flip_ratio=0),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ]),\n",
      "    test=dict(\n",
      "        type='VideoDataset',\n",
      "        ann_file='data/childact_split/childact_test_video.txt',\n",
      "        data_prefix='data/childact_split/test/',\n",
      "        pipeline=[\n",
      "            dict(type='DecordInit'),\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=1,\n",
      "                frame_interval=1,\n",
      "                num_clips=25,\n",
      "                test_mode=True),\n",
      "            dict(type='DecordDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='ThreeCrop', crop_size=256),\n",
      "            dict(type='Flip', flip_ratio=0),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ]))\n",
      "evaluation = dict(\n",
      "    interval=5, metrics=['top_k_accuracy', 'mean_class_accuracy'])\n",
      "work_dir = './childact-mm/'\n",
      "omnisource = False\n",
      "seed = 42\n",
      "gpu_ids = range(0, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mmcv.runner import set_random_seed\n",
    "\n",
    "# Modify dataset type and path\n",
    "cfg.dataset_type = 'VideoDataset'\n",
    "cfg.data_root = 'data/childact_split/train/'\n",
    "cfg.data_root_val = 'data/childact_split/val/'\n",
    "cfg.ann_file_train = 'data/childact_split/childact_train_video.txt'\n",
    "cfg.ann_file_val = 'data/childact_split/childact_val_video.txt'\n",
    "cfg.ann_file_test = 'data/childact_split/childact_test_video.txt'\n",
    "\n",
    "cfg.data.test.type = 'VideoDataset'\n",
    "cfg.data.test.ann_file = 'data/childact_split/childact_test_video.txt'\n",
    "cfg.data.test.data_prefix = 'data/childact_split/test/'\n",
    "\n",
    "cfg.data.train.type = 'VideoDataset'\n",
    "cfg.data.train.ann_file = 'data/childact_split/childact_train_video.txt'\n",
    "cfg.data.train.data_prefix = 'data/childact_split/train/'\n",
    "\n",
    "cfg.data.val.type = 'VideoDataset'\n",
    "cfg.data.val.ann_file = 'data/childact_split/childact_val_video.txt'\n",
    "cfg.data.val.data_prefix = 'data/childact_split/val/'\n",
    "\n",
    "# The flag is used to determine whether it is omnisource training\n",
    "cfg.setdefault('omnisource', False)\n",
    "# Modify num classes of the model in cls_head\n",
    "cfg.model.cls_head.num_classes = 7\n",
    "# We can use the pre-trained TSN model\n",
    "# cfg.load_from = './checkpoints/best_top1_acc_epoch_5.pth'\n",
    "cfg.resume_from = './childact-mm/latest.pth'\n",
    "\n",
    "# Set up working dir to save files and logs.\n",
    "cfg.work_dir = './childact-mm/'\n",
    "\n",
    "# The original learning rate (LR) is set for 8-GPU training.\n",
    "# We divide it by 8 since we only use one GPU.\n",
    "cfg.data.videos_per_gpu = 32\n",
    "cfg.optimizer.lr = 0.0001\n",
    "# cfg.lr_config.type = 'cyclic'\n",
    "cfg.total_epochs = 30\n",
    "\n",
    "# We can set the checkpoint saving interval to reduce the storage cost\n",
    "cfg.checkpoint_config.interval = 2\n",
    "# We can set the log print interval to reduce the the times of printing log\n",
    "cfg.log_config.interval = 10\n",
    "\n",
    "# Set seed thus the results are more reproducible\n",
    "cfg.seed = 42\n",
    "set_random_seed(42, deterministic=False)\n",
    "cfg.gpu_ids = range(1)\n",
    "\n",
    "# We can initialize the logger for training and have a look\n",
    "# at the final config used for training\n",
    "print(f'Config:\\n{cfg.pretty_text}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tES-qnZ3k38Z"
   },
   "source": [
    "### Train a new recognizer\n",
    "\n",
    "Finally, lets initialize the dataset and recognizer, then train a new recognizer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "dDBWkdDRk6oz",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "85a52ef3-7b5c-4c52-8fef-00322a8c65e6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-22 16:04:14,045 - mmaction - INFO - These parameters in pretrained checkpoint are not loaded: {'fc.bias', 'fc.weight'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use load_from_torchvision loader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-22 16:04:16,416 - mmaction - INFO - load checkpoint from ./childact-mm/latest.pth\n",
      "2021-03-22 16:04:16,417 - mmaction - INFO - Use load_from_local loader\n",
      "2021-03-22 16:04:16,539 - mmaction - INFO - resumed epoch 12, iter 561\n",
      "2021-03-22 16:04:16,541 - mmaction - INFO - Start running, host: actrec@actrec-HP-Z4-G4-Workstation, work_dir: /home/actrec/.virtualenvs/mmaction/mmaction2/childact-mm\n",
      "2021-03-22 16:04:16,542 - mmaction - INFO - workflow: [('train', 1)], max: 30 epochs\n",
      "/home/actrec/.virtualenvs/mmaction/mmaction2/mmaction/core/evaluation/eval_hooks.py:131: UserWarning: runner.meta is None. Creating a empty one.\n",
      "  warnings.warn('runner.meta is None. Creating a empty one.')\n",
      "2021-03-22 16:05:56,931 - mmaction - INFO - Epoch [13][10/33]\tlr: 5.000e-04, eta: 1:10:06, time: 10.039, data_time: 9.158, memory: 21448, top1_acc: 0.9375, top5_acc: 1.0000, loss_cls: 0.2199, loss: 0.2199, grad_norm: 3.0833\n",
      "2021-03-22 16:07:05,519 - mmaction - INFO - Epoch [13][20/33]\tlr: 5.000e-04, eta: 0:57:35, time: 6.859, data_time: 5.968, memory: 21448, top1_acc: 0.8750, top5_acc: 1.0000, loss_cls: 0.2856, loss: 0.2856, grad_norm: 3.9159\n",
      "2021-03-22 16:08:40,494 - mmaction - INFO - Epoch [13][30/33]\tlr: 5.000e-04, eta: 0:58:30, time: 9.498, data_time: 8.622, memory: 21448, top1_acc: 0.8875, top5_acc: 1.0000, loss_cls: 0.2845, loss: 0.2845, grad_norm: 3.6887\n",
      "2021-03-22 16:10:45,688 - mmaction - INFO - Epoch [14][10/33]\tlr: 5.000e-04, eta: 0:55:00, time: 10.373, data_time: 9.495, memory: 21448, top1_acc: 0.9375, top5_acc: 1.0000, loss_cls: 0.2317, loss: 0.2317, grad_norm: 3.2582\n",
      "2021-03-22 16:11:54,243 - mmaction - INFO - Epoch [14][20/33]\tlr: 5.000e-04, eta: 0:51:34, time: 6.856, data_time: 5.979, memory: 21448, top1_acc: 0.8969, top5_acc: 1.0000, loss_cls: 0.2885, loss: 0.2885, grad_norm: 3.9673\n",
      "2021-03-22 16:13:30,616 - mmaction - INFO - Epoch [14][30/33]\tlr: 5.000e-04, eta: 0:51:34, time: 9.637, data_time: 8.756, memory: 21448, top1_acc: 0.9219, top5_acc: 1.0000, loss_cls: 0.2582, loss: 0.2582, grad_norm: 3.7690\n",
      "2021-03-22 16:13:46,849 - mmaction - INFO - Saving checkpoint at 14 epochs\n",
      "2021-03-22 16:15:31,371 - mmaction - INFO - Epoch [15][10/33]\tlr: 5.000e-04, eta: 0:49:17, time: 10.422, data_time: 9.546, memory: 21448, top1_acc: 0.9156, top5_acc: 1.0000, loss_cls: 0.2607, loss: 0.2607, grad_norm: 3.9742\n",
      "2021-03-22 16:16:36,546 - mmaction - INFO - Epoch [15][20/33]\tlr: 5.000e-04, eta: 0:46:39, time: 6.518, data_time: 5.645, memory: 21448, top1_acc: 0.8906, top5_acc: 1.0000, loss_cls: 0.2774, loss: 0.2774, grad_norm: 4.2109\n",
      "2021-03-22 16:18:12,739 - mmaction - INFO - Epoch [15][30/33]\tlr: 5.000e-04, eta: 0:46:08, time: 9.619, data_time: 8.747, memory: 21448, top1_acc: 0.9000, top5_acc: 1.0000, loss_cls: 0.2686, loss: 0.2686, grad_norm: 4.0837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 126/126, 3.5 task/s, elapsed: 35s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-22 16:19:08,703 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-03-22 16:19:08,705 - mmaction - INFO - \n",
      "top1_acc\t0.8492\n",
      "top5_acc\t1.0000\n",
      "2021-03-22 16:19:08,706 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-03-22 16:19:08,707 - mmaction - INFO - \n",
      "mean_acc\t0.8492\n",
      "2021-03-22 16:19:09,000 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_15.pth.\n",
      "2021-03-22 16:19:09,001 - mmaction - INFO - Best top1_acc is 0.8492 at 15 epoch.\n",
      "2021-03-22 16:19:09,002 - mmaction - INFO - Epoch(val) [15][33]\ttop1_acc: 0.8492, top5_acc: 1.0000, mean_class_accuracy: 0.8492\n",
      "2021-03-22 16:20:54,290 - mmaction - INFO - Epoch [16][10/33]\tlr: 5.000e-04, eta: 0:44:12, time: 10.529, data_time: 9.656, memory: 21448, top1_acc: 0.8656, top5_acc: 1.0000, loss_cls: 0.3135, loss: 0.3135, grad_norm: 5.0392\n",
      "2021-03-22 16:22:01,583 - mmaction - INFO - Epoch [16][20/33]\tlr: 5.000e-04, eta: 0:42:08, time: 6.729, data_time: 5.830, memory: 21448, top1_acc: 0.9094, top5_acc: 0.9969, loss_cls: 0.2887, loss: 0.2887, grad_norm: 4.6742\n",
      "2021-03-22 16:23:36,416 - mmaction - INFO - Epoch [16][30/33]\tlr: 5.000e-04, eta: 0:41:18, time: 9.483, data_time: 8.587, memory: 21448, top1_acc: 0.8875, top5_acc: 1.0000, loss_cls: 0.2946, loss: 0.2946, grad_norm: 4.6754\n",
      "2021-03-22 16:23:49,993 - mmaction - INFO - Saving checkpoint at 16 epochs\n",
      "2021-03-22 16:25:33,204 - mmaction - INFO - Epoch [17][10/33]\tlr: 5.000e-04, eta: 0:39:21, time: 10.292, data_time: 9.408, memory: 21448, top1_acc: 0.8656, top5_acc: 1.0000, loss_cls: 0.3344, loss: 0.3344, grad_norm: 4.8666\n",
      "2021-03-22 16:26:46,080 - mmaction - INFO - Epoch [17][20/33]\tlr: 5.000e-04, eta: 0:37:42, time: 7.288, data_time: 6.408, memory: 21448, top1_acc: 0.9344, top5_acc: 1.0000, loss_cls: 0.2459, loss: 0.2459, grad_norm: 4.0389\n",
      "2021-03-22 16:28:21,720 - mmaction - INFO - Epoch [17][30/33]\tlr: 5.000e-04, eta: 0:36:43, time: 9.564, data_time: 8.689, memory: 21448, top1_acc: 0.8844, top5_acc: 1.0000, loss_cls: 0.3178, loss: 0.3178, grad_norm: 4.5435\n",
      "2021-03-22 16:30:33,123 - mmaction - INFO - Epoch [18][10/33]\tlr: 5.000e-04, eta: 0:34:58, time: 10.863, data_time: 9.958, memory: 21448, top1_acc: 0.9219, top5_acc: 1.0000, loss_cls: 0.2423, loss: 0.2423, grad_norm: 3.7765\n",
      "2021-03-22 16:31:40,469 - mmaction - INFO - Epoch [18][20/33]\tlr: 5.000e-04, eta: 0:33:15, time: 6.735, data_time: 5.828, memory: 21448, top1_acc: 0.9187, top5_acc: 1.0000, loss_cls: 0.2488, loss: 0.2488, grad_norm: 3.7101\n",
      "2021-03-22 16:33:15,571 - mmaction - INFO - Epoch [18][30/33]\tlr: 5.000e-04, eta: 0:32:09, time: 9.510, data_time: 8.597, memory: 21448, top1_acc: 0.9281, top5_acc: 1.0000, loss_cls: 0.2478, loss: 0.2478, grad_norm: 3.9696\n",
      "2021-03-22 16:33:38,128 - mmaction - INFO - Saving checkpoint at 18 epochs\n",
      "2021-03-22 16:35:26,146 - mmaction - INFO - Epoch [19][10/33]\tlr: 5.000e-04, eta: 0:30:23, time: 10.771, data_time: 9.890, memory: 21448, top1_acc: 0.9281, top5_acc: 1.0000, loss_cls: 0.2272, loss: 0.2272, grad_norm: 4.1818\n",
      "2021-03-22 16:36:30,975 - mmaction - INFO - Epoch [19][20/33]\tlr: 5.000e-04, eta: 0:28:43, time: 6.483, data_time: 5.602, memory: 21448, top1_acc: 0.9094, top5_acc: 1.0000, loss_cls: 0.2351, loss: 0.2351, grad_norm: 4.1198\n",
      "2021-03-22 16:38:01,164 - mmaction - INFO - Epoch [19][30/33]\tlr: 5.000e-04, eta: 0:27:29, time: 9.019, data_time: 8.134, memory: 21448, top1_acc: 0.9281, top5_acc: 1.0000, loss_cls: 0.2176, loss: 0.2176, grad_norm: 3.7628\n",
      "2021-03-22 16:40:05,490 - mmaction - INFO - Epoch [20][10/33]\tlr: 5.000e-04, eta: 0:25:38, time: 10.189, data_time: 9.315, memory: 21448, top1_acc: 0.8906, top5_acc: 1.0000, loss_cls: 0.3233, loss: 0.3233, grad_norm: 4.8384\n",
      "2021-03-22 16:41:11,675 - mmaction - INFO - Epoch [20][20/33]\tlr: 5.000e-04, eta: 0:24:05, time: 6.619, data_time: 5.745, memory: 21448, top1_acc: 0.9062, top5_acc: 1.0000, loss_cls: 0.2407, loss: 0.2407, grad_norm: 4.3140\n",
      "2021-03-22 16:42:46,974 - mmaction - INFO - Epoch [20][30/33]\tlr: 5.000e-04, eta: 0:22:53, time: 9.530, data_time: 8.654, memory: 21448, top1_acc: 0.9031, top5_acc: 1.0000, loss_cls: 0.2469, loss: 0.2469, grad_norm: 3.9892\n",
      "2021-03-22 16:43:06,859 - mmaction - INFO - Saving checkpoint at 20 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 126/126, 3.5 task/s, elapsed: 36s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-22 16:43:42,967 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-03-22 16:43:42,969 - mmaction - INFO - \n",
      "top1_acc\t0.8571\n",
      "top5_acc\t1.0000\n",
      "2021-03-22 16:43:42,970 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-03-22 16:43:42,971 - mmaction - INFO - \n",
      "mean_acc\t0.8571\n",
      "2021-03-22 16:43:43,294 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_20.pth.\n",
      "2021-03-22 16:43:43,295 - mmaction - INFO - Best top1_acc is 0.8571 at 20 epoch.\n",
      "2021-03-22 16:43:43,296 - mmaction - INFO - Epoch(val) [20][33]\ttop1_acc: 0.8571, top5_acc: 1.0000, mean_class_accuracy: 0.8571\n",
      "2021-03-22 16:45:20,813 - mmaction - INFO - Epoch [21][10/33]\tlr: 5.000e-04, eta: 0:21:02, time: 9.751, data_time: 8.872, memory: 21448, top1_acc: 0.8875, top5_acc: 1.0000, loss_cls: 0.2959, loss: 0.2959, grad_norm: 5.2473\n",
      "2021-03-22 16:46:30,437 - mmaction - INFO - Epoch [21][20/33]\tlr: 5.000e-04, eta: 0:19:34, time: 6.962, data_time: 6.085, memory: 21448, top1_acc: 0.8875, top5_acc: 1.0000, loss_cls: 0.2686, loss: 0.2686, grad_norm: 4.9177\n",
      "2021-03-22 16:48:02,045 - mmaction - INFO - Epoch [21][30/33]\tlr: 5.000e-04, eta: 0:18:18, time: 9.161, data_time: 8.283, memory: 21448, top1_acc: 0.9031, top5_acc: 1.0000, loss_cls: 0.3019, loss: 0.3019, grad_norm: 4.7605\n",
      "2021-03-22 16:50:06,699 - mmaction - INFO - Epoch [22][10/33]\tlr: 5.000e-04, eta: 0:16:30, time: 10.034, data_time: 9.160, memory: 21448, top1_acc: 0.8688, top5_acc: 1.0000, loss_cls: 0.3052, loss: 0.3052, grad_norm: 5.0368\n",
      "2021-03-22 16:51:13,408 - mmaction - INFO - Epoch [22][20/33]\tlr: 5.000e-04, eta: 0:15:04, time: 6.671, data_time: 5.797, memory: 21448, top1_acc: 0.9094, top5_acc: 1.0000, loss_cls: 0.2709, loss: 0.2709, grad_norm: 5.0456\n",
      "2021-03-22 16:52:45,713 - mmaction - INFO - Epoch [22][30/33]\tlr: 5.000e-04, eta: 0:13:47, time: 9.230, data_time: 8.352, memory: 21448, top1_acc: 0.8875, top5_acc: 1.0000, loss_cls: 0.3308, loss: 0.3308, grad_norm: 5.8989\n",
      "2021-03-22 16:53:09,991 - mmaction - INFO - Saving checkpoint at 22 epochs\n",
      "2021-03-22 16:54:55,630 - mmaction - INFO - Epoch [23][10/33]\tlr: 5.000e-04, eta: 0:12:01, time: 10.536, data_time: 9.659, memory: 21448, top1_acc: 0.8781, top5_acc: 1.0000, loss_cls: 0.2764, loss: 0.2764, grad_norm: 4.5259\n",
      "2021-03-22 16:56:03,253 - mmaction - INFO - Epoch [23][20/33]\tlr: 5.000e-04, eta: 0:10:37, time: 6.762, data_time: 5.851, memory: 21448, top1_acc: 0.9187, top5_acc: 1.0000, loss_cls: 0.2443, loss: 0.2443, grad_norm: 4.7793\n",
      "2021-03-22 16:57:41,139 - mmaction - INFO - Epoch [23][30/33]\tlr: 5.000e-04, eta: 0:09:20, time: 9.789, data_time: 8.896, memory: 21448, top1_acc: 0.8875, top5_acc: 1.0000, loss_cls: 0.2643, loss: 0.2643, grad_norm: 4.7593\n",
      "2021-03-22 16:59:43,807 - mmaction - INFO - Epoch [24][10/33]\tlr: 5.000e-04, eta: 0:07:33, time: 10.066, data_time: 9.187, memory: 21448, top1_acc: 0.8844, top5_acc: 0.9969, loss_cls: 0.2928, loss: 0.2928, grad_norm: 4.9920\n",
      "2021-03-22 17:00:52,804 - mmaction - INFO - Epoch [24][20/33]\tlr: 5.000e-04, eta: 0:06:11, time: 6.900, data_time: 6.024, memory: 21448, top1_acc: 0.9094, top5_acc: 0.9969, loss_cls: 0.2648, loss: 0.2648, grad_norm: 4.4870\n",
      "2021-03-22 17:02:26,723 - mmaction - INFO - Epoch [24][30/33]\tlr: 5.000e-04, eta: 0:04:51, time: 9.392, data_time: 8.515, memory: 21448, top1_acc: 0.8938, top5_acc: 0.9969, loss_cls: 0.2742, loss: 0.2742, grad_norm: 4.9958\n",
      "2021-03-22 17:02:48,610 - mmaction - INFO - Saving checkpoint at 24 epochs\n",
      "2021-03-22 17:04:31,720 - mmaction - INFO - Epoch [25][10/33]\tlr: 5.000e-04, eta: 0:03:06, time: 10.283, data_time: 9.394, memory: 21448, top1_acc: 0.9125, top5_acc: 1.0000, loss_cls: 0.2537, loss: 0.2537, grad_norm: 4.5034\n",
      "2021-03-22 17:05:39,963 - mmaction - INFO - Epoch [25][20/33]\tlr: 5.000e-04, eta: 0:01:44, time: 6.824, data_time: 5.936, memory: 21448, top1_acc: 0.8750, top5_acc: 1.0000, loss_cls: 0.2804, loss: 0.2804, grad_norm: 4.7272\n",
      "2021-03-22 17:07:15,466 - mmaction - INFO - Epoch [25][30/33]\tlr: 5.000e-04, eta: 0:00:24, time: 9.550, data_time: 8.677, memory: 21448, top1_acc: 0.9000, top5_acc: 1.0000, loss_cls: 0.2522, loss: 0.2522, grad_norm: 4.5856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 126/126, 3.5 task/s, elapsed: 36s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-22 17:08:12,494 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-03-22 17:08:12,496 - mmaction - INFO - \n",
      "top1_acc\t0.8571\n",
      "top5_acc\t1.0000\n",
      "2021-03-22 17:08:12,496 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-03-22 17:08:12,497 - mmaction - INFO - \n",
      "mean_acc\t0.8571\n",
      "2021-03-22 17:08:12,497 - mmaction - INFO - Epoch(val) [25][33]\ttop1_acc: 0.8571, top5_acc: 1.0000, mean_class_accuracy: 0.8571\n",
      "2021-03-22 17:09:53,555 - mmaction - INFO - Epoch [26][10/33]\tlr: 5.000e-04, eta: -1 day, 23:58:40, time: 10.105, data_time: 9.229, memory: 21448, top1_acc: 0.8969, top5_acc: 1.0000, loss_cls: 0.2787, loss: 0.2787, grad_norm: 4.8332\n",
      "2021-03-22 17:11:02,020 - mmaction - INFO - Epoch [26][20/33]\tlr: 5.000e-04, eta: -1 day, 23:57:19, time: 6.847, data_time: 5.975, memory: 21448, top1_acc: 0.9000, top5_acc: 1.0000, loss_cls: 0.2513, loss: 0.2513, grad_norm: 5.1216\n",
      "2021-03-22 17:12:38,741 - mmaction - INFO - Epoch [26][30/33]\tlr: 5.000e-04, eta: -1 day, 23:55:57, time: 9.672, data_time: 8.776, memory: 21448, top1_acc: 0.8812, top5_acc: 1.0000, loss_cls: 0.2711, loss: 0.2711, grad_norm: 5.1664\n",
      "2021-03-22 17:12:58,372 - mmaction - INFO - Saving checkpoint at 26 epochs\n",
      "2021-03-22 17:14:39,326 - mmaction - INFO - Epoch [27][10/33]\tlr: 5.000e-04, eta: -1 day, 23:54:12, time: 10.068, data_time: 9.185, memory: 21448, top1_acc: 0.9125, top5_acc: 1.0000, loss_cls: 0.2284, loss: 0.2284, grad_norm: 4.4295\n",
      "2021-03-22 17:15:48,990 - mmaction - INFO - Epoch [27][20/33]\tlr: 5.000e-04, eta: -1 day, 23:52:53, time: 6.966, data_time: 6.081, memory: 21448, top1_acc: 0.8906, top5_acc: 1.0000, loss_cls: 0.2763, loss: 0.2763, grad_norm: 4.5929\n",
      "2021-03-22 17:17:19,566 - mmaction - INFO - Epoch [27][30/33]\tlr: 5.000e-04, eta: -1 day, 23:51:31, time: 9.058, data_time: 8.169, memory: 21448, top1_acc: 0.9062, top5_acc: 1.0000, loss_cls: 0.2449, loss: 0.2449, grad_norm: 5.1436\n",
      "2021-03-22 17:19:25,203 - mmaction - INFO - Epoch [28][10/33]\tlr: 5.000e-04, eta: -1 day, 23:49:46, time: 10.396, data_time: 9.516, memory: 21448, top1_acc: 0.8844, top5_acc: 1.0000, loss_cls: 0.2822, loss: 0.2822, grad_norm: 5.1844\n",
      "2021-03-22 17:20:36,427 - mmaction - INFO - Epoch [28][20/33]\tlr: 5.000e-04, eta: -1 day, 23:48:27, time: 7.122, data_time: 6.221, memory: 21448, top1_acc: 0.8875, top5_acc: 1.0000, loss_cls: 0.2617, loss: 0.2617, grad_norm: 4.6933\n",
      "2021-03-22 17:22:09,101 - mmaction - INFO - Epoch [28][30/33]\tlr: 5.000e-04, eta: -1 day, 23:47:04, time: 9.267, data_time: 8.391, memory: 21448, top1_acc: 0.8781, top5_acc: 1.0000, loss_cls: 0.2761, loss: 0.2761, grad_norm: 5.3211\n",
      "2021-03-22 17:22:29,398 - mmaction - INFO - Saving checkpoint at 28 epochs\n",
      "2021-03-22 17:24:12,905 - mmaction - INFO - Epoch [29][10/33]\tlr: 5.000e-04, eta: -1 day, 23:45:19, time: 10.320, data_time: 9.411, memory: 21448, top1_acc: 0.8906, top5_acc: 1.0000, loss_cls: 0.2990, loss: 0.2990, grad_norm: 5.5218\n",
      "2021-03-22 17:25:21,920 - mmaction - INFO - Epoch [29][20/33]\tlr: 5.000e-04, eta: -1 day, 23:44:01, time: 6.902, data_time: 6.019, memory: 21448, top1_acc: 0.8719, top5_acc: 1.0000, loss_cls: 0.2904, loss: 0.2904, grad_norm: 5.0257\n",
      "2021-03-22 17:26:59,871 - mmaction - INFO - Epoch [29][30/33]\tlr: 5.000e-04, eta: -1 day, 23:42:36, time: 9.795, data_time: 8.907, memory: 21448, top1_acc: 0.9031, top5_acc: 1.0000, loss_cls: 0.2559, loss: 0.2559, grad_norm: 5.4150\n",
      "2021-03-22 17:28:54,465 - mmaction - INFO - Epoch [30][10/33]\tlr: 5.000e-04, eta: -1 day, 23:40:52, time: 10.129, data_time: 9.256, memory: 21448, top1_acc: 0.9000, top5_acc: 1.0000, loss_cls: 0.2891, loss: 0.2891, grad_norm: 6.1167\n",
      "2021-03-22 17:30:05,159 - mmaction - INFO - Epoch [30][20/33]\tlr: 5.000e-04, eta: -1 day, 23:39:33, time: 7.069, data_time: 6.184, memory: 21448, top1_acc: 0.8750, top5_acc: 1.0000, loss_cls: 0.2935, loss: 0.2935, grad_norm: 5.4480\n",
      "2021-03-22 17:31:41,483 - mmaction - INFO - Epoch [30][30/33]\tlr: 5.000e-04, eta: -1 day, 23:38:08, time: 9.632, data_time: 8.729, memory: 21448, top1_acc: 0.9375, top5_acc: 1.0000, loss_cls: 0.2138, loss: 0.2138, grad_norm: 4.4516\n",
      "2021-03-22 17:32:03,974 - mmaction - INFO - Saving checkpoint at 30 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 126/126, 3.5 task/s, elapsed: 36s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-22 17:32:40,845 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-03-22 17:32:40,847 - mmaction - INFO - \n",
      "top1_acc\t0.8413\n",
      "top5_acc\t1.0000\n",
      "2021-03-22 17:32:40,848 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-03-22 17:32:40,849 - mmaction - INFO - \n",
      "mean_acc\t0.8413\n",
      "2021-03-22 17:32:40,850 - mmaction - INFO - Epoch(val) [30][33]\ttop1_acc: 0.8413, top5_acc: 1.0000, mean_class_accuracy: 0.8413\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "from mmaction.datasets import build_dataset\n",
    "from mmaction.models import build_model\n",
    "from mmaction.apis import train_model\n",
    "\n",
    "import mmcv\n",
    "\n",
    "# Build the dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# Build the recognizer\n",
    "model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.train_cfg, test_cfg=cfg.test_cfg)\n",
    "\n",
    "# Create work_dir\n",
    "mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
    "train_model(model, datasets, cfg, distributed=False, validate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ryVoSfZVmogw",
    "tags": []
   },
   "source": [
    "## Test the trained recognizer\n",
    "\n",
    "After finetuning the recognizer, let's check the prediction results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eyY3hCMwyTct",
    "outputId": "200e37c7-0da4-421f-98da-41418c3110ca",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 126/126, 2.1 task/s, elapsed: 59s, ETA:     0s\n",
      "Evaluating top_k_accuracy ...\n",
      "\n",
      "top1_acc\t0.9048\n",
      "top5_acc\t1.0000\n",
      "\n",
      "Evaluating mean_class_accuracy ...\n",
      "\n",
      "mean_acc\t0.9048\n",
      "top1_acc: 0.9048\n",
      "top5_acc: 1.0000\n",
      "mean_class_accuracy: 0.9048\n"
     ]
    }
   ],
   "source": [
    "from mmaction.apis import single_gpu_test\n",
    "from mmaction.datasets import build_dataloader\n",
    "from mmcv.parallel import MMDataParallel\n",
    "\n",
    "# Build a test dataloader\n",
    "dataset = build_dataset(cfg.data.test, dict(test_mode=True))\n",
    "data_loader = build_dataloader(\n",
    "        dataset,\n",
    "        videos_per_gpu=16,\n",
    "        workers_per_gpu=cfg.data.workers_per_gpu,\n",
    "        dist=False,\n",
    "        shuffle=False)\n",
    "model = MMDataParallel(model, device_ids=[0])\n",
    "outputs = single_gpu_test(model, data_loader)\n",
    "\n",
    "eval_config = cfg.evaluation\n",
    "eval_config.pop('interval')\n",
    "eval_res = dataset.evaluate(outputs, **eval_config)\n",
    "for name, val in eval_res.items():\n",
    "    print(f'{name}: {val:.04f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "# SlowFast 91.27%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "64CW6d_AaT-Q",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "3b284fd8-4ee7-4a34-90d7-5023cd123a04",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-03-22 18:16:35--  https://download.openmmlab.com/mmaction/recognition/slowfast/slowfast_r50_video_4x16x1_256e_kinetics400_rgb/slowfast_r50_video_4x16x1_256e_kinetics400_rgb_20200826-f85b90c5.pth\n",
      "Resolving download.openmmlab.com (download.openmmlab.com)... 47.254.186.225\n",
      "Connecting to download.openmmlab.com (download.openmmlab.com)|47.254.186.225|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 138274276 (132M) [application/octet-stream]\n",
      "Saving to: ‘checkpoints/slowfast_r50_video_4x16x1_256e_kinetics400_rgb_20200826-f85b90c5.pth’\n",
      "\n",
      "checkpoints/slowfas 100%[===================>] 131,87M  6,72MB/s    in 17s     \n",
      "\n",
      "2021-03-22 18:16:55 (7,79 MB/s) - ‘checkpoints/slowfast_r50_video_4x16x1_256e_kinetics400_rgb_20200826-f85b90c5.pth’ saved [138274276/138274276]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !mkdir checkpoints\n",
    "!wget -c https://download.openmmlab.com/mmaction/recognition/slowfast/slowfast_r50_video_4x16x1_256e_kinetics400_rgb/slowfast_r50_video_4x16x1_256e_kinetics400_rgb_20200826-f85b90c5.pth \\\n",
    "      -O checkpoints/slowfast_r50_video_4x16x1_256e_kinetics400_rgb_20200826-f85b90c5.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "LjCcmCKOjktc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mmcv import Config\n",
    "cfg = Config.fromfile('./configs/recognition/slowfast/slowfast_r50_video_4x16x1_256e_kinetics400_rgb.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tlhu9byjjt-K",
    "outputId": "13d1bb01-f351-48c0-9efb-0bdf2c125d29",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mmcv.runner import set_random_seed\n",
    "\n",
    "# Modify dataset type and path\n",
    "cfg.dataset_type = 'VideoDataset'\n",
    "cfg.data_root = 'data/childact_split/train/'\n",
    "cfg.data_root_val = 'data/childact_split/val/'\n",
    "cfg.ann_file_train = 'data/childact_split/childact_train_video.txt'\n",
    "cfg.ann_file_val = 'data/childact_split/childact_val_video.txt'\n",
    "cfg.ann_file_test = 'data/childact_split/childact_test_video.txt'\n",
    "\n",
    "cfg.data.test.type = 'VideoDataset'\n",
    "cfg.data.test.ann_file = 'data/childact_split/childact_test_video.txt'\n",
    "cfg.data.test.data_prefix = 'data/childact_split/test/'\n",
    "\n",
    "cfg.data.train.type = 'VideoDataset'\n",
    "cfg.data.train.ann_file = 'data/childact_split/childact_train_video.txt'\n",
    "cfg.data.train.data_prefix = 'data/childact_split/train/'\n",
    "\n",
    "cfg.data.val.type = 'VideoDataset'\n",
    "cfg.data.val.ann_file = 'data/childact_split/childact_val_video.txt'\n",
    "cfg.data.val.data_prefix = 'data/childact_split/val/'\n",
    "\n",
    "# The flag is used to determine whether it is omnisource training\n",
    "cfg.setdefault('omnisource', False)\n",
    "# Modify num classes of the model in cls_head\n",
    "cfg.model.cls_head.num_classes = 7\n",
    "# We can use the pre-trained TSN model\n",
    "cfg.load_from = './childact-checkpoints/childact-slowfast2/best_top1_acc_epoch_5.pth'\n",
    "# cfg.resume_from = './childact-checkpoints/childact-slowfast/best_top1_acc_epoch_50.pth'\n",
    "\n",
    "# Set up working dir to save files and logs.\n",
    "cfg.work_dir = './childact-checkpoints/childact-slowfast3/'\n",
    "\n",
    "# The original learning rate (LR) is set for 8-GPU training.\n",
    "# We divide it by 8 since we only use one GPU.\n",
    "cfg.data.videos_per_gpu = 16\n",
    "# cfg.data.workers_per_gpu = 4\n",
    "# cfg.optimizer.type = 'Adam'\n",
    "# cfg.optimizer.weight_decay=0.0001\n",
    "\n",
    "# cfg.optimizer_config.grad_clip=None\n",
    "# cfg.optimizer.lr = 0.01\n",
    "\n",
    "cfg.lr_config.type = 'cyclic'\n",
    "cfg.total_epochs = 5\n",
    "\n",
    "# We can set the checkpoint saving interval to reduce the storage cost\n",
    "cfg.checkpoint_config.interval = 4\n",
    "# We can set the log print interval to reduce the the times of printing log\n",
    "cfg.log_config.interval = 40\n",
    "\n",
    "# Set seed thus the results are more reproducible\n",
    "cfg.seed = 42\n",
    "set_random_seed(42, deterministic=False)\n",
    "cfg.gpu_ids = range(1)\n",
    "\n",
    "# We can initialize the logger for training and have a look\n",
    "# at the final config used for training\n",
    "# print(f'Config:\\n{cfg.pretty_text}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del cfg.optimizer['momentum']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tES-qnZ3k38Z"
   },
   "source": [
    "### Train a new recognizer\n",
    "\n",
    "Finally, lets initialize the dataset and recognizer, then train a new recognizer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dDBWkdDRk6oz",
    "outputId": "85a52ef3-7b5c-4c52-8fef-00322a8c65e6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-24 13:17:39,463 - mmaction - INFO - load checkpoint from ./childact-checkpoints/childact-slowfast2/best_top1_acc_epoch_5.pth\n",
      "2021-03-24 13:17:39,464 - mmaction - INFO - Use load_from_local loader\n",
      "2021-03-24 13:17:40,173 - mmaction - INFO - Start running, host: actrec@actrec-HP-Z4-G4-Workstation, work_dir: /home/actrec/.virtualenvs/mmaction/mmaction2/childact-checkpoints/childact-slowfast3\n",
      "2021-03-24 13:17:40,174 - mmaction - INFO - workflow: [('train', 1)], max: 5 epochs\n",
      "/home/actrec/.virtualenvs/mmaction/mmaction2/mmaction/core/evaluation/eval_hooks.py:131: UserWarning: runner.meta is None. Creating a empty one.\n",
      "  warnings.warn('runner.meta is None. Creating a empty one.')\n",
      "/home/actrec/.virtualenvs/mmaction/lib/python3.6/site-packages/torch/nn/functional.py:3103: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
      "2021-03-24 13:19:14,433 - mmaction - INFO - Epoch [1][40/66]\tlr: 1.156e-02, eta: 0:11:23, time: 2.356, data_time: 1.445, memory: 12209, top1_acc: 0.7344, top5_acc: 0.9844, loss_cls: 0.6538, loss: 0.6538, grad_norm: 0.9537\n",
      "2021-03-24 13:21:38,875 - mmaction - INFO - Epoch [2][40/66]\tlr: 1.285e-02, eta: 0:06:30, time: 2.263, data_time: 1.342, memory: 12209, top1_acc: 0.7125, top5_acc: 0.9922, loss_cls: 0.6799, loss: 0.6799, grad_norm: 1.0023\n",
      "2021-03-24 13:24:06,492 - mmaction - INFO - Epoch [3][40/66]\tlr: 1.103e-02, eta: 0:04:14, time: 2.302, data_time: 1.392, memory: 12209, top1_acc: 0.7234, top5_acc: 0.9891, loss_cls: 0.6831, loss: 0.6831, grad_norm: 0.9548\n",
      "2021-03-24 13:26:32,066 - mmaction - INFO - Epoch [4][40/66]\tlr: 6.739e-03, eta: 0:02:21, time: 2.258, data_time: 1.358, memory: 12209, top1_acc: 0.7172, top5_acc: 0.9906, loss_cls: 0.6565, loss: 0.6565, grad_norm: 0.9893\n",
      "2021-03-24 13:27:28,452 - mmaction - INFO - Saving checkpoint at 4 epochs\n",
      "2021-03-24 13:29:00,810 - mmaction - INFO - Epoch [5][40/66]\tlr: 2.115e-03, eta: 0:00:39, time: 2.300, data_time: 1.373, memory: 12209, top1_acc: 0.7172, top5_acc: 0.9875, loss_cls: 0.7379, loss: 0.7379, grad_norm: 1.1530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 126/126, 6.3 task/s, elapsed: 20s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-24 13:30:14,656 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-03-24 13:30:14,658 - mmaction - INFO - \n",
      "top1_acc\t0.8571\n",
      "top5_acc\t1.0000\n",
      "2021-03-24 13:30:14,658 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-03-24 13:30:14,659 - mmaction - INFO - \n",
      "mean_acc\t0.8571\n",
      "2021-03-24 13:30:15,020 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_5.pth.\n",
      "2021-03-24 13:30:15,021 - mmaction - INFO - Best top1_acc is 0.8571 at 5 epoch.\n",
      "2021-03-24 13:30:15,021 - mmaction - INFO - Epoch(val) [5][66]\ttop1_acc: 0.8571, top5_acc: 1.0000, mean_class_accuracy: 0.8571\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "from mmaction.datasets import build_dataset\n",
    "from mmaction.models import build_model\n",
    "from mmaction.apis import train_model\n",
    "\n",
    "import mmcv\n",
    "\n",
    "# Build the dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# Build the recognizer\n",
    "model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.train_cfg, test_cfg=cfg.test_cfg)\n",
    "\n",
    "# Create work_dir\n",
    "mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
    "train_model(model, datasets, cfg, distributed=False, validate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.ipc_collect()\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "# gc.enable()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ryVoSfZVmogw"
   },
   "source": [
    "## Test the trained recognizer\n",
    "\n",
    "After finetuning the recognizer, let's check the prediction results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmaction.apis import single_gpu_test, inference_recognizer, init_recognizer\n",
    "from mmaction.datasets import build_dataloader\n",
    "from mmcv.parallel import MMDataParallel\n",
    "from mmaction.models import build_model\n",
    "from mmaction.datasets import build_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eyY3hCMwyTct",
    "outputId": "200e37c7-0da4-421f-98da-41418c3110ca",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 126/126, 0.4 task/s, elapsed: 303s, ETA:     0s\n",
      "Evaluating top_k_accuracy ...\n",
      "\n",
      "top1_acc\t0.9127\n",
      "top5_acc\t1.0000\n",
      "\n",
      "Evaluating mean_class_accuracy ...\n",
      "\n",
      "mean_acc\t0.9127\n",
      "top1_acc: 0.9127\n",
      "top5_acc: 1.0000\n",
      "mean_class_accuracy: 0.9127\n"
     ]
    }
   ],
   "source": [
    "from mmaction.apis import single_gpu_test\n",
    "from mmaction.datasets import build_dataloader\n",
    "from mmcv.parallel import MMDataParallel\n",
    "from mmaction.models import build_model\n",
    "from mmaction.datasets import build_dataset\n",
    "\n",
    "# checkpoint = './childact-checkpoints/childact-slowfast/best_top1_acc_epoch_50.pth'\n",
    "# modelt = init_recognizer(cfg, checkpoint, device='cuda:0')\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# Build a test dataloader\n",
    "dataset = build_dataset(cfg.data.test, dict(test_mode=True))\n",
    "data_loader = build_dataloader(\n",
    "        dataset,\n",
    "        videos_per_gpu=2,\n",
    "        workers_per_gpu=1,\n",
    "        dist=False,\n",
    "        shuffle=False)\n",
    "model = MMDataParallel(model, device_ids=[0])\n",
    "outputs = single_gpu_test(model, data_loader)\n",
    "\n",
    "eval_config = cfg.evaluation\n",
    "eval_config.pop('interval')\n",
    "eval_res = dataset.evaluate(outputs, **eval_config)\n",
    "for name, val in eval_res.items():\n",
    "    print(f'{name}: {val:.04f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mmaction.core.evaluation.confusion_matrix()\n",
    "# gt_confusion_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "# R(2+1)D 59.52%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "64CW6d_AaT-Q",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "3b284fd8-4ee7-4a34-90d7-5023cd123a04",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-03-24 13:44:27--  https://download.openmmlab.com/mmaction/recognition/r2plus1d/r2plus1d_r34_video_8x8x1_180e_kinetics400_rgb/r2plus1d_r34_video_8x8x1_180e_kinetics400_rgb_20200826-ab35a529.pth\n",
      "Resolving download.openmmlab.com (download.openmmlab.com)... 47.252.96.35\n",
      "Connecting to download.openmmlab.com (download.openmmlab.com)|47.252.96.35|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 255320099 (243M) [application/octet-stream]\n",
      "Saving to: ‘checkpoints/r2plus1d_r34_video_8x8x1_180e_kinetics400_rgb_20200826-ab35a529.pth’\n",
      "\n",
      "checkpoints/r2plus1 100%[===================>] 243,49M  6,98MB/s    in 32s     \n",
      "\n",
      "2021-03-24 13:45:05 (7,72 MB/s) - ‘checkpoints/r2plus1d_r34_video_8x8x1_180e_kinetics400_rgb_20200826-ab35a529.pth’ saved [255320099/255320099]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !mkdir checkpoints\n",
    "!wget -c https://download.openmmlab.com/mmaction/recognition/r2plus1d/r2plus1d_r34_video_8x8x1_180e_kinetics400_rgb/r2plus1d_r34_video_8x8x1_180e_kinetics400_rgb_20200826-ab35a529.pth \\\n",
    "      -O checkpoints/r2plus1d_r34_video_8x8x1_180e_kinetics400_rgb_20200826-ab35a529.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "LjCcmCKOjktc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mmcv import Config\n",
    "cfg = Config.fromfile('./configs/recognition/r2plus1d/r2plus1d_r34_video_8x8x1_180e_kinetics400_rgb.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "tlhu9byjjt-K",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "13d1bb01-f351-48c0-9efb-0bdf2c125d29",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "model = dict(\n",
      "    type='Recognizer3D',\n",
      "    backbone=dict(\n",
      "        type='ResNet2Plus1d',\n",
      "        depth=34,\n",
      "        pretrained=None,\n",
      "        pretrained2d=False,\n",
      "        norm_eval=False,\n",
      "        conv_cfg=dict(type='Conv2plus1d'),\n",
      "        norm_cfg=dict(type='BN3d', requires_grad=True, eps=0.001),\n",
      "        conv1_kernel=(3, 7, 7),\n",
      "        conv1_stride_t=1,\n",
      "        pool1_stride_t=1,\n",
      "        inflate=(1, 1, 1, 1),\n",
      "        spatial_strides=(1, 2, 2, 2),\n",
      "        temporal_strides=(1, 2, 2, 2),\n",
      "        zero_init_residual=False,\n",
      "        act_cfg=dict(type='ReLU')),\n",
      "    cls_head=dict(\n",
      "        type='I3DHead',\n",
      "        num_classes=7,\n",
      "        in_channels=512,\n",
      "        spatial_type='avg',\n",
      "        dropout_ratio=0.5,\n",
      "        init_std=0.01),\n",
      "    train_cfg=None,\n",
      "    test_cfg=dict(average_clips='prob'))\n",
      "checkpoint_config = dict(interval=4)\n",
      "log_config = dict(interval=25, hooks=[dict(type='TextLoggerHook')])\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = './checkpoints/r2plus1d_r34_video_8x8x1_180e_kinetics400_rgb_20200826-ab35a529.pth'\n",
      "resume_from = None\n",
      "workflow = [('train', 1)]\n",
      "dataset_type = 'VideoDataset'\n",
      "data_root = 'data/childact_split/train/'\n",
      "data_root_val = 'data/childact_split/val/'\n",
      "ann_file_train = 'data/childact_split/childact_train_video.txt'\n",
      "ann_file_val = 'data/childact_split/childact_val_video.txt'\n",
      "ann_file_test = 'data/childact_split/childact_test_video.txt'\n",
      "img_norm_cfg = dict(\n",
      "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_bgr=False)\n",
      "train_pipeline = [\n",
      "    dict(type='DecordInit'),\n",
      "    dict(type='SampleFrames', clip_len=8, frame_interval=8, num_clips=1),\n",
      "    dict(type='DecordDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='RandomResizedCrop'),\n",
      "    dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "    dict(type='Flip', flip_ratio=0.5),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "]\n",
      "val_pipeline = [\n",
      "    dict(type='DecordInit'),\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=8,\n",
      "        frame_interval=8,\n",
      "        num_clips=1,\n",
      "        test_mode=True),\n",
      "    dict(type='DecordDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='CenterCrop', crop_size=224),\n",
      "    dict(type='Flip', flip_ratio=0),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(type='DecordInit'),\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=8,\n",
      "        frame_interval=8,\n",
      "        num_clips=10,\n",
      "        test_mode=True),\n",
      "    dict(type='DecordDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='ThreeCrop', crop_size=256),\n",
      "    dict(type='Flip', flip_ratio=0),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "data = dict(\n",
      "    videos_per_gpu=16,\n",
      "    workers_per_gpu=4,\n",
      "    train=dict(\n",
      "        type='VideoDataset',\n",
      "        ann_file='data/childact_split/childact_train_video.txt',\n",
      "        data_prefix='data/childact_split/train/',\n",
      "        pipeline=[\n",
      "            dict(type='DecordInit'),\n",
      "            dict(\n",
      "                type='SampleFrames', clip_len=8, frame_interval=8,\n",
      "                num_clips=1),\n",
      "            dict(type='DecordDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='RandomResizedCrop'),\n",
      "            dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "            dict(type='Flip', flip_ratio=0.5),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "        ]),\n",
      "    val=dict(\n",
      "        type='VideoDataset',\n",
      "        ann_file='data/childact_split/childact_val_video.txt',\n",
      "        data_prefix='data/childact_split/val/',\n",
      "        pipeline=[\n",
      "            dict(type='DecordInit'),\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=8,\n",
      "                frame_interval=8,\n",
      "                num_clips=1,\n",
      "                test_mode=True),\n",
      "            dict(type='DecordDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='CenterCrop', crop_size=224),\n",
      "            dict(type='Flip', flip_ratio=0),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ],\n",
      "        test_mode=True),\n",
      "    test=dict(\n",
      "        type='VideoDataset',\n",
      "        ann_file='data/childact_split/childact_test_video.txt',\n",
      "        data_prefix='data/childact_split/test/',\n",
      "        pipeline=[\n",
      "            dict(type='DecordInit'),\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=8,\n",
      "                frame_interval=8,\n",
      "                num_clips=10,\n",
      "                test_mode=True),\n",
      "            dict(type='DecordDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='ThreeCrop', crop_size=256),\n",
      "            dict(type='Flip', flip_ratio=0),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ],\n",
      "        test_mode=True))\n",
      "evaluation = dict(\n",
      "    interval=5, metrics=['top_k_accuracy', 'mean_class_accuracy'])\n",
      "optimizer = dict(type='SGD', lr=0.2, momentum=0.9, weight_decay=0.0001)\n",
      "optimizer_config = dict(grad_clip=dict(max_norm=40, norm_type=2))\n",
      "lr_config = dict(\n",
      "    policy='cyclic',\n",
      "    target_ratio=(10, 0.0001),\n",
      "    cyclic_times=1,\n",
      "    step_ratio_up=0.4)\n",
      "total_epochs = 51\n",
      "work_dir = './childact-checkpoints/childact-r2plus1d-2'\n",
      "find_unused_parameters = False\n",
      "omnisource = False\n",
      "momentum_config = dict(\n",
      "    policy='cyclic',\n",
      "    target_ratio=(0.8947368421052632, 1),\n",
      "    cyclic_times=1,\n",
      "    step_ratio_up=0.4)\n",
      "seed = 42\n",
      "gpu_ids = range(0, 1)\n",
      "output_config = dict(\n",
      "    out='./childact-checkpoints/childact-r2plus1d-2/results.json')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mmcv.runner import set_random_seed\n",
    "\n",
    "# Modify dataset type and path\n",
    "cfg.dataset_type = 'VideoDataset'\n",
    "cfg.data_root = 'data/childact_split/train/'\n",
    "cfg.data_root_val = 'data/childact_split/val/'\n",
    "cfg.ann_file_train = 'data/childact_split/childact_train_video.txt'\n",
    "cfg.ann_file_val = 'data/childact_split/childact_val_video.txt'\n",
    "cfg.ann_file_test = 'data/childact_split/childact_test_video.txt'\n",
    "\n",
    "cfg.data.test.type = 'VideoDataset'\n",
    "cfg.data.test.ann_file = 'data/childact_split/childact_test_video.txt'\n",
    "cfg.data.test.data_prefix = 'data/childact_split/test/'\n",
    "\n",
    "cfg.data.train.type = 'VideoDataset'\n",
    "cfg.data.train.ann_file = 'data/childact_split/childact_train_video.txt'\n",
    "cfg.data.train.data_prefix = 'data/childact_split/train/'\n",
    "\n",
    "cfg.data.val.type = 'VideoDataset'\n",
    "cfg.data.val.ann_file = 'data/childact_split/childact_val_video.txt'\n",
    "cfg.data.val.data_prefix = 'data/childact_split/val/'\n",
    "\n",
    "# The flag is used to determine whether it is omnisource training\n",
    "cfg.setdefault('omnisource', False)\n",
    "# Modify num classes of the model in cls_head\n",
    "cfg.model.cls_head.num_classes = 7\n",
    "# We can use the pre-trained TSN model\n",
    "cfg.load_from = './checkpoints/r2plus1d_r34_video_8x8x1_180e_kinetics400_rgb_20200826-ab35a529.pth'\n",
    "# cfg.resume_from = './childact-checkpoints/childact-r2plus1d/latest.pth'\n",
    "\n",
    "# Set up working dir to save files and logs.\n",
    "cfg.work_dir = './childact-checkpoints/childact-r2plus1d-2'\n",
    "\n",
    "# The original learning rate (LR) is set for 8-GPU training.\n",
    "# We divide it by 8 since we only use one GPU.\n",
    "cfg.data.videos_per_gpu = 16\n",
    "# cfg.optimizer.type = 'Adam'\n",
    "# cfg.optimizer.weight_decay=0.0001\n",
    "\n",
    "# cfg.optimizer_config.grad_clip=None\n",
    "# cfg.optimizer.lr = 0.01\n",
    "\n",
    "cfg.lr_config = dict(\n",
    "    policy='cyclic',\n",
    "    target_ratio=(10, 1e-4),\n",
    "    cyclic_times=1,\n",
    "    step_ratio_up=0.4,\n",
    ")\n",
    "\n",
    "cfg.total_epochs = 51\n",
    "\n",
    "cfg.momentum_config = dict(\n",
    "    policy='cyclic',\n",
    "    target_ratio=(0.85 / 0.95, 1),\n",
    "    cyclic_times=1,\n",
    "    step_ratio_up=0.4,\n",
    ")\n",
    "\n",
    "# We can set the checkpoint saving interval to reduce the storage cost\n",
    "cfg.checkpoint_config.interval = 4\n",
    "# We can set the log print interval to reduce the the times of printing log\n",
    "cfg.log_config.interval = 25\n",
    "\n",
    "# Set seed thus the results are more reproducible\n",
    "cfg.seed = 42\n",
    "set_random_seed(42, deterministic=False)\n",
    "cfg.gpu_ids = range(1)\n",
    "\n",
    "cfg.output_config = dict(out=f'{cfg.work_dir}/results.json')\n",
    "# We can initialize the logger for training and have a look\n",
    "# at the final config used for training\n",
    "# del cfg.optimizer['momentum']\n",
    "cfg.model.backbone.norm_cfg.type = 'BN3d'\n",
    "# cfg.model.cls_head.type = \"TSNHead\"\n",
    "print(f'Config:\\n{cfg.pretty_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "dDBWkdDRk6oz",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "85a52ef3-7b5c-4c52-8fef-00322a8c65e6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-25 11:23:21,172 - mmaction - INFO - load checkpoint from ./checkpoints/r2plus1d_r34_video_8x8x1_180e_kinetics400_rgb_20200826-ab35a529.pth\n",
      "2021-03-25 11:23:21,173 - mmaction - INFO - Use load_from_local loader\n",
      "2021-03-25 11:23:21,319 - mmaction - WARNING - The model and loaded state dict do not match exactly\n",
      "\n",
      "size mismatch for cls_head.fc_cls.weight: copying a param with shape torch.Size([400, 512]) from checkpoint, the shape in current model is torch.Size([7, 512]).\n",
      "size mismatch for cls_head.fc_cls.bias: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([7]).\n",
      "2021-03-25 11:23:21,323 - mmaction - INFO - Start running, host: actrec@actrec-HP-Z4-G4-Workstation, work_dir: /home/actrec/.virtualenvs/mmaction/mmaction2/childact-checkpoints/childact-r2plus1d-2\n",
      "2021-03-25 11:23:21,324 - mmaction - INFO - workflow: [('train', 1)], max: 51 epochs\n",
      "2021-03-25 11:24:08,150 - mmaction - INFO - Epoch [1][25/66]\tlr: 2.014e-01, eta: 1:44:17, time: 1.873, data_time: 1.191, memory: 10924, top1_acc: 0.2500, top5_acc: 0.8300, loss_cls: 2.6415, loss: 2.6415, grad_norm: 6.8179\n",
      "2021-03-25 11:24:47,811 - mmaction - INFO - Epoch [1][50/66]\tlr: 2.059e-01, eta: 1:35:35, time: 1.586, data_time: 0.882, memory: 10924, top1_acc: 0.1575, top5_acc: 0.7250, loss_cls: 2.9285, loss: 2.9285, grad_norm: 7.3555\n",
      "2021-03-25 11:26:01,499 - mmaction - INFO - Epoch [2][25/66]\tlr: 2.198e-01, eta: 1:21:01, time: 1.944, data_time: 1.263, memory: 10924, top1_acc: 0.2275, top5_acc: 0.8400, loss_cls: 2.0843, loss: 2.0843, grad_norm: 1.9056\n",
      "2021-03-25 11:26:38,522 - mmaction - INFO - Epoch [2][50/66]\tlr: 2.322e-01, eta: 1:20:22, time: 1.481, data_time: 0.797, memory: 10924, top1_acc: 0.2275, top5_acc: 0.9175, loss_cls: 1.9361, loss: 1.9361, grad_norm: 1.6195\n",
      "2021-03-25 11:27:48,259 - mmaction - INFO - Epoch [3][25/66]\tlr: 2.590e-01, eta: 1:14:28, time: 1.861, data_time: 1.173, memory: 10924, top1_acc: 0.2600, top5_acc: 0.9375, loss_cls: 1.7320, loss: 1.7320, grad_norm: 0.6743\n",
      "2021-03-25 11:28:29,877 - mmaction - INFO - Epoch [3][50/66]\tlr: 2.791e-01, eta: 1:15:53, time: 1.665, data_time: 0.967, memory: 10924, top1_acc: 0.2825, top5_acc: 0.9500, loss_cls: 1.6261, loss: 1.6261, grad_norm: 0.6084\n",
      "2021-03-25 11:29:39,745 - mmaction - INFO - Epoch [4][25/66]\tlr: 3.181e-01, eta: 1:12:07, time: 1.873, data_time: 1.173, memory: 10924, top1_acc: 0.2900, top5_acc: 0.9300, loss_cls: 1.7037, loss: 1.7037, grad_norm: 0.6589\n",
      "2021-03-25 11:30:17,357 - mmaction - INFO - Epoch [4][50/66]\tlr: 3.455e-01, eta: 1:12:13, time: 1.504, data_time: 0.812, memory: 10924, top1_acc: 0.2775, top5_acc: 0.9425, loss_cls: 1.6498, loss: 1.6498, grad_norm: 0.5230\n",
      "2021-03-25 11:30:42,815 - mmaction - INFO - Saving checkpoint at 4 epochs\n",
      "2021-03-25 11:31:29,959 - mmaction - INFO - Epoch [5][25/66]\tlr: 3.958e-01, eta: 1:09:22, time: 1.852, data_time: 1.148, memory: 10924, top1_acc: 0.2800, top5_acc: 0.9625, loss_cls: 1.6421, loss: 1.6421, grad_norm: 0.4813\n",
      "2021-03-25 11:32:10,624 - mmaction - INFO - Epoch [5][50/66]\tlr: 4.297e-01, eta: 1:09:55, time: 1.627, data_time: 0.943, memory: 10924, top1_acc: 0.2600, top5_acc: 0.9500, loss_cls: 1.6521, loss: 1.6521, grad_norm: 0.4970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 126/126, 8.2 task/s, elapsed: 15s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-25 11:32:49,233 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-03-25 11:32:49,235 - mmaction - INFO - \n",
      "top1_acc\t0.3333\n",
      "top5_acc\t1.0000\n",
      "2021-03-25 11:32:49,236 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-03-25 11:32:49,237 - mmaction - INFO - \n",
      "mean_acc\t0.3333\n",
      "2021-03-25 11:32:50,071 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_5.pth.\n",
      "2021-03-25 11:32:50,072 - mmaction - INFO - Best top1_acc is 0.3333 at 5 epoch.\n",
      "2021-03-25 11:32:50,073 - mmaction - INFO - Epoch(val) [5][66]\ttop1_acc: 0.3333, top5_acc: 1.0000, mean_class_accuracy: 0.3333\n",
      "2021-03-25 11:33:34,649 - mmaction - INFO - Epoch [6][25/66]\tlr: 4.901e-01, eta: 1:07:19, time: 1.783, data_time: 1.102, memory: 10924, top1_acc: 0.2500, top5_acc: 0.9500, loss_cls: 1.5939, loss: 1.5939, grad_norm: 0.4893\n",
      "2021-03-25 11:34:17,280 - mmaction - INFO - Epoch [6][50/66]\tlr: 5.298e-01, eta: 1:07:57, time: 1.705, data_time: 1.021, memory: 10924, top1_acc: 0.2550, top5_acc: 0.9350, loss_cls: 1.7240, loss: 1.7240, grad_norm: 0.6000\n",
      "2021-03-25 11:35:24,803 - mmaction - INFO - Epoch [7][25/66]\tlr: 5.989e-01, eta: 1:05:44, time: 1.803, data_time: 1.116, memory: 10924, top1_acc: 0.2575, top5_acc: 0.9300, loss_cls: 1.7504, loss: 1.7504, grad_norm: 0.5424\n",
      "2021-03-25 11:36:02,343 - mmaction - INFO - Epoch [7][50/66]\tlr: 6.433e-01, eta: 1:05:37, time: 1.502, data_time: 0.819, memory: 10924, top1_acc: 0.2500, top5_acc: 0.9100, loss_cls: 1.8867, loss: 1.8867, grad_norm: 0.6254\n",
      "2021-03-25 11:37:14,760 - mmaction - INFO - Epoch [8][25/66]\tlr: 7.195e-01, eta: 1:03:47, time: 1.840, data_time: 1.156, memory: 10924, top1_acc: 0.2575, top5_acc: 0.9500, loss_cls: 1.7186, loss: 1.7186, grad_norm: 0.5311\n",
      "2021-03-25 11:37:57,249 - mmaction - INFO - Epoch [8][50/66]\tlr: 7.678e-01, eta: 1:04:05, time: 1.700, data_time: 1.015, memory: 10924, top1_acc: 0.2675, top5_acc: 0.9450, loss_cls: 1.7536, loss: 1.7536, grad_norm: 0.5395\n",
      "2021-03-25 11:38:19,423 - mmaction - INFO - Saving checkpoint at 8 epochs\n",
      "2021-03-25 11:39:08,826 - mmaction - INFO - Epoch [9][25/66]\tlr: 8.492e-01, eta: 1:02:36, time: 1.943, data_time: 1.259, memory: 10924, top1_acc: 0.2675, top5_acc: 0.9100, loss_cls: 1.8096, loss: 1.8096, grad_norm: 0.5640\n",
      "2021-03-25 11:39:46,717 - mmaction - INFO - Epoch [9][50/66]\tlr: 9.000e-01, eta: 1:02:25, time: 1.516, data_time: 0.832, memory: 10924, top1_acc: 0.2025, top5_acc: 0.8800, loss_cls: 1.9571, loss: 1.9571, grad_norm: 0.5745\n",
      "2021-03-25 11:40:56,513 - mmaction - INFO - Epoch [10][25/66]\tlr: 9.848e-01, eta: 1:00:53, time: 1.877, data_time: 1.194, memory: 10924, top1_acc: 0.2375, top5_acc: 0.9150, loss_cls: 1.8340, loss: 1.8340, grad_norm: 0.7156\n",
      "2021-03-25 11:41:32,522 - mmaction - INFO - Epoch [10][50/66]\tlr: 1.037e+00, eta: 1:00:32, time: 1.440, data_time: 0.760, memory: 10924, top1_acc: 0.2325, top5_acc: 0.9000, loss_cls: 1.8315, loss: 1.8315, grad_norm: 0.4903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 126/126, 8.3 task/s, elapsed: 15s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-25 11:42:12,199 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-03-25 11:42:12,202 - mmaction - INFO - \n",
      "top1_acc\t0.2698\n",
      "top5_acc\t0.9921\n",
      "2021-03-25 11:42:12,203 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-03-25 11:42:12,204 - mmaction - INFO - \n",
      "mean_acc\t0.2698\n",
      "2021-03-25 11:42:12,205 - mmaction - INFO - Epoch(val) [10][66]\ttop1_acc: 0.2698, top5_acc: 0.9921, mean_class_accuracy: 0.2698\n",
      "2021-03-25 11:43:00,088 - mmaction - INFO - Epoch [11][25/66]\tlr: 1.123e+00, eta: 0:59:10, time: 1.915, data_time: 1.231, memory: 10924, top1_acc: 0.2625, top5_acc: 0.9075, loss_cls: 1.9964, loss: 1.9964, grad_norm: 0.7338\n",
      "2021-03-25 11:43:36,159 - mmaction - INFO - Epoch [11][50/66]\tlr: 1.176e+00, eta: 0:58:48, time: 1.443, data_time: 0.760, memory: 10924, top1_acc: 0.2050, top5_acc: 0.9025, loss_cls: 1.9441, loss: 1.9441, grad_norm: 0.6621\n",
      "2021-03-25 11:44:46,809 - mmaction - INFO - Epoch [12][25/66]\tlr: 1.261e+00, eta: 0:57:31, time: 1.922, data_time: 1.238, memory: 10924, top1_acc: 0.2775, top5_acc: 0.8800, loss_cls: 2.0211, loss: 2.0211, grad_norm: 0.8548\n",
      "2021-03-25 11:45:24,244 - mmaction - INFO - Epoch [12][50/66]\tlr: 1.312e+00, eta: 0:57:13, time: 1.497, data_time: 0.815, memory: 10924, top1_acc: 0.2450, top5_acc: 0.9025, loss_cls: 1.9141, loss: 1.9141, grad_norm: 0.6086\n",
      "2021-03-25 11:45:49,590 - mmaction - INFO - Saving checkpoint at 12 epochs\n",
      "2021-03-25 11:46:36,707 - mmaction - INFO - Epoch [13][25/66]\tlr: 1.395e+00, eta: 0:55:54, time: 1.851, data_time: 1.164, memory: 10924, top1_acc: 0.2275, top5_acc: 0.8650, loss_cls: 1.8921, loss: 1.8921, grad_norm: 0.5190\n",
      "2021-03-25 11:47:14,910 - mmaction - INFO - Epoch [13][50/66]\tlr: 1.444e+00, eta: 0:55:37, time: 1.528, data_time: 0.845, memory: 10924, top1_acc: 0.2275, top5_acc: 0.8800, loss_cls: 2.6578, loss: 2.6578, grad_norm: 1.8239\n",
      "2021-03-25 11:48:19,708 - mmaction - INFO - Epoch [14][25/66]\tlr: 1.522e+00, eta: 0:54:10, time: 1.713, data_time: 1.031, memory: 10924, top1_acc: 0.2675, top5_acc: 0.8850, loss_cls: 1.9409, loss: 1.9409, grad_norm: 0.6772\n",
      "2021-03-25 11:48:58,829 - mmaction - INFO - Epoch [14][50/66]\tlr: 1.567e+00, eta: 0:53:55, time: 1.565, data_time: 0.882, memory: 10924, top1_acc: 0.2250, top5_acc: 0.8275, loss_cls: 2.1836, loss: 2.1836, grad_norm: 0.6624\n",
      "2021-03-25 11:50:06,195 - mmaction - INFO - Epoch [15][25/66]\tlr: 1.639e+00, eta: 0:52:37, time: 1.779, data_time: 1.096, memory: 10924, top1_acc: 0.2875, top5_acc: 0.8950, loss_cls: 1.8644, loss: 1.8644, grad_norm: 0.4446\n",
      "2021-03-25 11:50:45,385 - mmaction - INFO - Epoch [15][50/66]\tlr: 1.680e+00, eta: 0:52:20, time: 1.568, data_time: 0.886, memory: 10924, top1_acc: 0.2575, top5_acc: 0.8950, loss_cls: 1.8736, loss: 1.8736, grad_norm: 0.6062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 126/126, 8.4 task/s, elapsed: 15s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-25 11:51:23,839 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-03-25 11:51:23,841 - mmaction - INFO - \n",
      "top1_acc\t0.3016\n",
      "top5_acc\t0.8571\n",
      "2021-03-25 11:51:23,841 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-03-25 11:51:23,843 - mmaction - INFO - \n",
      "mean_acc\t0.3016\n",
      "2021-03-25 11:51:23,844 - mmaction - INFO - Epoch(val) [15][66]\ttop1_acc: 0.3016, top5_acc: 0.8571, mean_class_accuracy: 0.3016\n",
      "2021-03-25 11:52:07,919 - mmaction - INFO - Epoch [16][25/66]\tlr: 1.743e+00, eta: 0:51:04, time: 1.763, data_time: 1.083, memory: 10924, top1_acc: 0.2350, top5_acc: 0.9025, loss_cls: 1.8946, loss: 1.8946, grad_norm: 0.5508\n",
      "2021-03-25 11:52:49,289 - mmaction - INFO - Epoch [16][50/66]\tlr: 1.779e+00, eta: 0:50:51, time: 1.655, data_time: 0.973, memory: 10924, top1_acc: 0.3075, top5_acc: 0.8850, loss_cls: 2.0915, loss: 2.0915, grad_norm: 0.7043\n",
      "2021-03-25 11:53:09,907 - mmaction - INFO - Saving checkpoint at 16 epochs\n",
      "2021-03-25 11:53:57,810 - mmaction - INFO - Epoch [17][25/66]\tlr: 1.832e+00, eta: 0:49:43, time: 1.883, data_time: 1.202, memory: 10924, top1_acc: 0.1850, top5_acc: 0.8425, loss_cls: 2.0193, loss: 2.0193, grad_norm: 0.5125\n",
      "2021-03-25 11:54:34,941 - mmaction - INFO - Epoch [17][50/66]\tlr: 1.861e+00, eta: 0:49:19, time: 1.485, data_time: 0.803, memory: 10924, top1_acc: 0.2150, top5_acc: 0.8100, loss_cls: 2.0975, loss: 2.0975, grad_norm: 0.5265\n",
      "2021-03-25 11:55:45,840 - mmaction - INFO - Epoch [18][25/66]\tlr: 1.904e+00, eta: 0:48:14, time: 1.901, data_time: 1.220, memory: 10924, top1_acc: 0.2050, top5_acc: 0.7975, loss_cls: 2.1686, loss: 2.1686, grad_norm: 0.6301\n",
      "2021-03-25 11:56:24,895 - mmaction - INFO - Epoch [18][50/66]\tlr: 1.926e+00, eta: 0:47:53, time: 1.562, data_time: 0.879, memory: 10924, top1_acc: 0.2200, top5_acc: 0.8650, loss_cls: 2.0165, loss: 2.0165, grad_norm: 0.5438\n",
      "2021-03-25 11:57:36,601 - mmaction - INFO - Epoch [19][25/66]\tlr: 1.956e+00, eta: 0:46:51, time: 1.958, data_time: 1.276, memory: 10924, top1_acc: 0.1650, top5_acc: 0.8075, loss_cls: 2.0558, loss: 2.0558, grad_norm: 0.5081\n",
      "2021-03-25 11:58:14,935 - mmaction - INFO - Epoch [19][50/66]\tlr: 1.971e+00, eta: 0:46:28, time: 1.533, data_time: 0.851, memory: 10924, top1_acc: 0.2375, top5_acc: 0.8525, loss_cls: 1.9690, loss: 1.9690, grad_norm: 0.4917\n",
      "2021-03-25 11:59:28,169 - mmaction - INFO - Epoch [20][25/66]\tlr: 1.989e+00, eta: 0:45:26, time: 1.945, data_time: 1.263, memory: 10924, top1_acc: 0.2325, top5_acc: 0.8325, loss_cls: 2.0179, loss: 2.0179, grad_norm: 0.4786\n",
      "2021-03-25 12:00:07,706 - mmaction - INFO - Epoch [20][50/66]\tlr: 1.995e+00, eta: 0:45:04, time: 1.581, data_time: 0.902, memory: 10924, top1_acc: 0.1800, top5_acc: 0.8275, loss_cls: 1.9952, loss: 1.9952, grad_norm: 0.4913\n",
      "2021-03-25 12:00:31,111 - mmaction - INFO - Saving checkpoint at 20 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 126/126, 8.3 task/s, elapsed: 15s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-25 12:00:47,178 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-03-25 12:00:47,180 - mmaction - INFO - \n",
      "top1_acc\t0.1508\n",
      "top5_acc\t0.7143\n",
      "2021-03-25 12:00:47,181 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-03-25 12:00:47,182 - mmaction - INFO - \n",
      "mean_acc\t0.1508\n",
      "2021-03-25 12:00:47,183 - mmaction - INFO - Epoch(val) [20][66]\ttop1_acc: 0.1508, top5_acc: 0.7143, mean_class_accuracy: 0.1508\n",
      "2021-03-25 12:01:35,442 - mmaction - INFO - Epoch [21][25/66]\tlr: 2.000e+00, eta: 0:44:02, time: 1.930, data_time: 1.244, memory: 10924, top1_acc: 0.1975, top5_acc: 0.7975, loss_cls: 2.0711, loss: 2.0711, grad_norm: 0.7345\n",
      "2021-03-25 12:02:14,519 - mmaction - INFO - Epoch [21][50/66]\tlr: 1.999e+00, eta: 0:43:39, time: 1.563, data_time: 0.854, memory: 10924, top1_acc: 0.2025, top5_acc: 0.8775, loss_cls: 1.9971, loss: 1.9971, grad_norm: 0.5171\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-be92e24cff32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Create work_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mmmcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir_or_exist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mosp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwork_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistributed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.virtualenvs/mmaction/mmaction2/mmaction/apis/train.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dataset, cfg, distributed, validate, timestamp, meta)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0momnisource\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0mrunner_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_ratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkflow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mrunner_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.virtualenvs/mmaction/lib/python3.6/site-packages/mmcv/runner/epoch_based_runner.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data_loaders, workflow, max_epochs, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_epochs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m                     \u001b[0mepoch_runner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# wait for some hooks like loggers to finish\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/mmaction/lib/python3.6/site-packages/mmcv/runner/epoch_based_runner.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_loader, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inner_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'before_train_iter'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_train_iter'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/mmaction/lib/python3.6/site-packages/mmcv/runner/epoch_based_runner.py\u001b[0m in \u001b[0;36mrun_iter\u001b[0;34m(self, data_batch, train_mode, **kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtrain_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             outputs = self.model.train_step(data_batch, self.optimizer,\n\u001b[0;32m---> 30\u001b[0;31m                                             **kwargs)\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/mmaction/lib/python3.6/site-packages/mmcv/parallel/data_parallel.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mval_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/mmaction/mmaction2/mmaction/models/recognizers/base.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, data_batch, optimizer, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0maux_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0maux_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/mmaction/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/mmaction/mmaction2/mmaction/models/recognizers/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, imgs, label, return_loss, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Label should not be None.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/mmaction/mmaction2/mmaction/models/recognizers/recognizer3d.py\u001b[0m in \u001b[0;36mforward_train\u001b[0;34m(self, imgs, labels, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mcls_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcls_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mgt_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mloss_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcls_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/mmaction/mmaction2/mmaction/models/heads/base.py\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, cls_score, labels, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_class\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             top_k_acc = top_k_accuracy(cls_score.detach().cpu().numpy(),\n\u001b[0m\u001b[1;32m     85\u001b[0m                                        labels.detach().cpu().numpy(), (1, 5))\n\u001b[1;32m     86\u001b[0m             losses['top1_acc'] = torch.tensor(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "from mmaction.datasets import build_dataset\n",
    "from mmaction.models import build_model\n",
    "from mmaction.apis import train_model\n",
    "\n",
    "import mmcv\n",
    "\n",
    "# Build the dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# Build the recognizer\n",
    "model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.train_cfg, test_cfg=cfg.test_cfg)\n",
    "\n",
    "# Create work_dir\n",
    "mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
    "train_model(model, datasets, cfg, distributed=False, validate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f\"{cfg.work_dir}/model50e\", 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eyY3hCMwyTct",
    "outputId": "200e37c7-0da4-421f-98da-41418c3110ca",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 126/126, 0.7 task/s, elapsed: 180s, ETA:     0s\n",
      "Evaluating top_k_accuracy ...\n",
      "\n",
      "top1_acc\t0.5952\n",
      "top5_acc\t1.0000\n",
      "\n",
      "Evaluating mean_class_accuracy ...\n",
      "\n",
      "mean_acc\t0.5952\n",
      "top1_acc: 0.5952\n",
      "top5_acc: 1.0000\n",
      "mean_class_accuracy: 0.5952\n"
     ]
    }
   ],
   "source": [
    "from mmaction.apis import single_gpu_test\n",
    "from mmaction.datasets import build_dataloader, load_annotations, build_dataset\n",
    "from mmcv.parallel import MMDataParallel\n",
    "from mmaction.models import build_model\n",
    "\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# Build a test dataloader\n",
    "dataset = build_dataset(cfg.data.test, dict(test_mode=True))\n",
    "data_loader = build_dataloader(\n",
    "        dataset,\n",
    "        videos_per_gpu=4,\n",
    "        workers_per_gpu=1,\n",
    "        dist=False,\n",
    "        shuffle=False)\n",
    "model = MMDataParallel(model, device_ids=[0])\n",
    "outputs = single_gpu_test(model, data_loader)\n",
    "\n",
    "eval_config = cfg.evaluation\n",
    "eval_config.pop('interval')\n",
    "eval_res = dataset.evaluate(outputs, **eval_config)\n",
    "for name, val in eval_res.items():\n",
    "    print(f'{name}: {val:.04f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from mmaction.core import confusion_matrix \n",
    "\n",
    "gt_labels = [ann['label'] for ann in dataset.load_annotations()]\n",
    "pred = np.argmax(outputs, axis=1)\n",
    "cf_mat = confusion_matrix(pred, gt_labels).astype(float)\n",
    "\n",
    "sns.heatmap(cf_mat, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "# TSM 93.65%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "64CW6d_AaT-Q",
    "outputId": "3b284fd8-4ee7-4a34-90d7-5023cd123a04",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-03-24 19:15:04--  https://download.openmmlab.com/mmaction/recognition/tsm/tsm_r50_video_1x1x8_100e_kinetics400_rgb/tsm_r50_video_1x1x8_100e_kinetics400_rgb_20200702-a77f4328.pth\n",
      "Resolving download.openmmlab.com (download.openmmlab.com)... 47.75.20.25\n",
      "Connecting to download.openmmlab.com (download.openmmlab.com)|47.75.20.25|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 97579687 (93M) [application/octet-stream]\n",
      "Saving to: ‘checkpoints/tsm_r50_video_1x1x8_100e_kinetics400_rgb_20200702-a77f4328.pth’\n",
      "\n",
      "checkpoints/tsm_r50 100%[===================>]  93,06M  8,72MB/s    in 12s     \n",
      "\n",
      "2021-03-24 19:15:20 (7,71 MB/s) - ‘checkpoints/tsm_r50_video_1x1x8_100e_kinetics400_rgb_20200702-a77f4328.pth’ saved [97579687/97579687]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !mkdir checkpoints\n",
    "!wget -c https://download.openmmlab.com/mmaction/recognition/tsm/tsm_r50_video_1x1x8_100e_kinetics400_rgb/tsm_r50_video_1x1x8_100e_kinetics400_rgb_20200702-a77f4328.pth \\\n",
    "      -O checkpoints/tsm_r50_video_1x1x8_100e_kinetics400_rgb_20200702-a77f4328.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "LjCcmCKOjktc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mmcv import Config\n",
    "cfg = Config.fromfile('./configs/recognition/tsm/tsm_r50_video_1x1x8_50e_kinetics400_rgb.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "tlhu9byjjt-K",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "13d1bb01-f351-48c0-9efb-0bdf2c125d29",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "model = dict(\n",
      "    type='Recognizer2D',\n",
      "    backbone=dict(\n",
      "        type='ResNetTSM',\n",
      "        pretrained='torchvision://resnet50',\n",
      "        depth=50,\n",
      "        norm_eval=False,\n",
      "        shift_div=8),\n",
      "    cls_head=dict(\n",
      "        type='TSMHead',\n",
      "        num_classes=7,\n",
      "        in_channels=2048,\n",
      "        spatial_type='avg',\n",
      "        consensus=dict(type='AvgConsensus', dim=1),\n",
      "        dropout_ratio=0.5,\n",
      "        init_std=0.001,\n",
      "        is_shift=True),\n",
      "    train_cfg=None,\n",
      "    test_cfg=dict(average_clips='prob'))\n",
      "optimizer = dict(\n",
      "    type='SGD',\n",
      "    constructor='TSMOptimizerConstructor',\n",
      "    paramwise_cfg=dict(fc_lr5=True),\n",
      "    lr=0.02,\n",
      "    weight_decay=0.0001)\n",
      "optimizer_config = dict(grad_clip=dict(max_norm=20, norm_type=2))\n",
      "lr_config = dict(\n",
      "    policy='cyclic',\n",
      "    target_ratio=(10, 0.0001),\n",
      "    cyclic_times=1,\n",
      "    step_ratio_up=0.4)\n",
      "total_epochs = 51\n",
      "checkpoint_config = dict(interval=4)\n",
      "log_config = dict(interval=25, hooks=[dict(type='TextLoggerHook')])\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = 'checkpoints/tsm_r50_video_1x1x8_100e_kinetics400_rgb_20200702-a77f4328.pth'\n",
      "resume_from = None\n",
      "workflow = [('train', 1)]\n",
      "dataset_type = 'VideoDataset'\n",
      "data_root = 'data/childact_split/train/'\n",
      "data_root_val = 'data/childact_split/val/'\n",
      "ann_file_train = 'data/childact_split/childact_train_video.txt'\n",
      "ann_file_val = 'data/childact_split/childact_val_video.txt'\n",
      "ann_file_test = 'data/childact_split/childact_test_video.txt'\n",
      "img_norm_cfg = dict(\n",
      "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_bgr=False)\n",
      "train_pipeline = [\n",
      "    dict(type='DecordInit'),\n",
      "    dict(type='SampleFrames', clip_len=1, frame_interval=1, num_clips=8),\n",
      "    dict(type='DecordDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(\n",
      "        type='MultiScaleCrop',\n",
      "        input_size=224,\n",
      "        scales=(1, 0.875, 0.75, 0.66),\n",
      "        random_crop=False,\n",
      "        max_wh_scale_gap=1,\n",
      "        num_fixed_crops=13),\n",
      "    dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "    dict(type='Flip', flip_ratio=0.5),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "]\n",
      "val_pipeline = [\n",
      "    dict(type='DecordInit'),\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=1,\n",
      "        frame_interval=1,\n",
      "        num_clips=8,\n",
      "        test_mode=True),\n",
      "    dict(type='DecordDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='CenterCrop', crop_size=224),\n",
      "    dict(type='Flip', flip_ratio=0),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(type='DecordInit'),\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=1,\n",
      "        frame_interval=1,\n",
      "        num_clips=8,\n",
      "        test_mode=True),\n",
      "    dict(type='DecordDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='CenterCrop', crop_size=224),\n",
      "    dict(type='Flip', flip_ratio=0),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "data = dict(\n",
      "    videos_per_gpu=24,\n",
      "    workers_per_gpu=4,\n",
      "    train=dict(\n",
      "        type='VideoDataset',\n",
      "        ann_file='data/childact_split/childact_train_video.txt',\n",
      "        data_prefix='data/childact_split/train/',\n",
      "        pipeline=[\n",
      "            dict(type='DecordInit'),\n",
      "            dict(\n",
      "                type='SampleFrames', clip_len=1, frame_interval=1,\n",
      "                num_clips=8),\n",
      "            dict(type='DecordDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(\n",
      "                type='MultiScaleCrop',\n",
      "                input_size=224,\n",
      "                scales=(1, 0.875, 0.75, 0.66),\n",
      "                random_crop=False,\n",
      "                max_wh_scale_gap=1,\n",
      "                num_fixed_crops=13),\n",
      "            dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "            dict(type='Flip', flip_ratio=0.5),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "        ]),\n",
      "    val=dict(\n",
      "        type='VideoDataset',\n",
      "        ann_file='data/childact_split/childact_val_video.txt',\n",
      "        data_prefix='data/childact_split/val/',\n",
      "        pipeline=[\n",
      "            dict(type='DecordInit'),\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=1,\n",
      "                frame_interval=1,\n",
      "                num_clips=8,\n",
      "                test_mode=True),\n",
      "            dict(type='DecordDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='CenterCrop', crop_size=224),\n",
      "            dict(type='Flip', flip_ratio=0),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ]),\n",
      "    test=dict(\n",
      "        type='VideoDataset',\n",
      "        ann_file='data/childact_split/childact_test_video.txt',\n",
      "        data_prefix='data/childact_split/test/',\n",
      "        pipeline=[\n",
      "            dict(type='DecordInit'),\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=1,\n",
      "                frame_interval=1,\n",
      "                num_clips=8,\n",
      "                test_mode=True),\n",
      "            dict(type='DecordDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='CenterCrop', crop_size=224),\n",
      "            dict(type='Flip', flip_ratio=0),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ]))\n",
      "evaluation = dict(\n",
      "    interval=5, metrics=['top_k_accuracy', 'mean_class_accuracy'])\n",
      "work_dir = './childact-checkpoints/childact-tsm'\n",
      "omnisource = False\n",
      "seed = 42\n",
      "gpu_ids = range(0, 1)\n",
      "output_config = dict(out='./childact-checkpoints/childact-tsm/results.json')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mmcv.runner import set_random_seed\n",
    "\n",
    "# Modify dataset type and path\n",
    "cfg.dataset_type = 'VideoDataset'\n",
    "cfg.data_root = 'data/childact_split/train/'\n",
    "cfg.data_root_val = 'data/childact_split/val/'\n",
    "cfg.ann_file_train = 'data/childact_split/childact_train_video.txt'\n",
    "cfg.ann_file_val = 'data/childact_split/childact_val_video.txt'\n",
    "cfg.ann_file_test = 'data/childact_split/childact_test_video.txt'\n",
    "\n",
    "cfg.data.test.type = 'VideoDataset'\n",
    "cfg.data.test.ann_file = 'data/childact_split/childact_test_video.txt'\n",
    "cfg.data.test.data_prefix = 'data/childact_split/test/'\n",
    "\n",
    "cfg.data.train.type = 'VideoDataset'\n",
    "cfg.data.train.ann_file = 'data/childact_split/childact_train_video.txt'\n",
    "cfg.data.train.data_prefix = 'data/childact_split/train/'\n",
    "\n",
    "cfg.data.val.type = 'VideoDataset'\n",
    "cfg.data.val.ann_file = 'data/childact_split/childact_val_video.txt'\n",
    "cfg.data.val.data_prefix = 'data/childact_split/val/'\n",
    "\n",
    "# The flag is used to determine whether it is omnisource training\n",
    "cfg.setdefault('omnisource', False)\n",
    "# Modify num classes of the model in cls_head\n",
    "cfg.model.cls_head.num_classes = 7\n",
    "# We can use the pre-trained TSN model\n",
    "cfg.load_from = 'checkpoints/tsm_r50_video_1x1x8_100e_kinetics400_rgb_20200702-a77f4328.pth'\n",
    "# cfg.resume_from = './childact-mm/latest.pth'\n",
    "\n",
    "# Set up working dir to save files and logs.\n",
    "cfg.work_dir = './childact-checkpoints/childact-tsm'\n",
    "\n",
    "# The original learning rate (LR) is set for 8-GPU training.\n",
    "# We divide it by 8 since we only use one GPU.\n",
    "cfg.data.videos_per_gpu = 24\n",
    "# cfg.optimizer.type = 'Adam'\n",
    "# cfg.optimizer.weight_decay=0.0001\n",
    "\n",
    "# cfg.optimizer_config.grad_clip=None\n",
    "# cfg.optimizer.lr = 0.01\n",
    "\n",
    "cfg.lr_config = dict(\n",
    "    policy='cyclic',\n",
    "    target_ratio=(10, 1e-4),\n",
    "    cyclic_times=1,\n",
    "    step_ratio_up=0.4,\n",
    ")\n",
    "\n",
    "cfg.total_epochs = 51\n",
    "\n",
    "cfg.momentum_config = dict(\n",
    "    policy='cyclic',\n",
    "    target_ratio=(0.85 / 0.95, 1),\n",
    "    cyclic_times=1,\n",
    "    step_ratio_up=0.4,\n",
    ")\n",
    "\n",
    "# We can set the checkpoint saving interval to reduce the storage cost\n",
    "cfg.checkpoint_config.interval = 4\n",
    "# We can set the log print interval to reduce the the times of printing log\n",
    "cfg.log_config.interval = 25\n",
    "\n",
    "# Set seed thus the results are more reproducible\n",
    "cfg.seed = 42\n",
    "set_random_seed(42, deterministic=False)\n",
    "cfg.gpu_ids = range(1)\n",
    "\n",
    "cfg.output_config = dict(out=f'{cfg.work_dir}/results.json')\n",
    "# We can initialize the logger for training and have a look\n",
    "# at the final config used for training\n",
    "del cfg.optimizer['momentum']\n",
    "print(f'Config:\\n{cfg.pretty_text}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "dDBWkdDRk6oz",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "85a52ef3-7b5c-4c52-8fef-00322a8c65e6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-24 21:03:31,550 - mmaction - INFO - These parameters in pretrained checkpoint are not loaded: {'fc.weight', 'fc.bias'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use load_from_torchvision loader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-24 21:03:33,980 - mmaction - INFO - load checkpoint from checkpoints/tsm_r50_video_1x1x8_100e_kinetics400_rgb_20200702-a77f4328.pth\n",
      "2021-03-24 21:03:33,982 - mmaction - INFO - Use load_from_local loader\n",
      "2021-03-24 21:03:34,081 - mmaction - WARNING - The model and loaded state dict do not match exactly\n",
      "\n",
      "size mismatch for cls_head.fc_cls.weight: copying a param with shape torch.Size([400, 2048]) from checkpoint, the shape in current model is torch.Size([7, 2048]).\n",
      "size mismatch for cls_head.fc_cls.bias: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([7]).\n",
      "2021-03-24 21:03:34,086 - mmaction - INFO - Start running, host: actrec@actrec-HP-Z4-G4-Workstation, work_dir: /home/actrec/.virtualenvs/mmaction/mmaction2/childact-checkpoints/childact-tsm\n",
      "2021-03-24 21:03:34,087 - mmaction - INFO - workflow: [('train', 1)], max: 51 epochs\n",
      "/home/actrec/.virtualenvs/mmaction/mmaction2/mmaction/core/evaluation/eval_hooks.py:131: UserWarning: runner.meta is None. Creating a empty one.\n",
      "  warnings.warn('runner.meta is None. Creating a empty one.')\n",
      "2021-03-24 21:06:25,694 - mmaction - INFO - Epoch [1][25/44]\tlr: 2.032e-02, eta: 4:13:51, time: 6.864, data_time: 5.883, memory: 20204, top1_acc: 0.4633, top5_acc: 0.9567, loss_cls: 1.3184, loss: 1.3184, grad_norm: 3.1978\n",
      "2021-03-24 21:11:00,838 - mmaction - INFO - Epoch [2][25/44]\tlr: 2.254e-02, eta: 3:01:16, time: 6.938, data_time: 5.963, memory: 20204, top1_acc: 0.7883, top5_acc: 0.9983, loss_cls: 0.5617, loss: 0.5617, grad_norm: 4.1688\n",
      "2021-03-24 21:15:26,642 - mmaction - INFO - Epoch [3][25/44]\tlr: 2.684e-02, eta: 2:42:07, time: 6.832, data_time: 5.857, memory: 20204, top1_acc: 0.8417, top5_acc: 1.0000, loss_cls: 0.4150, loss: 0.4150, grad_norm: 4.0095\n",
      "2021-03-24 21:19:46,715 - mmaction - INFO - Epoch [4][25/44]\tlr: 3.310e-02, eta: 2:30:22, time: 6.516, data_time: 5.540, memory: 20204, top1_acc: 0.8383, top5_acc: 0.9967, loss_cls: 0.4149, loss: 0.4149, grad_norm: 4.5450\n",
      "2021-03-24 21:21:35,996 - mmaction - INFO - Saving checkpoint at 4 epochs\n",
      "2021-03-24 21:24:21,918 - mmaction - INFO - Epoch [5][25/44]\tlr: 4.119e-02, eta: 2:23:03, time: 6.630, data_time: 5.657, memory: 20204, top1_acc: 0.8517, top5_acc: 0.9983, loss_cls: 0.3703, loss: 0.3703, grad_norm: 4.2365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 126/126, 2.6 task/s, elapsed: 48s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-24 21:26:56,186 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-03-24 21:26:56,188 - mmaction - INFO - \n",
      "top1_acc\t0.8016\n",
      "top5_acc\t1.0000\n",
      "2021-03-24 21:26:56,188 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-03-24 21:26:56,190 - mmaction - INFO - \n",
      "mean_acc\t0.8016\n",
      "2021-03-24 21:26:56,341 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_5.pth.\n",
      "2021-03-24 21:26:56,342 - mmaction - INFO - Best top1_acc is 0.8016 at 5 epoch.\n",
      "2021-03-24 21:26:56,343 - mmaction - INFO - Epoch(val) [5][44]\ttop1_acc: 0.8016, top5_acc: 1.0000, mean_class_accuracy: 0.8016\n",
      "2021-03-24 21:29:50,484 - mmaction - INFO - Epoch [6][25/44]\tlr: 5.091e-02, eta: 2:18:31, time: 6.965, data_time: 5.991, memory: 20204, top1_acc: 0.8633, top5_acc: 0.9983, loss_cls: 0.3578, loss: 0.3578, grad_norm: 4.3697\n",
      "2021-03-24 21:34:28,680 - mmaction - INFO - Epoch [7][25/44]\tlr: 6.203e-02, eta: 2:15:02, time: 7.168, data_time: 6.192, memory: 20204, top1_acc: 0.8700, top5_acc: 0.9983, loss_cls: 0.3594, loss: 0.3594, grad_norm: 3.4862\n",
      "2021-03-24 21:38:51,003 - mmaction - INFO - Epoch [8][25/44]\tlr: 7.429e-02, eta: 2:10:30, time: 6.668, data_time: 5.691, memory: 20204, top1_acc: 0.8433, top5_acc: 0.9983, loss_cls: 0.4086, loss: 0.4086, grad_norm: 3.8250\n",
      "2021-03-24 21:40:36,587 - mmaction - INFO - Saving checkpoint at 8 epochs\n",
      "2021-03-24 21:43:27,767 - mmaction - INFO - Epoch [9][25/44]\tlr: 8.739e-02, eta: 2:06:44, time: 6.841, data_time: 5.864, memory: 20204, top1_acc: 0.8467, top5_acc: 0.9983, loss_cls: 0.3808, loss: 0.3808, grad_norm: 3.2192\n",
      "2021-03-24 21:47:55,665 - mmaction - INFO - Epoch [10][25/44]\tlr: 1.010e-01, eta: 2:03:01, time: 6.763, data_time: 5.787, memory: 20204, top1_acc: 0.8433, top5_acc: 1.0000, loss_cls: 0.3620, loss: 0.3620, grad_norm: 3.3093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 126/126, 2.6 task/s, elapsed: 48s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-24 21:50:22,735 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-03-24 21:50:22,737 - mmaction - INFO - \n",
      "top1_acc\t0.8492\n",
      "top5_acc\t1.0000\n",
      "2021-03-24 21:50:22,737 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-03-24 21:50:22,739 - mmaction - INFO - \n",
      "mean_acc\t0.8492\n",
      "2021-03-24 21:50:22,912 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_10.pth.\n",
      "2021-03-24 21:50:22,912 - mmaction - INFO - Best top1_acc is 0.8492 at 10 epoch.\n",
      "2021-03-24 21:50:22,913 - mmaction - INFO - Epoch(val) [10][44]\ttop1_acc: 0.8492, top5_acc: 1.0000, mean_class_accuracy: 0.8492\n",
      "2021-03-24 21:53:16,024 - mmaction - INFO - Epoch [11][25/44]\tlr: 1.149e-01, eta: 1:59:43, time: 6.924, data_time: 5.948, memory: 20204, top1_acc: 0.8817, top5_acc: 1.0000, loss_cls: 0.3227, loss: 0.3227, grad_norm: 2.9154\n",
      "2021-03-24 21:57:42,847 - mmaction - INFO - Epoch [12][25/44]\tlr: 1.286e-01, eta: 1:56:20, time: 6.806, data_time: 5.830, memory: 20204, top1_acc: 0.8550, top5_acc: 0.9983, loss_cls: 0.3954, loss: 0.3954, grad_norm: 3.6179\n",
      "2021-03-24 21:59:27,183 - mmaction - INFO - Saving checkpoint at 12 epochs\n",
      "2021-03-24 22:02:22,179 - mmaction - INFO - Epoch [13][25/44]\tlr: 1.419e-01, eta: 1:53:16, time: 6.994, data_time: 6.020, memory: 20204, top1_acc: 0.8467, top5_acc: 0.9967, loss_cls: 0.4247, loss: 0.4247, grad_norm: 2.8231\n",
      "2021-03-24 22:06:52,903 - mmaction - INFO - Epoch [14][25/44]\tlr: 1.545e-01, eta: 1:50:05, time: 6.867, data_time: 5.891, memory: 20204, top1_acc: 0.8617, top5_acc: 1.0000, loss_cls: 0.3434, loss: 0.3434, grad_norm: 2.8790\n",
      "2021-03-24 22:11:20,068 - mmaction - INFO - Epoch [15][25/44]\tlr: 1.659e-01, eta: 1:46:49, time: 6.736, data_time: 5.761, memory: 20204, top1_acc: 0.8483, top5_acc: 1.0000, loss_cls: 0.3807, loss: 0.3807, grad_norm: 3.1518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 126/126, 2.6 task/s, elapsed: 48s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-24 22:13:51,448 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-03-24 22:13:51,450 - mmaction - INFO - \n",
      "top1_acc\t0.8254\n",
      "top5_acc\t1.0000\n",
      "2021-03-24 22:13:51,451 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-03-24 22:13:51,452 - mmaction - INFO - \n",
      "mean_acc\t0.8254\n",
      "2021-03-24 22:13:51,453 - mmaction - INFO - Epoch(val) [15][44]\ttop1_acc: 0.8254, top5_acc: 1.0000, mean_class_accuracy: 0.8254\n",
      "2021-03-24 22:16:43,943 - mmaction - INFO - Epoch [16][25/44]\tlr: 1.761e-01, eta: 1:43:45, time: 6.899, data_time: 5.905, memory: 20204, top1_acc: 0.8417, top5_acc: 1.0000, loss_cls: 0.3540, loss: 0.3540, grad_norm: 2.6687\n",
      "2021-03-24 22:18:21,773 - mmaction - INFO - Saving checkpoint at 16 epochs\n",
      "2021-03-24 22:21:12,732 - mmaction - INFO - Epoch [17][25/44]\tlr: 1.847e-01, eta: 1:40:39, time: 6.831, data_time: 5.806, memory: 20204, top1_acc: 0.8600, top5_acc: 1.0000, loss_cls: 0.3870, loss: 0.3870, grad_norm: 3.2032\n",
      "2021-03-24 22:25:49,296 - mmaction - INFO - Epoch [18][25/44]\tlr: 1.915e-01, eta: 1:37:45, time: 7.039, data_time: 6.065, memory: 20204, top1_acc: 0.8467, top5_acc: 1.0000, loss_cls: 0.3955, loss: 0.3955, grad_norm: 2.7949\n",
      "2021-03-24 22:30:21,211 - mmaction - INFO - Epoch [19][25/44]\tlr: 1.964e-01, eta: 1:34:43, time: 6.867, data_time: 5.877, memory: 20204, top1_acc: 0.8483, top5_acc: 0.9967, loss_cls: 0.4092, loss: 0.4092, grad_norm: 2.6388\n",
      "2021-03-24 22:34:51,821 - mmaction - INFO - Epoch [20][25/44]\tlr: 1.992e-01, eta: 1:31:41, time: 6.859, data_time: 5.879, memory: 20204, top1_acc: 0.8667, top5_acc: 1.0000, loss_cls: 0.3351, loss: 0.3351, grad_norm: 2.3010\n",
      "2021-03-24 22:36:29,026 - mmaction - INFO - Saving checkpoint at 20 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 126/126, 2.6 task/s, elapsed: 48s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-24 22:37:17,321 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-03-24 22:37:17,323 - mmaction - INFO - \n",
      "top1_acc\t0.8254\n",
      "top5_acc\t1.0000\n",
      "2021-03-24 22:37:17,323 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-03-24 22:37:17,325 - mmaction - INFO - \n",
      "mean_acc\t0.8254\n",
      "2021-03-24 22:37:17,326 - mmaction - INFO - Epoch(val) [20][44]\ttop1_acc: 0.8254, top5_acc: 1.0000, mean_class_accuracy: 0.8254\n",
      "2021-03-24 22:40:03,624 - mmaction - INFO - Epoch [21][25/44]\tlr: 2.000e-01, eta: 1:28:33, time: 6.652, data_time: 5.674, memory: 20204, top1_acc: 0.8833, top5_acc: 1.0000, loss_cls: 0.2851, loss: 0.2851, grad_norm: 1.9111\n",
      "2021-03-24 22:44:38,747 - mmaction - INFO - Epoch [22][25/44]\tlr: 1.993e-01, eta: 1:25:35, time: 6.870, data_time: 5.893, memory: 20204, top1_acc: 0.8867, top5_acc: 1.0000, loss_cls: 0.2601, loss: 0.2601, grad_norm: 2.1638\n",
      "2021-03-24 22:49:22,392 - mmaction - INFO - Epoch [23][25/44]\tlr: 1.976e-01, eta: 1:22:50, time: 7.289, data_time: 6.287, memory: 20204, top1_acc: 0.8250, top5_acc: 1.0000, loss_cls: 0.5038, loss: 0.5038, grad_norm: 3.5063\n",
      "2021-03-24 22:53:49,042 - mmaction - INFO - Epoch [24][25/44]\tlr: 1.948e-01, eta: 1:19:52, time: 6.871, data_time: 5.877, memory: 20204, top1_acc: 0.8867, top5_acc: 0.9983, loss_cls: 0.2872, loss: 0.2872, grad_norm: 2.0569\n",
      "2021-03-24 22:55:28,710 - mmaction - INFO - Saving checkpoint at 24 epochs\n",
      "2021-03-24 22:58:30,402 - mmaction - INFO - Epoch [25][25/44]\tlr: 1.910e-01, eta: 1:17:04, time: 7.261, data_time: 6.271, memory: 20204, top1_acc: 0.8983, top5_acc: 1.0000, loss_cls: 0.2459, loss: 0.2459, grad_norm: 2.1627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 126/126, 2.6 task/s, elapsed: 48s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-24 23:00:59,644 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-03-24 23:00:59,647 - mmaction - INFO - \n",
      "top1_acc\t0.8175\n",
      "top5_acc\t1.0000\n",
      "2021-03-24 23:00:59,648 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-03-24 23:00:59,651 - mmaction - INFO - \n",
      "mean_acc\t0.8175\n",
      "2021-03-24 23:00:59,652 - mmaction - INFO - Epoch(val) [25][44]\ttop1_acc: 0.8175, top5_acc: 1.0000, mean_class_accuracy: 0.8175\n",
      "2021-03-24 23:03:51,401 - mmaction - INFO - Epoch [26][25/44]\tlr: 1.863e-01, eta: 1:14:06, time: 6.870, data_time: 5.887, memory: 20204, top1_acc: 0.8933, top5_acc: 1.0000, loss_cls: 0.2764, loss: 0.2764, grad_norm: 2.1380\n",
      "2021-03-24 23:08:27,656 - mmaction - INFO - Epoch [27][25/44]\tlr: 1.807e-01, eta: 1:11:07, time: 6.820, data_time: 5.837, memory: 20204, top1_acc: 0.8933, top5_acc: 0.9983, loss_cls: 0.2690, loss: 0.2690, grad_norm: 2.0275\n",
      "2021-03-24 23:12:53,090 - mmaction - INFO - Epoch [28][25/44]\tlr: 1.742e-01, eta: 1:08:04, time: 6.593, data_time: 5.600, memory: 20204, top1_acc: 0.9167, top5_acc: 1.0000, loss_cls: 0.2153, loss: 0.2153, grad_norm: 1.7606\n",
      "2021-03-24 23:14:46,727 - mmaction - INFO - Saving checkpoint at 28 epochs\n",
      "2021-03-24 23:17:38,587 - mmaction - INFO - Epoch [29][25/44]\tlr: 1.669e-01, eta: 1:05:08, time: 6.868, data_time: 5.887, memory: 20204, top1_acc: 0.8950, top5_acc: 1.0000, loss_cls: 0.2431, loss: 0.2431, grad_norm: 1.8443\n",
      "2021-03-24 23:22:12,771 - mmaction - INFO - Epoch [30][25/44]\tlr: 1.590e-01, eta: 1:02:12, time: 6.900, data_time: 5.912, memory: 20204, top1_acc: 0.9117, top5_acc: 1.0000, loss_cls: 0.2420, loss: 0.2420, grad_norm: 2.0150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 126/126, 2.6 task/s, elapsed: 49s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-24 23:24:46,975 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-03-24 23:24:46,977 - mmaction - INFO - \n",
      "top1_acc\t0.8730\n",
      "top5_acc\t1.0000\n",
      "2021-03-24 23:24:46,977 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-03-24 23:24:46,979 - mmaction - INFO - \n",
      "mean_acc\t0.8730\n",
      "2021-03-24 23:24:47,130 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_30.pth.\n",
      "2021-03-24 23:24:47,131 - mmaction - INFO - Best top1_acc is 0.8730 at 30 epoch.\n",
      "2021-03-24 23:24:47,132 - mmaction - INFO - Epoch(val) [30][44]\ttop1_acc: 0.8730, top5_acc: 1.0000, mean_class_accuracy: 0.8730\n",
      "2021-03-24 23:27:36,817 - mmaction - INFO - Epoch [31][25/44]\tlr: 1.504e-01, eta: 0:59:15, time: 6.787, data_time: 5.808, memory: 20204, top1_acc: 0.9150, top5_acc: 0.9983, loss_cls: 0.2196, loss: 0.2196, grad_norm: 1.8701\n",
      "2021-03-24 23:32:14,860 - mmaction - INFO - Epoch [32][25/44]\tlr: 1.413e-01, eta: 0:56:22, time: 7.025, data_time: 6.044, memory: 20204, top1_acc: 0.9217, top5_acc: 1.0000, loss_cls: 0.1901, loss: 0.1901, grad_norm: 1.7479\n",
      "2021-03-24 23:33:52,908 - mmaction - INFO - Saving checkpoint at 32 epochs\n",
      "2021-03-24 23:36:38,792 - mmaction - INFO - Epoch [33][25/44]\tlr: 1.317e-01, eta: 0:53:23, time: 6.629, data_time: 5.654, memory: 20204, top1_acc: 0.9033, top5_acc: 0.9983, loss_cls: 0.2275, loss: 0.2275, grad_norm: 1.9888\n",
      "2021-03-24 23:41:10,659 - mmaction - INFO - Epoch [34][25/44]\tlr: 1.219e-01, eta: 0:50:26, time: 6.672, data_time: 5.694, memory: 20204, top1_acc: 0.9117, top5_acc: 1.0000, loss_cls: 0.2229, loss: 0.2229, grad_norm: 1.9745\n",
      "2021-03-24 23:45:50,226 - mmaction - INFO - Epoch [35][25/44]\tlr: 1.118e-01, eta: 0:47:31, time: 6.810, data_time: 5.810, memory: 20204, top1_acc: 0.9350, top5_acc: 1.0000, loss_cls: 0.1698, loss: 0.1698, grad_norm: 1.7408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 126/126, 2.6 task/s, elapsed: 49s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-24 23:48:22,662 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-03-24 23:48:22,664 - mmaction - INFO - \n",
      "top1_acc\t0.8254\n",
      "top5_acc\t1.0000\n",
      "2021-03-24 23:48:22,665 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-03-24 23:48:22,667 - mmaction - INFO - \n",
      "mean_acc\t0.8254\n",
      "2021-03-24 23:48:22,668 - mmaction - INFO - Epoch(val) [35][44]\ttop1_acc: 0.8254, top5_acc: 1.0000, mean_class_accuracy: 0.8254\n",
      "2021-03-24 23:51:14,376 - mmaction - INFO - Epoch [36][25/44]\tlr: 1.015e-01, eta: 0:44:36, time: 6.868, data_time: 5.877, memory: 20204, top1_acc: 0.9167, top5_acc: 1.0000, loss_cls: 0.1841, loss: 0.1841, grad_norm: 2.0693\n",
      "2021-03-24 23:52:51,813 - mmaction - INFO - Saving checkpoint at 36 epochs\n",
      "2021-03-24 23:55:40,270 - mmaction - INFO - Epoch [37][25/44]\tlr: 9.127e-02, eta: 0:41:41, time: 6.732, data_time: 5.754, memory: 20204, top1_acc: 0.9533, top5_acc: 1.0000, loss_cls: 0.1247, loss: 0.1247, grad_norm: 1.3474\n",
      "2021-03-25 00:00:14,754 - mmaction - INFO - Epoch [38][25/44]\tlr: 8.111e-02, eta: 0:38:47, time: 6.818, data_time: 5.801, memory: 20204, top1_acc: 0.9250, top5_acc: 1.0000, loss_cls: 0.1797, loss: 0.1797, grad_norm: 2.0569\n",
      "2021-03-25 00:04:50,878 - mmaction - INFO - Epoch [39][25/44]\tlr: 7.115e-02, eta: 0:35:52, time: 6.759, data_time: 5.761, memory: 20204, top1_acc: 0.9433, top5_acc: 1.0000, loss_cls: 0.1380, loss: 0.1380, grad_norm: 1.4081\n",
      "2021-03-25 00:09:30,047 - mmaction - INFO - Epoch [40][25/44]\tlr: 6.149e-02, eta: 0:32:59, time: 6.947, data_time: 5.930, memory: 20204, top1_acc: 0.9483, top5_acc: 1.0000, loss_cls: 0.1448, loss: 0.1448, grad_norm: 1.5499\n",
      "2021-03-25 00:11:11,701 - mmaction - INFO - Saving checkpoint at 40 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 126/126, 2.6 task/s, elapsed: 48s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-25 00:11:59,894 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-03-25 00:11:59,896 - mmaction - INFO - \n",
      "top1_acc\t0.9206\n",
      "top5_acc\t1.0000\n",
      "2021-03-25 00:11:59,897 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-03-25 00:11:59,898 - mmaction - INFO - \n",
      "mean_acc\t0.9206\n",
      "2021-03-25 00:12:00,066 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_40.pth.\n",
      "2021-03-25 00:12:00,067 - mmaction - INFO - Best top1_acc is 0.9206 at 40 epoch.\n",
      "2021-03-25 00:12:00,068 - mmaction - INFO - Epoch(val) [40][44]\ttop1_acc: 0.9206, top5_acc: 1.0000, mean_class_accuracy: 0.9206\n",
      "2021-03-25 00:15:02,371 - mmaction - INFO - Epoch [41][25/44]\tlr: 5.224e-02, eta: 0:30:08, time: 7.292, data_time: 6.300, memory: 20204, top1_acc: 0.9417, top5_acc: 1.0000, loss_cls: 0.1471, loss: 0.1471, grad_norm: 1.4060\n",
      "2021-03-25 00:19:40,726 - mmaction - INFO - Epoch [42][25/44]\tlr: 4.349e-02, eta: 0:27:16, time: 7.081, data_time: 6.099, memory: 20204, top1_acc: 0.9417, top5_acc: 0.9983, loss_cls: 0.1504, loss: 0.1504, grad_norm: 1.7364\n",
      "2021-03-25 00:24:19,106 - mmaction - INFO - Epoch [43][25/44]\tlr: 3.534e-02, eta: 0:24:23, time: 7.103, data_time: 6.101, memory: 20204, top1_acc: 0.9533, top5_acc: 0.9983, loss_cls: 0.1330, loss: 0.1330, grad_norm: 1.5412\n",
      "2021-03-25 00:29:08,938 - mmaction - INFO - Epoch [44][25/44]\tlr: 2.786e-02, eta: 0:21:32, time: 7.522, data_time: 6.529, memory: 20204, top1_acc: 0.9633, top5_acc: 1.0000, loss_cls: 0.1166, loss: 0.1166, grad_norm: 1.6169\n",
      "2021-03-25 00:31:25,146 - mmaction - INFO - Saving checkpoint at 44 epochs\n",
      "2021-03-25 00:35:51,517 - mmaction - INFO - Epoch [45][25/44]\tlr: 2.115e-02, eta: 0:18:51, time: 10.648, data_time: 9.640, memory: 20204, top1_acc: 0.9550, top5_acc: 1.0000, loss_cls: 0.1182, loss: 0.1182, grad_norm: 1.4543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 126/126, 2.6 task/s, elapsed: 49s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-25 00:39:22,798 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-03-25 00:39:22,800 - mmaction - INFO - \n",
      "top1_acc\t0.9048\n",
      "top5_acc\t1.0000\n",
      "2021-03-25 00:39:22,800 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-03-25 00:39:22,802 - mmaction - INFO - \n",
      "mean_acc\t0.9048\n",
      "2021-03-25 00:39:22,803 - mmaction - INFO - Epoch(val) [45][44]\ttop1_acc: 0.9048, top5_acc: 1.0000, mean_class_accuracy: 0.9048\n",
      "2021-03-25 00:42:20,754 - mmaction - INFO - Epoch [46][25/44]\tlr: 1.526e-02, eta: 0:15:56, time: 7.118, data_time: 6.136, memory: 20204, top1_acc: 0.9550, top5_acc: 1.0000, loss_cls: 0.1165, loss: 0.1165, grad_norm: 1.5267\n",
      "2021-03-25 00:46:57,192 - mmaction - INFO - Epoch [47][25/44]\tlr: 1.027e-02, eta: 0:12:59, time: 7.019, data_time: 6.046, memory: 20204, top1_acc: 0.9633, top5_acc: 1.0000, loss_cls: 0.0944, loss: 0.0944, grad_norm: 1.2650\n",
      "2021-03-25 00:51:21,230 - mmaction - INFO - Epoch [48][25/44]\tlr: 6.220e-03, eta: 0:10:03, time: 6.794, data_time: 5.818, memory: 20204, top1_acc: 0.9600, top5_acc: 1.0000, loss_cls: 0.0976, loss: 0.0976, grad_norm: 1.2563\n",
      "2021-03-25 00:53:03,163 - mmaction - INFO - Saving checkpoint at 48 epochs\n",
      "2021-03-25 00:55:47,302 - mmaction - INFO - Epoch [49][25/44]\tlr: 3.158e-03, eta: 0:07:07, time: 6.559, data_time: 5.584, memory: 20204, top1_acc: 0.9550, top5_acc: 1.0000, loss_cls: 0.1207, loss: 0.1207, grad_norm: 1.5171\n",
      "2021-03-25 01:00:24,345 - mmaction - INFO - Epoch [50][25/44]\tlr: 1.114e-03, eta: 0:04:11, time: 6.680, data_time: 5.703, memory: 20204, top1_acc: 0.9500, top5_acc: 1.0000, loss_cls: 0.1192, loss: 0.1192, grad_norm: 1.5985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 126/126, 1.3 task/s, elapsed: 98s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-25 01:03:51,321 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-03-25 01:03:51,323 - mmaction - INFO - \n",
      "top1_acc\t0.9127\n",
      "top5_acc\t1.0000\n",
      "2021-03-25 01:03:51,324 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-03-25 01:03:51,326 - mmaction - INFO - \n",
      "mean_acc\t0.9127\n",
      "2021-03-25 01:03:51,327 - mmaction - INFO - Epoch(val) [50][44]\ttop1_acc: 0.9127, top5_acc: 1.0000, mean_class_accuracy: 0.9127\n",
      "2021-03-25 01:06:43,848 - mmaction - INFO - Epoch [51][25/44]\tlr: 1.108e-04, eta: 0:01:15, time: 6.901, data_time: 5.915, memory: 20204, top1_acc: 0.9583, top5_acc: 1.0000, loss_cls: 0.0970, loss: 0.0970, grad_norm: 1.2868\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "from mmaction.datasets import build_dataset\n",
    "from mmaction.models import build_model\n",
    "from mmaction.apis import train_model\n",
    "\n",
    "import mmcv\n",
    "\n",
    "# Build the dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# Build the recognizer\n",
    "model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.train_cfg, test_cfg=cfg.test_cfg)\n",
    "\n",
    "# Create work_dir\n",
    "mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
    "train_model(model, datasets, cfg, distributed=False, validate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f\"{cfg.work_dir}/model50e\", 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eyY3hCMwyTct",
    "outputId": "200e37c7-0da4-421f-98da-41418c3110ca",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 126/126, 1.1 task/s, elapsed: 120s, ETA:     0s\n",
      "Evaluating top_k_accuracy ...\n",
      "\n",
      "top1_acc\t0.9365\n",
      "top5_acc\t1.0000\n",
      "\n",
      "Evaluating mean_class_accuracy ...\n",
      "\n",
      "mean_acc\t0.9365\n",
      "top1_acc: 0.9365\n",
      "top5_acc: 1.0000\n",
      "mean_class_accuracy: 0.9365\n"
     ]
    }
   ],
   "source": [
    "from mmaction.apis import single_gpu_test\n",
    "from mmaction.datasets import build_dataloader\n",
    "from mmcv.parallel import MMDataParallel\n",
    "from mmaction.models import build_model\n",
    "from mmaction.datasets import build_dataset\n",
    "\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# Build a test dataloader\n",
    "dataset = build_dataset(cfg.data.test, dict(test_mode=True))\n",
    "data_loader = build_dataloader(\n",
    "        dataset,\n",
    "        videos_per_gpu=4,\n",
    "        workers_per_gpu=1,\n",
    "        dist=False,\n",
    "        shuffle=False)\n",
    "model = MMDataParallel(model, device_ids=[0])\n",
    "outputs = single_gpu_test(model, data_loader)\n",
    "\n",
    "eval_config = cfg.evaluation\n",
    "eval_config.pop('interval')\n",
    "eval_res = dataset.evaluate(outputs, **eval_config)\n",
    "for name, val in eval_res.items():\n",
    "    print(f'{name}: {val:.04f}')\n",
    "    \n",
    "output_config = cfg.output_config\n",
    "dataset.dump_results(outputs, **output_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from mmaction.core import confusion_matrix \n",
    "\n",
    "gt_labels = [ann['label'] for ann in dataset.load_annotations()]\n",
    "pred = np.argmax(outputs, axis=1)\n",
    "cf_mat = confusion_matrix(pred, gt_labels).astype(float)\n",
    "\n",
    "sns.heatmap(cf_mat, annot=True, xticklabels = ['box', 'clap', 'go', 'jog', 'run', 'walk', 'wave'], yticklabels = ['box', 'clap', 'go', 'jog', 'run', 'walk', 'wave'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "MMAction2 Tutorial.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64b5ec5a-f125-4f1d-a286-14a5d4235ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/robt427nv/childact\n"
     ]
    }
   ],
   "source": [
    "cd /home/robt427nv/childact/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4f06af-f33e-4e37-a67e-6d85f8f39088",
   "metadata": {},
   "source": [
    "# CSN gender BOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8af680b2-4c74-4630-b539-c1e1ffa39fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/robt427nv/childact\n"
     ]
    }
   ],
   "source": [
    "cd /home/robt427nv/childact/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3c84914-7eeb-4730-a8a1-50e9ee02354e",
   "metadata": {
    "id": "LjCcmCKOjktc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mmcv import Config\n",
    "cfg = Config.fromfile('mmaction2/configs/recognition/csn/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "097ca45c-f34b-45bb-b933-e0e36d5fbb45",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "tlhu9byjjt-K",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "13d1bb01-f351-48c0-9efb-0bdf2c125d29",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "model = dict(\n",
      "    type='Recognizer3D',\n",
      "    backbone=dict(\n",
      "        type='ResNet3dCSN',\n",
      "        pretrained2d=False,\n",
      "        pretrained=\n",
      "        'https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth',\n",
      "        depth=152,\n",
      "        with_pool2=False,\n",
      "        bottleneck_mode='ir',\n",
      "        norm_eval=True,\n",
      "        zero_init_residual=False,\n",
      "        bn_frozen=True),\n",
      "    cls_head=dict(\n",
      "        type='I3DHead',\n",
      "        num_classes=2,\n",
      "        in_channels=2048,\n",
      "        spatial_type='avg',\n",
      "        dropout_ratio=0.5,\n",
      "        init_std=0.01),\n",
      "    train_cfg=None,\n",
      "    test_cfg=dict(average_clips='prob'))\n",
      "checkpoint_config = dict(interval=20)\n",
      "log_config = dict(interval=100, hooks=[dict(type='TextLoggerHook')])\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = 'checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth'\n",
      "resume_from = None\n",
      "workflow = [('train', 1)]\n",
      "dataset_type = 'RawframeDataset'\n",
      "data_root = 'age-gender-3split-rgb-frames/'\n",
      "data_root_val = 'age-gender-3split-rgb-frames/val/'\n",
      "ann_file_train = 'age-gender-3split-rgb-frames/childact_train_rgb320_gender_box.txt'\n",
      "ann_file_val = 'age-gender-3split-rgb-frames/childact_val_rgb320_gender_box.txt'\n",
      "ann_file_test = 'age-gender-3split-rgb-frames/childact_test_rgb320_gender_box.txt'\n",
      "img_norm_cfg = dict(\n",
      "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_bgr=False)\n",
      "train_pipeline = [\n",
      "    dict(type='SampleFrames', clip_len=32, frame_interval=2, num_clips=1),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='RandomResizedCrop'),\n",
      "    dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "    dict(type='Flip', flip_ratio=0.5),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "]\n",
      "val_pipeline = [\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=32,\n",
      "        frame_interval=2,\n",
      "        num_clips=1,\n",
      "        test_mode=True),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='CenterCrop', crop_size=224),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=32,\n",
      "        frame_interval=2,\n",
      "        num_clips=10,\n",
      "        test_mode=True),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='ThreeCrop', crop_size=256),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "data = dict(\n",
      "    videos_per_gpu=6,\n",
      "    workers_per_gpu=4,\n",
      "    train=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_train_rgb320_gender_box.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/train/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=1),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='RandomResizedCrop'),\n",
      "            dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "            dict(type='Flip', flip_ratio=0.5),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'),\n",
      "    val=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_val_rgb320_gender_box.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/val/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=1,\n",
      "                test_mode=True),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='CenterCrop', crop_size=224),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'),\n",
      "    test=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_test_rgb320_gender_box.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/test/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=10,\n",
      "                test_mode=True),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='ThreeCrop', crop_size=256),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'))\n",
      "evaluation = dict(\n",
      "    interval=5, metrics=['top_k_accuracy', 'mean_class_accuracy'])\n",
      "optimizer = dict(type='SGD', lr=0.000125, momentum=0.9, weight_decay=0.0001)\n",
      "optimizer_config = dict(grad_clip=dict(max_norm=40, norm_type=2))\n",
      "lr_config = dict(\n",
      "    policy='step',\n",
      "    step=[32, 48],\n",
      "    warmup='linear',\n",
      "    warmup_ratio=0.1,\n",
      "    warmup_by_epoch=True,\n",
      "    warmup_iters=16)\n",
      "total_epochs = 50\n",
      "work_dir = './childact-checkpoints/CSN-gender-box'\n",
      "find_unused_parameters = True\n",
      "omnisource = False\n",
      "seed = 42\n",
      "gpu_ids = range(0, 1)\n",
      "output_config = dict(out='./childact-checkpoints/CSN-gender-box/results.json')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mmcv.runner import set_random_seed\n",
    "\n",
    "# Modify dataset type and path\n",
    "# cfg.dataset_type = 'VideoDataset'\n",
    "cfg.data_root = 'age-gender-3split-rgb-frames/'\n",
    "cfg.data_root_val = 'age-gender-3split-rgb-frames/val/'\n",
    "cfg.ann_file_train = 'age-gender-3split-rgb-frames/childact_train_rgb320_gender_box.txt'\n",
    "cfg.ann_file_val = 'age-gender-3split-rgb-frames/childact_val_rgb320_gender_box.txt'\n",
    "cfg.ann_file_test = 'age-gender-3split-rgb-frames/childact_test_rgb320_gender_box.txt'\n",
    "\n",
    "# cfg.data.test.type = 'VideoDataset'\n",
    "cfg.data.test.ann_file = 'age-gender-3split-rgb-frames/childact_test_rgb320_gender_box.txt'\n",
    "cfg.data.test.data_prefix = 'age-gender-3split-rgb-frames/test/'\n",
    "\n",
    "# cfg.data.train.type = 'VideoDataset'\n",
    "cfg.data.train.ann_file = 'age-gender-3split-rgb-frames/childact_train_rgb320_gender_box.txt'\n",
    "cfg.data.train.data_prefix = 'age-gender-3split-rgb-frames/train/'\n",
    "\n",
    "# cfg.data.val.type = 'VideoDataset'\n",
    "cfg.data.val.ann_file = 'age-gender-3split-rgb-frames/childact_val_rgb320_gender_box.txt'\n",
    "cfg.data.val.data_prefix = 'age-gender-3split-rgb-frames/val/'\n",
    "\n",
    "# cfg.data.test.modality = 'Flow'\n",
    "# cfg.data.val.modality = 'Flow'\n",
    "# cfg.data.train.modality = 'Flow'\n",
    "\n",
    "# cfg.data.train.start_index = 0\n",
    "# cfg.data.test.start_index = 0\n",
    "# cfg.data.val.start_index = 0\n",
    "\n",
    "cfg.data.test.filename_tmpl = '{:03}.jpeg'\n",
    "cfg.data.train.filename_tmpl = '{:03}.jpeg'\n",
    "cfg.data.val.filename_tmpl = '{:03}.jpeg'\n",
    "# The flag is used to determine whether it is omnisource training\n",
    "cfg.setdefault('omnisource', False)\n",
    "# Modify num classes of the model in cls_head\n",
    "cfg.model.cls_head.num_classes = 2\n",
    "# We can use the pre-trained TSN model\n",
    "cfg.load_from = 'checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth'\n",
    "# cfg.resume_from = './childact-mm/latest.pth'\n",
    "\n",
    "# Set up working dir to save files and logs.\n",
    "cfg.work_dir = './childact-checkpoints/CSN-gender-box'\n",
    "\n",
    "cfg.total_epochs = 50\n",
    "\n",
    "# cfg.momentum_config = dict(\n",
    "#     policy='cyclic',\n",
    "#     target_ratio=(0.85 / 0.95, 1),\n",
    "#     cyclic_times=1,\n",
    "#     step_ratio_up=0.4,\n",
    "# )\n",
    "\n",
    "# We can set the checkpoint saving interval to reduce the storage cost\n",
    "cfg.checkpoint_config.interval = 20\n",
    "# We can set the log print interval to reduce the the times of printing log\n",
    "cfg.log_config.interval = 100\n",
    "\n",
    "# Set seed thus the results are more reproducible\n",
    "cfg.seed = 42\n",
    "set_random_seed(42, deterministic=False)\n",
    "cfg.gpu_ids = range(1)\n",
    "\n",
    "cfg.data.videos_per_gpu=6\n",
    "\n",
    "# cfg.model.backbone.in_channels = 2\n",
    "\n",
    "cfg.output_config = dict(out=f'{cfg.work_dir}/results.json')\n",
    "# We can initialize the logger for training and have a look\n",
    "# at the final config used for training\n",
    "# del cfg.optimizer['momentum']\n",
    "print(f'Config:\\n{cfg.pretty_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ca8d7c9-7adf-4080-9134-201eeafd2791",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "dDBWkdDRk6oz",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "85a52ef3-7b5c-4c52-8fef-00322a8c65e6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-30 23:48:31,331 - mmaction - INFO - load model from: https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth\n",
      "2021-08-30 23:48:31,332 - mmaction - INFO - Use load_from_http loader\n",
      "2021-08-30 23:48:33,419 - mmaction - INFO - load checkpoint from checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth\n",
      "2021-08-30 23:48:33,420 - mmaction - INFO - Use load_from_local loader\n",
      "2021-08-30 23:48:33,580 - mmaction - WARNING - The model and loaded state dict do not match exactly\n",
      "\n",
      "size mismatch for cls_head.fc_cls.weight: copying a param with shape torch.Size([400, 2048]) from checkpoint, the shape in current model is torch.Size([2, 2048]).\n",
      "size mismatch for cls_head.fc_cls.bias: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([2]).\n",
      "2021-08-30 23:48:33,587 - mmaction - INFO - Start running, host: robt427nv@robt427NV, work_dir: /home/robt427nv/childact/childact-checkpoints/CSN-gender-box\n",
      "2021-08-30 23:48:33,588 - mmaction - INFO - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(ABOVE_NORMAL) OptimizerHook                      \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "2021-08-30 23:48:33,588 - mmaction - INFO - workflow: [('train', 1)], max: 50 epochs\n",
      "/home/robt427nv/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/mmcv/runner/hooks/evaluation.py:190: UserWarning: runner.meta is None. Creating an empty one.\n",
      "  warnings.warn('runner.meta is None. Creating an empty one.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 11/11, 8.5 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-30 23:50:23,008 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-30 23:50:23,011 - mmaction - INFO - \n",
      "top1_acc\t0.9091\n",
      "top5_acc\t1.0000\n",
      "2021-08-30 23:50:23,011 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-30 23:50:23,013 - mmaction - INFO - \n",
      "mean_acc\t0.9286\n",
      "2021-08-30 23:50:23,339 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_5.pth.\n",
      "2021-08-30 23:50:23,340 - mmaction - INFO - Best top1_acc is 0.9091 at 5 epoch.\n",
      "2021-08-30 23:50:23,341 - mmaction - INFO - Epoch(val) [5][2]\ttop1_acc: 0.9091, top5_acc: 1.0000, mean_class_accuracy: 0.9286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 11/11, 9.0 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-30 23:52:11,356 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-30 23:52:11,357 - mmaction - INFO - \n",
      "top1_acc\t0.9091\n",
      "top5_acc\t1.0000\n",
      "2021-08-30 23:52:11,358 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-30 23:52:11,358 - mmaction - INFO - \n",
      "mean_acc\t0.9286\n",
      "2021-08-30 23:52:11,359 - mmaction - INFO - Epoch(val) [10][2]\ttop1_acc: 0.9091, top5_acc: 1.0000, mean_class_accuracy: 0.9286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 11/11, 8.9 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-30 23:53:59,660 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-30 23:53:59,661 - mmaction - INFO - \n",
      "top1_acc\t0.8182\n",
      "top5_acc\t1.0000\n",
      "2021-08-30 23:53:59,662 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-30 23:53:59,663 - mmaction - INFO - \n",
      "mean_acc\t0.8036\n",
      "2021-08-30 23:53:59,664 - mmaction - INFO - Epoch(val) [15][2]\ttop1_acc: 0.8182, top5_acc: 1.0000, mean_class_accuracy: 0.8036\n",
      "2021-08-30 23:55:47,107 - mmaction - INFO - Saving checkpoint at 20 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 11/11, 8.6 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-30 23:55:48,790 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-30 23:55:48,792 - mmaction - INFO - \n",
      "top1_acc\t0.7273\n",
      "top5_acc\t1.0000\n",
      "2021-08-30 23:55:48,792 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-30 23:55:48,794 - mmaction - INFO - \n",
      "mean_acc\t0.6786\n",
      "2021-08-30 23:55:48,794 - mmaction - INFO - Epoch(val) [20][2]\ttop1_acc: 0.7273, top5_acc: 1.0000, mean_class_accuracy: 0.6786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 11/11, 9.0 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-30 23:57:37,899 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-30 23:57:37,901 - mmaction - INFO - \n",
      "top1_acc\t0.7273\n",
      "top5_acc\t1.0000\n",
      "2021-08-30 23:57:37,902 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-30 23:57:37,903 - mmaction - INFO - \n",
      "mean_acc\t0.6786\n",
      "2021-08-30 23:57:37,903 - mmaction - INFO - Epoch(val) [25][2]\ttop1_acc: 0.7273, top5_acc: 1.0000, mean_class_accuracy: 0.6786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 11/11, 8.8 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-30 23:59:26,839 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-30 23:59:26,841 - mmaction - INFO - \n",
      "top1_acc\t0.9091\n",
      "top5_acc\t1.0000\n",
      "2021-08-30 23:59:26,842 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-30 23:59:26,843 - mmaction - INFO - \n",
      "mean_acc\t0.9286\n",
      "2021-08-30 23:59:26,844 - mmaction - INFO - Epoch(val) [30][2]\ttop1_acc: 0.9091, top5_acc: 1.0000, mean_class_accuracy: 0.9286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 11/11, 9.1 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 00:01:15,401 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 00:01:15,402 - mmaction - INFO - \n",
      "top1_acc\t0.8182\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 00:01:15,403 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 00:01:15,404 - mmaction - INFO - \n",
      "mean_acc\t0.8036\n",
      "2021-08-31 00:01:15,404 - mmaction - INFO - Epoch(val) [35][2]\ttop1_acc: 0.8182, top5_acc: 1.0000, mean_class_accuracy: 0.8036\n",
      "2021-08-31 00:03:02,825 - mmaction - INFO - Saving checkpoint at 40 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 11/11, 8.7 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 00:03:04,503 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 00:03:04,506 - mmaction - INFO - \n",
      "top1_acc\t0.9091\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 00:03:04,507 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 00:03:04,509 - mmaction - INFO - \n",
      "mean_acc\t0.9286\n",
      "2021-08-31 00:03:04,510 - mmaction - INFO - Epoch(val) [40][2]\ttop1_acc: 0.9091, top5_acc: 1.0000, mean_class_accuracy: 0.9286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 11/11, 8.9 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 00:04:53,694 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 00:04:53,696 - mmaction - INFO - \n",
      "top1_acc\t0.9091\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 00:04:53,697 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 00:04:53,697 - mmaction - INFO - \n",
      "mean_acc\t0.9286\n",
      "2021-08-31 00:04:53,698 - mmaction - INFO - Epoch(val) [45][2]\ttop1_acc: 0.9091, top5_acc: 1.0000, mean_class_accuracy: 0.9286\n",
      "2021-08-31 00:06:41,217 - mmaction - INFO - Saving checkpoint at 50 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 11/11, 8.5 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 00:06:42,902 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 00:06:42,903 - mmaction - INFO - \n",
      "top1_acc\t0.9091\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 00:06:42,903 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 00:06:42,904 - mmaction - INFO - \n",
      "mean_acc\t0.9286\n",
      "2021-08-31 00:06:42,905 - mmaction - INFO - Epoch(val) [50][2]\ttop1_acc: 0.9091, top5_acc: 1.0000, mean_class_accuracy: 0.9286\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "from mmaction.datasets import build_dataset\n",
    "from mmaction.models import build_model\n",
    "from mmaction.apis import train_model\n",
    "\n",
    "import mmcv\n",
    "\n",
    "# Build the dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# Build the recognizer\n",
    "model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.train_cfg, test_cfg=cfg.test_cfg)\n",
    "# CUDA_LAUNCH_BLOCKING=1\n",
    "# Create work_dir\n",
    "mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
    "train_model(model, datasets, cfg, distributed=False, validate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28671ef7-0902-4c5c-b137-0261341d6b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f\"{cfg.work_dir}/model50e\", 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3776a3ca-9d38-4efa-8c6c-b20e907da4ae",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 16:54:18,016 - mmaction - INFO - load model from: https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth\n",
      "2021-08-10 16:54:18,017 - mmaction - INFO - Use load_from_http loader\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "from mmaction.datasets import build_dataset\n",
    "from mmaction.models import build_model\n",
    "from mmaction.apis import train_model\n",
    "import pickle\n",
    "import mmcv\n",
    "# Build the dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# Build the recognizer\n",
    "model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "\n",
    "model = pickle.load(open(f\"{cfg.work_dir}/model50e\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a14875a1-8a85-4af0-a54a-6d54cdeb7c9e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eyY3hCMwyTct",
    "outputId": "200e37c7-0da4-421f-98da-41418c3110ca",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 8/8, 0.5 task/s, elapsed: 15s, ETA:     0s\n",
      "Evaluating top_k_accuracy ...\n",
      "\n",
      "top1_acc\t1.0000\n",
      "top5_acc\t1.0000\n",
      "\n",
      "Evaluating mean_class_accuracy ...\n",
      "\n",
      "mean_acc\t1.0000\n",
      "top1_acc: 1.0000\n",
      "top5_acc: 1.0000\n",
      "mean_class_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from mmaction.apis import single_gpu_test\n",
    "from mmaction.datasets import build_dataloader\n",
    "from mmcv.parallel import MMDataParallel\n",
    "# from mmaction.models import build_model\n",
    "# from mmaction.datasets import build_dataset\n",
    "\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# Build a test dataloader\n",
    "dataset = build_dataset(cfg.data.test, dict(test_mode=True))\n",
    "data_loader = build_dataloader(\n",
    "        dataset,\n",
    "        videos_per_gpu=1,\n",
    "        workers_per_gpu=1,\n",
    "        dist=False,\n",
    "        shuffle=False)\n",
    "model = MMDataParallel(model, device_ids=[0])\n",
    "outputs = single_gpu_test(model, data_loader)\n",
    "\n",
    "eval_config = cfg.evaluation\n",
    "eval_config.pop('interval')\n",
    "eval_res = dataset.evaluate(outputs, **eval_config)\n",
    "for name, val in eval_res.items():\n",
    "    print(f'{name}: {val:.04f}')\n",
    "    \n",
    "output_config = cfg.output_config\n",
    "dataset.dump_results(outputs, **output_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0994ae1f-adaa-4118-a0c9-709514bda2a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from mmaction.core import confusion_matrix \n",
    "\n",
    "gt_labels = [ann['label'] for ann in dataset.load_annotations()]\n",
    "pred = np.argmax(outputs, axis=1)\n",
    "cf_mat = confusion_matrix(pred, gt_labels).astype(float)\n",
    "cmap = sns.cubehelix_palette(50, hue=0.05, rot=0, light=0.9, dark=0, as_cmap=True)\n",
    "# /np.sum(cf_mat), fmt='.2%',\n",
    "# sns.heatmap(cf_mat, cmap=cmap, annot=True, xticklabels = ['6yo', '7yo', '8yo'], yticklabels = ['6yo', '7yo', '8yo'])\n",
    "sns.heatmap(cf_mat, cmap=cmap, annot=True, xticklabels = ['male','female'], yticklabels = ['male','female'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00b8100-8cbd-46ea-86d4-1a4743c197d6",
   "metadata": {},
   "source": [
    "# CSN gender CLAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "766557c8-a1e4-4df6-896f-872b7c659fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/robt427nv/childact\n"
     ]
    }
   ],
   "source": [
    "cd /home/robt427nv/childact/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b15232f-8ad3-4b69-819f-e8bdd4c8c435",
   "metadata": {
    "id": "LjCcmCKOjktc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mmcv import Config\n",
    "cfg = Config.fromfile('mmaction2/configs/recognition/csn/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a73424d-0824-4d90-9aad-f86f6c8fb0fd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "tlhu9byjjt-K",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "13d1bb01-f351-48c0-9efb-0bdf2c125d29",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "model = dict(\n",
      "    type='Recognizer3D',\n",
      "    backbone=dict(\n",
      "        type='ResNet3dCSN',\n",
      "        pretrained2d=False,\n",
      "        pretrained=\n",
      "        'https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth',\n",
      "        depth=152,\n",
      "        with_pool2=False,\n",
      "        bottleneck_mode='ir',\n",
      "        norm_eval=True,\n",
      "        zero_init_residual=False,\n",
      "        bn_frozen=True),\n",
      "    cls_head=dict(\n",
      "        type='I3DHead',\n",
      "        num_classes=2,\n",
      "        in_channels=2048,\n",
      "        spatial_type='avg',\n",
      "        dropout_ratio=0.5,\n",
      "        init_std=0.01),\n",
      "    train_cfg=None,\n",
      "    test_cfg=dict(average_clips='prob'))\n",
      "checkpoint_config = dict(interval=20)\n",
      "log_config = dict(interval=100, hooks=[dict(type='TextLoggerHook')])\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = 'checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth'\n",
      "resume_from = None\n",
      "workflow = [('train', 1)]\n",
      "dataset_type = 'RawframeDataset'\n",
      "data_root = 'age-gender-3split-rgb-frames/'\n",
      "data_root_val = 'age-gender-3split-rgb-frames/val/'\n",
      "ann_file_train = 'age-gender-3split-rgb-frames/childact_train_rgb320_gender_clap.txt'\n",
      "ann_file_val = 'age-gender-3split-rgb-frames/childact_val_rgb320_gender_clap.txt'\n",
      "ann_file_test = 'age-gender-3split-rgb-frames/childact_test_rgb320_gender_clap.txt'\n",
      "img_norm_cfg = dict(\n",
      "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_bgr=False)\n",
      "train_pipeline = [\n",
      "    dict(type='SampleFrames', clip_len=32, frame_interval=2, num_clips=1),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='RandomResizedCrop'),\n",
      "    dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "    dict(type='Flip', flip_ratio=0.5),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "]\n",
      "val_pipeline = [\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=32,\n",
      "        frame_interval=2,\n",
      "        num_clips=1,\n",
      "        test_mode=True),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='CenterCrop', crop_size=224),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=32,\n",
      "        frame_interval=2,\n",
      "        num_clips=10,\n",
      "        test_mode=True),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='ThreeCrop', crop_size=256),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "data = dict(\n",
      "    videos_per_gpu=6,\n",
      "    workers_per_gpu=4,\n",
      "    train=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_train_rgb320_gender_clap.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/train/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=1),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='RandomResizedCrop'),\n",
      "            dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "            dict(type='Flip', flip_ratio=0.5),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'),\n",
      "    val=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_val_rgb320_gender_clap.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/val/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=1,\n",
      "                test_mode=True),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='CenterCrop', crop_size=224),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'),\n",
      "    test=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_test_rgb320_gender_clap.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/test/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=10,\n",
      "                test_mode=True),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='ThreeCrop', crop_size=256),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'))\n",
      "evaluation = dict(\n",
      "    interval=5, metrics=['top_k_accuracy', 'mean_class_accuracy'])\n",
      "optimizer = dict(type='SGD', lr=0.000125, momentum=0.9, weight_decay=0.0001)\n",
      "optimizer_config = dict(grad_clip=dict(max_norm=40, norm_type=2))\n",
      "lr_config = dict(\n",
      "    policy='step',\n",
      "    step=[32, 48],\n",
      "    warmup='linear',\n",
      "    warmup_ratio=0.1,\n",
      "    warmup_by_epoch=True,\n",
      "    warmup_iters=16)\n",
      "total_epochs = 50\n",
      "work_dir = './childact-checkpoints/CSN-gender-clap'\n",
      "find_unused_parameters = True\n",
      "omnisource = False\n",
      "seed = 42\n",
      "gpu_ids = range(0, 1)\n",
      "output_config = dict(out='./childact-checkpoints/CSN-gender-clap/results.json')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mmcv.runner import set_random_seed\n",
    "\n",
    "# Modify dataset type and path\n",
    "# cfg.dataset_type = 'VideoDataset'\n",
    "cfg.data_root = 'age-gender-3split-rgb-frames/'\n",
    "cfg.data_root_val = 'age-gender-3split-rgb-frames/val/'\n",
    "cfg.ann_file_train = 'age-gender-3split-rgb-frames/childact_train_rgb320_gender_clap.txt'\n",
    "cfg.ann_file_val = 'age-gender-3split-rgb-frames/childact_val_rgb320_gender_clap.txt'\n",
    "cfg.ann_file_test = 'age-gender-3split-rgb-frames/childact_test_rgb320_gender_clap.txt'\n",
    "\n",
    "# cfg.data.test.type = 'VideoDataset'\n",
    "cfg.data.test.ann_file = 'age-gender-3split-rgb-frames/childact_test_rgb320_gender_clap.txt'\n",
    "cfg.data.test.data_prefix = 'age-gender-3split-rgb-frames/test/'\n",
    "\n",
    "# cfg.data.train.type = 'VideoDataset'\n",
    "cfg.data.train.ann_file = 'age-gender-3split-rgb-frames/childact_train_rgb320_gender_clap.txt'\n",
    "cfg.data.train.data_prefix = 'age-gender-3split-rgb-frames/train/'\n",
    "\n",
    "# cfg.data.val.type = 'VideoDataset'\n",
    "cfg.data.val.ann_file = 'age-gender-3split-rgb-frames/childact_val_rgb320_gender_clap.txt'\n",
    "cfg.data.val.data_prefix = 'age-gender-3split-rgb-frames/val/'\n",
    "\n",
    "# cfg.data.test.modality = 'Flow'\n",
    "# cfg.data.val.modality = 'Flow'\n",
    "# cfg.data.train.modality = 'Flow'\n",
    "\n",
    "# cfg.data.train.start_index = 0\n",
    "# cfg.data.test.start_index = 0\n",
    "# cfg.data.val.start_index = 0\n",
    "\n",
    "cfg.data.test.filename_tmpl = '{:03}.jpeg'\n",
    "cfg.data.train.filename_tmpl = '{:03}.jpeg'\n",
    "cfg.data.val.filename_tmpl = '{:03}.jpeg'\n",
    "# The flag is used to determine whether it is omnisource training\n",
    "cfg.setdefault('omnisource', False)\n",
    "# Modify num classes of the model in cls_head\n",
    "cfg.model.cls_head.num_classes = 2\n",
    "# We can use the pre-trained TSN model\n",
    "cfg.load_from = 'checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth'\n",
    "# cfg.resume_from = './childact-mm/latest.pth'\n",
    "\n",
    "# Set up working dir to save files and logs.\n",
    "cfg.work_dir = './childact-checkpoints/CSN-gender-clap'\n",
    "\n",
    "cfg.total_epochs = 50\n",
    "\n",
    "# cfg.momentum_config = dict(\n",
    "#     policy='cyclic',\n",
    "#     target_ratio=(0.85 / 0.95, 1),\n",
    "#     cyclic_times=1,\n",
    "#     step_ratio_up=0.4,\n",
    "# )\n",
    "\n",
    "# We can set the checkpoint saving interval to reduce the storage cost\n",
    "cfg.checkpoint_config.interval = 20\n",
    "# We can set the log print interval to reduce the the times of printing log\n",
    "cfg.log_config.interval = 100\n",
    "\n",
    "# Set seed thus the results are more reproducible\n",
    "cfg.seed = 42\n",
    "set_random_seed(42, deterministic=False)\n",
    "cfg.gpu_ids = range(1)\n",
    "\n",
    "cfg.data.videos_per_gpu=6\n",
    "\n",
    "# cfg.model.backbone.in_channels = 2\n",
    "\n",
    "cfg.output_config = dict(out=f'{cfg.work_dir}/results.json')\n",
    "# We can initialize the logger for training and have a look\n",
    "# at the final config used for training\n",
    "# del cfg.optimizer['momentum']\n",
    "print(f'Config:\\n{cfg.pretty_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80b895d9-4649-4bb0-9b72-0e08f66f30f7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dDBWkdDRk6oz",
    "outputId": "85a52ef3-7b5c-4c52-8fef-00322a8c65e6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 00:11:25,322 - mmaction - INFO - load model from: https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth\n",
      "2021-08-31 00:11:25,323 - mmaction - INFO - Use load_from_http loader\n",
      "2021-08-31 00:11:27,490 - mmaction - INFO - load checkpoint from checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth\n",
      "2021-08-31 00:11:27,491 - mmaction - INFO - Use load_from_local loader\n",
      "2021-08-31 00:11:27,634 - mmaction - WARNING - The model and loaded state dict do not match exactly\n",
      "\n",
      "size mismatch for cls_head.fc_cls.weight: copying a param with shape torch.Size([400, 2048]) from checkpoint, the shape in current model is torch.Size([2, 2048]).\n",
      "size mismatch for cls_head.fc_cls.bias: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([2]).\n",
      "2021-08-31 00:11:27,640 - mmaction - INFO - Start running, host: robt427nv@robt427NV, work_dir: /home/robt427nv/childact/childact-checkpoints/CSN-gender-clap\n",
      "2021-08-31 00:11:27,641 - mmaction - INFO - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(ABOVE_NORMAL) OptimizerHook                      \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "2021-08-31 00:11:27,641 - mmaction - INFO - workflow: [('train', 1)], max: 50 epochs\n",
      "/home/robt427nv/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/mmcv/runner/hooks/evaluation.py:190: UserWarning: runner.meta is None. Creating an empty one.\n",
      "  warnings.warn('runner.meta is None. Creating an empty one.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 14/14, 9.5 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 00:13:39,626 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 00:13:39,627 - mmaction - INFO - \n",
      "top1_acc\t0.7857\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 00:13:39,628 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 00:13:39,629 - mmaction - INFO - \n",
      "mean_acc\t0.8125\n",
      "2021-08-31 00:13:39,942 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_5.pth.\n",
      "2021-08-31 00:13:39,943 - mmaction - INFO - Best top1_acc is 0.7857 at 5 epoch.\n",
      "2021-08-31 00:13:39,944 - mmaction - INFO - Epoch(val) [5][3]\ttop1_acc: 0.7857, top5_acc: 1.0000, mean_class_accuracy: 0.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 14/14, 10.1 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 00:15:51,861 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 00:15:51,862 - mmaction - INFO - \n",
      "top1_acc\t0.7857\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 00:15:51,863 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 00:15:51,864 - mmaction - INFO - \n",
      "mean_acc\t0.7917\n",
      "2021-08-31 00:15:51,865 - mmaction - INFO - Epoch(val) [10][3]\ttop1_acc: 0.7857, top5_acc: 1.0000, mean_class_accuracy: 0.7917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 14/14, 9.7 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 00:18:00,494 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 00:18:00,496 - mmaction - INFO - \n",
      "top1_acc\t0.7857\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 00:18:00,497 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 00:18:00,498 - mmaction - INFO - \n",
      "mean_acc\t0.7917\n",
      "2021-08-31 00:18:00,500 - mmaction - INFO - Epoch(val) [15][3]\ttop1_acc: 0.7857, top5_acc: 1.0000, mean_class_accuracy: 0.7917\n",
      "2021-08-31 00:20:07,685 - mmaction - INFO - Saving checkpoint at 20 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 14/14, 9.1 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 00:20:09,620 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 00:20:09,621 - mmaction - INFO - \n",
      "top1_acc\t0.7143\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 00:20:09,623 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 00:20:09,624 - mmaction - INFO - \n",
      "mean_acc\t0.7500\n",
      "2021-08-31 00:20:09,625 - mmaction - INFO - Epoch(val) [20][3]\ttop1_acc: 0.7143, top5_acc: 1.0000, mean_class_accuracy: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 14/14, 9.5 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 00:22:18,429 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 00:22:18,430 - mmaction - INFO - \n",
      "top1_acc\t0.7857\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 00:22:18,431 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 00:22:18,432 - mmaction - INFO - \n",
      "mean_acc\t0.8125\n",
      "2021-08-31 00:22:18,433 - mmaction - INFO - Epoch(val) [25][3]\ttop1_acc: 0.7857, top5_acc: 1.0000, mean_class_accuracy: 0.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 14/14, 10.0 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 00:24:27,050 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 00:24:27,051 - mmaction - INFO - \n",
      "top1_acc\t0.7857\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 00:24:27,052 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 00:24:27,052 - mmaction - INFO - \n",
      "mean_acc\t0.7917\n",
      "2021-08-31 00:24:27,053 - mmaction - INFO - Epoch(val) [30][3]\ttop1_acc: 0.7857, top5_acc: 1.0000, mean_class_accuracy: 0.7917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 14/14, 9.1 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 00:26:40,111 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 00:26:40,112 - mmaction - INFO - \n",
      "top1_acc\t0.7857\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 00:26:40,113 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 00:26:40,113 - mmaction - INFO - \n",
      "mean_acc\t0.7917\n",
      "2021-08-31 00:26:40,114 - mmaction - INFO - Epoch(val) [35][3]\ttop1_acc: 0.7857, top5_acc: 1.0000, mean_class_accuracy: 0.7917\n",
      "2021-08-31 00:28:54,932 - mmaction - INFO - Saving checkpoint at 40 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 14/14, 9.1 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 00:28:56,907 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 00:28:56,908 - mmaction - INFO - \n",
      "top1_acc\t0.7857\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 00:28:56,909 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 00:28:56,910 - mmaction - INFO - \n",
      "mean_acc\t0.7917\n",
      "2021-08-31 00:28:56,910 - mmaction - INFO - Epoch(val) [40][3]\ttop1_acc: 0.7857, top5_acc: 1.0000, mean_class_accuracy: 0.7917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 14/14, 9.6 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 00:31:12,015 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 00:31:12,017 - mmaction - INFO - \n",
      "top1_acc\t0.7143\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 00:31:12,017 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 00:31:12,019 - mmaction - INFO - \n",
      "mean_acc\t0.7292\n",
      "2021-08-31 00:31:12,019 - mmaction - INFO - Epoch(val) [45][3]\ttop1_acc: 0.7143, top5_acc: 1.0000, mean_class_accuracy: 0.7292\n",
      "2021-08-31 00:33:25,584 - mmaction - INFO - Saving checkpoint at 50 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 14/14, 9.4 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 00:33:27,508 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 00:33:27,509 - mmaction - INFO - \n",
      "top1_acc\t0.7857\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 00:33:27,510 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 00:33:27,510 - mmaction - INFO - \n",
      "mean_acc\t0.7917\n",
      "2021-08-31 00:33:27,511 - mmaction - INFO - Epoch(val) [50][3]\ttop1_acc: 0.7857, top5_acc: 1.0000, mean_class_accuracy: 0.7917\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "from mmaction.datasets import build_dataset\n",
    "from mmaction.models import build_model\n",
    "from mmaction.apis import train_model\n",
    "\n",
    "import mmcv\n",
    "\n",
    "# Build the dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# Build the recognizer\n",
    "model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.train_cfg, test_cfg=cfg.test_cfg)\n",
    "# CUDA_LAUNCH_BLOCKING=1\n",
    "# Create work_dir\n",
    "mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
    "train_model(model, datasets, cfg, distributed=False, validate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ced646b8-25d1-4c42-92b2-7cf01e98084b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f\"{cfg.work_dir}/model50e\", 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db180f14-773a-4743-8097-6f82f13d3758",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 16:54:18,016 - mmaction - INFO - load model from: https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth\n",
      "2021-08-10 16:54:18,017 - mmaction - INFO - Use load_from_http loader\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "from mmaction.datasets import build_dataset\n",
    "from mmaction.models import build_model\n",
    "from mmaction.apis import train_model\n",
    "import pickle\n",
    "import mmcv\n",
    "# Build the dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# Build the recognizer\n",
    "model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "\n",
    "model = pickle.load(open(f\"{cfg.work_dir}/model50e\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1601075-e8af-4c00-9dab-0dddab540448",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eyY3hCMwyTct",
    "outputId": "200e37c7-0da4-421f-98da-41418c3110ca",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 18/18, 0.3 task/s, elapsed: 70s, ETA:     0s\n",
      "Evaluating top_k_accuracy ...\n",
      "\n",
      "top1_acc\t0.9444\n",
      "top5_acc\t1.0000\n",
      "\n",
      "Evaluating mean_class_accuracy ...\n",
      "\n",
      "mean_acc\t0.9444\n",
      "top1_acc: 0.9444\n",
      "top5_acc: 1.0000\n",
      "mean_class_accuracy: 0.9444\n"
     ]
    }
   ],
   "source": [
    "from mmaction.apis import single_gpu_test\n",
    "from mmaction.datasets import build_dataloader\n",
    "from mmcv.parallel import MMDataParallel\n",
    "# from mmaction.models import build_model\n",
    "# from mmaction.datasets import build_dataset\n",
    "\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# Build a test dataloader\n",
    "dataset = build_dataset(cfg.data.test, dict(test_mode=True))\n",
    "data_loader = build_dataloader(\n",
    "        dataset,\n",
    "        videos_per_gpu=1,\n",
    "        workers_per_gpu=1,\n",
    "        dist=False,\n",
    "        shuffle=False)\n",
    "model = MMDataParallel(model, device_ids=[0])\n",
    "outputs = single_gpu_test(model, data_loader)\n",
    "\n",
    "eval_config = cfg.evaluation\n",
    "eval_config.pop('interval')\n",
    "eval_res = dataset.evaluate(outputs, **eval_config)\n",
    "for name, val in eval_res.items():\n",
    "    print(f'{name}: {val:.04f}')\n",
    "    \n",
    "output_config = cfg.output_config\n",
    "dataset.dump_results(outputs, **output_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cd251b4-b332-43e9-bc1d-12d847bb7a11",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from mmaction.core import confusion_matrix \n",
    "\n",
    "gt_labels = [ann['label'] for ann in dataset.load_annotations()]\n",
    "pred = np.argmax(outputs, axis=1)\n",
    "cf_mat = confusion_matrix(pred, gt_labels).astype(float)\n",
    "cmap = sns.cubehelix_palette(50, hue=0.05, rot=0, light=0.9, dark=0, as_cmap=True)\n",
    "# /np.sum(cf_mat), fmt='.2%',\n",
    "# sns.heatmap(cf_mat, cmap=cmap, annot=True, xticklabels = ['6yo', '7yo', '8yo'], yticklabels = ['6yo', '7yo', '8yo'])\n",
    "sns.heatmap(cf_mat, cmap=cmap, annot=True, xticklabels = ['male','female'], yticklabels = ['male','female'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7099c023-5e2a-4a8e-aa21-fe135c36a630",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79a865c-51bd-4e5a-8f62-2eb8ad59e64a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0ae1e7f-259f-462c-8228-cc57ca868cd7",
   "metadata": {},
   "source": [
    "# CSN gender Jog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9df612fa-685a-4d7e-9802-fb42ac4eb5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/robt427nv/childact\n"
     ]
    }
   ],
   "source": [
    "cd /home/robt427nv/childact/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5df7f334-3f69-443f-9a1b-e0818451f3ab",
   "metadata": {
    "id": "LjCcmCKOjktc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mmcv import Config\n",
    "cfg = Config.fromfile('mmaction2/configs/recognition/csn/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "217ffb92-ae3c-4fc8-8efc-abb79a98773a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "tlhu9byjjt-K",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "13d1bb01-f351-48c0-9efb-0bdf2c125d29",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "model = dict(\n",
      "    type='Recognizer3D',\n",
      "    backbone=dict(\n",
      "        type='ResNet3dCSN',\n",
      "        pretrained2d=False,\n",
      "        pretrained=\n",
      "        'https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth',\n",
      "        depth=152,\n",
      "        with_pool2=False,\n",
      "        bottleneck_mode='ir',\n",
      "        norm_eval=True,\n",
      "        zero_init_residual=False,\n",
      "        bn_frozen=True),\n",
      "    cls_head=dict(\n",
      "        type='I3DHead',\n",
      "        num_classes=2,\n",
      "        in_channels=2048,\n",
      "        spatial_type='avg',\n",
      "        dropout_ratio=0.5,\n",
      "        init_std=0.01),\n",
      "    train_cfg=None,\n",
      "    test_cfg=dict(average_clips='prob'))\n",
      "checkpoint_config = dict(interval=20)\n",
      "log_config = dict(interval=100, hooks=[dict(type='TextLoggerHook')])\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = 'checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth'\n",
      "resume_from = None\n",
      "workflow = [('train', 1)]\n",
      "dataset_type = 'RawframeDataset'\n",
      "data_root = 'age-gender-3split-rgb-frames/'\n",
      "data_root_val = 'age-gender-3split-rgb-frames/val/'\n",
      "ann_file_train = 'age-gender-3split-rgb-frames/childact_train_rgb320_gender_jog.txt'\n",
      "ann_file_val = 'age-gender-3split-rgb-frames/childact_val_rgb320_gender_jog.txt'\n",
      "ann_file_test = 'age-gender-3split-rgb-frames/childact_test_rgb320_gender_jog.txt'\n",
      "img_norm_cfg = dict(\n",
      "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_bgr=False)\n",
      "train_pipeline = [\n",
      "    dict(type='SampleFrames', clip_len=32, frame_interval=2, num_clips=1),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='RandomResizedCrop'),\n",
      "    dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "    dict(type='Flip', flip_ratio=0.5),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "]\n",
      "val_pipeline = [\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=32,\n",
      "        frame_interval=2,\n",
      "        num_clips=1,\n",
      "        test_mode=True),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='CenterCrop', crop_size=224),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=32,\n",
      "        frame_interval=2,\n",
      "        num_clips=10,\n",
      "        test_mode=True),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='ThreeCrop', crop_size=256),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "data = dict(\n",
      "    videos_per_gpu=6,\n",
      "    workers_per_gpu=4,\n",
      "    train=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_train_rgb320_gender_jog.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/train/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=1),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='RandomResizedCrop'),\n",
      "            dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "            dict(type='Flip', flip_ratio=0.5),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'),\n",
      "    val=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_val_rgb320_gender_jog.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/val/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=1,\n",
      "                test_mode=True),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='CenterCrop', crop_size=224),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'),\n",
      "    test=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_test_rgb320_gender_jog.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/test/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=10,\n",
      "                test_mode=True),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='ThreeCrop', crop_size=256),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'))\n",
      "evaluation = dict(\n",
      "    interval=5, metrics=['top_k_accuracy', 'mean_class_accuracy'])\n",
      "optimizer = dict(type='SGD', lr=0.000125, momentum=0.9, weight_decay=0.0001)\n",
      "optimizer_config = dict(grad_clip=dict(max_norm=40, norm_type=2))\n",
      "lr_config = dict(\n",
      "    policy='step',\n",
      "    step=[32, 48],\n",
      "    warmup='linear',\n",
      "    warmup_ratio=0.1,\n",
      "    warmup_by_epoch=True,\n",
      "    warmup_iters=16)\n",
      "total_epochs = 50\n",
      "work_dir = './childact-checkpoints/CSN-gender-jog'\n",
      "find_unused_parameters = True\n",
      "omnisource = False\n",
      "seed = 42\n",
      "gpu_ids = range(0, 1)\n",
      "output_config = dict(out='./childact-checkpoints/CSN-gender-jog/results.json')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mmcv.runner import set_random_seed\n",
    "\n",
    "# Modify dataset type and path\n",
    "# cfg.dataset_type = 'VideoDataset'\n",
    "cfg.data_root = 'age-gender-3split-rgb-frames/'\n",
    "cfg.data_root_val = 'age-gender-3split-rgb-frames/val/'\n",
    "cfg.ann_file_train = 'age-gender-3split-rgb-frames/childact_train_rgb320_gender_jog.txt'\n",
    "cfg.ann_file_val = 'age-gender-3split-rgb-frames/childact_val_rgb320_gender_jog.txt'\n",
    "cfg.ann_file_test = 'age-gender-3split-rgb-frames/childact_test_rgb320_gender_jog.txt'\n",
    "\n",
    "# cfg.data.test.type = 'VideoDataset'\n",
    "cfg.data.test.ann_file = 'age-gender-3split-rgb-frames/childact_test_rgb320_gender_jog.txt'\n",
    "cfg.data.test.data_prefix = 'age-gender-3split-rgb-frames/test/'\n",
    "\n",
    "# cfg.data.train.type = 'VideoDataset'\n",
    "cfg.data.train.ann_file = 'age-gender-3split-rgb-frames/childact_train_rgb320_gender_jog.txt'\n",
    "cfg.data.train.data_prefix = 'age-gender-3split-rgb-frames/train/'\n",
    "\n",
    "# cfg.data.val.type = 'VideoDataset'\n",
    "cfg.data.val.ann_file = 'age-gender-3split-rgb-frames/childact_val_rgb320_gender_jog.txt'\n",
    "cfg.data.val.data_prefix = 'age-gender-3split-rgb-frames/val/'\n",
    "\n",
    "# cfg.data.test.modality = 'Flow'\n",
    "# cfg.data.val.modality = 'Flow'\n",
    "# cfg.data.train.modality = 'Flow'\n",
    "\n",
    "# cfg.data.train.start_index = 0\n",
    "# cfg.data.test.start_index = 0\n",
    "# cfg.data.val.start_index = 0\n",
    "\n",
    "cfg.data.test.filename_tmpl = '{:03}.jpeg'\n",
    "cfg.data.train.filename_tmpl = '{:03}.jpeg'\n",
    "cfg.data.val.filename_tmpl = '{:03}.jpeg'\n",
    "# The flag is used to determine whether it is omnisource training\n",
    "cfg.setdefault('omnisource', False)\n",
    "# Modify num classes of the model in cls_head\n",
    "cfg.model.cls_head.num_classes = 2\n",
    "# We can use the pre-trained TSN model\n",
    "cfg.load_from = 'checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth'\n",
    "# cfg.resume_from = './childact-mm/latest.pth'\n",
    "\n",
    "# Set up working dir to save files and logs.\n",
    "cfg.work_dir = './childact-checkpoints/CSN-gender-jog'\n",
    "\n",
    "cfg.total_epochs = 50\n",
    "\n",
    "# cfg.momentum_config = dict(\n",
    "#     policy='cyclic',\n",
    "#     target_ratio=(0.85 / 0.95, 1),\n",
    "#     cyclic_times=1,\n",
    "#     step_ratio_up=0.4,\n",
    "# )\n",
    "\n",
    "# We can set the checkpoint saving interval to reduce the storage cost\n",
    "cfg.checkpoint_config.interval = 20\n",
    "# We can set the log print interval to reduce the the times of printing log\n",
    "cfg.log_config.interval = 100\n",
    "\n",
    "# Set seed thus the results are more reproducible\n",
    "cfg.seed = 42\n",
    "set_random_seed(42, deterministic=False)\n",
    "cfg.gpu_ids = range(1)\n",
    "\n",
    "cfg.data.videos_per_gpu=6\n",
    "\n",
    "# cfg.model.backbone.in_channels = 2\n",
    "\n",
    "cfg.output_config = dict(out=f'{cfg.work_dir}/results.json')\n",
    "# We can initialize the logger for training and have a look\n",
    "# at the final config used for training\n",
    "# del cfg.optimizer['momentum']\n",
    "print(f'Config:\\n{cfg.pretty_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db1a69ff-5133-4344-bbba-b59dfa4742d6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dDBWkdDRk6oz",
    "outputId": "85a52ef3-7b5c-4c52-8fef-00322a8c65e6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 00:37:44,948 - mmaction - INFO - load model from: https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth\n",
      "2021-08-31 00:37:44,949 - mmaction - INFO - Use load_from_http loader\n",
      "2021-08-31 00:37:47,030 - mmaction - INFO - load checkpoint from checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth\n",
      "2021-08-31 00:37:47,031 - mmaction - INFO - Use load_from_local loader\n",
      "2021-08-31 00:37:47,191 - mmaction - WARNING - The model and loaded state dict do not match exactly\n",
      "\n",
      "size mismatch for cls_head.fc_cls.weight: copying a param with shape torch.Size([400, 2048]) from checkpoint, the shape in current model is torch.Size([2, 2048]).\n",
      "size mismatch for cls_head.fc_cls.bias: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([2]).\n",
      "2021-08-31 00:37:47,197 - mmaction - INFO - Start running, host: robt427nv@robt427NV, work_dir: /home/robt427nv/childact/childact-checkpoints/CSN-gender-jog\n",
      "2021-08-31 00:37:47,198 - mmaction - INFO - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(ABOVE_NORMAL) OptimizerHook                      \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "2021-08-31 00:37:47,199 - mmaction - INFO - workflow: [('train', 1)], max: 50 epochs\n",
      "/home/robt427nv/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/mmcv/runner/hooks/evaluation.py:190: UserWarning: runner.meta is None. Creating an empty one.\n",
      "  warnings.warn('runner.meta is None. Creating an empty one.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 15/15, 10.2 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 00:40:03,100 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 00:40:03,102 - mmaction - INFO - \n",
      "top1_acc\t0.8000\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 00:40:03,103 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 00:40:03,104 - mmaction - INFO - \n",
      "mean_acc\t0.7857\n",
      "2021-08-31 00:40:03,466 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_5.pth.\n",
      "2021-08-31 00:40:03,467 - mmaction - INFO - Best top1_acc is 0.8000 at 5 epoch.\n",
      "2021-08-31 00:40:03,468 - mmaction - INFO - Epoch(val) [5][3]\ttop1_acc: 0.8000, top5_acc: 1.0000, mean_class_accuracy: 0.7857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 15/15, 9.5 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 00:42:19,194 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 00:42:19,196 - mmaction - INFO - \n",
      "top1_acc\t0.8000\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 00:42:19,196 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 00:42:19,198 - mmaction - INFO - \n",
      "mean_acc\t0.7857\n",
      "2021-08-31 00:42:19,198 - mmaction - INFO - Epoch(val) [10][3]\ttop1_acc: 0.8000, top5_acc: 1.0000, mean_class_accuracy: 0.7857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 15/15, 9.5 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 00:44:33,102 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 00:44:33,104 - mmaction - INFO - \n",
      "top1_acc\t0.8000\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 00:44:33,105 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 00:44:33,105 - mmaction - INFO - \n",
      "mean_acc\t0.7857\n",
      "2021-08-31 00:44:33,106 - mmaction - INFO - Epoch(val) [15][3]\ttop1_acc: 0.8000, top5_acc: 1.0000, mean_class_accuracy: 0.7857\n",
      "2021-08-31 00:46:45,438 - mmaction - INFO - Saving checkpoint at 20 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 15/15, 10.0 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 00:46:47,317 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 00:46:47,319 - mmaction - INFO - \n",
      "top1_acc\t0.8667\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 00:46:47,319 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 00:46:47,320 - mmaction - INFO - \n",
      "mean_acc\t0.8571\n",
      "2021-08-31 00:46:47,636 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_20.pth.\n",
      "2021-08-31 00:46:47,637 - mmaction - INFO - Best top1_acc is 0.8667 at 20 epoch.\n",
      "2021-08-31 00:46:47,638 - mmaction - INFO - Epoch(val) [20][3]\ttop1_acc: 0.8667, top5_acc: 1.0000, mean_class_accuracy: 0.8571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 15/15, 9.7 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 00:49:01,364 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 00:49:01,365 - mmaction - INFO - \n",
      "top1_acc\t0.8000\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 00:49:01,366 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 00:49:01,367 - mmaction - INFO - \n",
      "mean_acc\t0.7857\n",
      "2021-08-31 00:49:01,367 - mmaction - INFO - Epoch(val) [25][3]\ttop1_acc: 0.8000, top5_acc: 1.0000, mean_class_accuracy: 0.7857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 15/15, 9.6 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 00:51:15,294 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 00:51:15,295 - mmaction - INFO - \n",
      "top1_acc\t0.8000\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 00:51:15,296 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 00:51:15,297 - mmaction - INFO - \n",
      "mean_acc\t0.7857\n",
      "2021-08-31 00:51:15,297 - mmaction - INFO - Epoch(val) [30][3]\ttop1_acc: 0.8000, top5_acc: 1.0000, mean_class_accuracy: 0.7857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 15/15, 9.7 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 00:53:29,673 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 00:53:29,675 - mmaction - INFO - \n",
      "top1_acc\t0.8000\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 00:53:29,676 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 00:53:29,678 - mmaction - INFO - \n",
      "mean_acc\t0.7857\n",
      "2021-08-31 00:53:29,679 - mmaction - INFO - Epoch(val) [35][3]\ttop1_acc: 0.8000, top5_acc: 1.0000, mean_class_accuracy: 0.7857\n",
      "2021-08-31 00:55:42,021 - mmaction - INFO - Saving checkpoint at 40 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 15/15, 9.8 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 00:55:43,923 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 00:55:43,924 - mmaction - INFO - \n",
      "top1_acc\t0.8000\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 00:55:43,924 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 00:55:43,925 - mmaction - INFO - \n",
      "mean_acc\t0.7857\n",
      "2021-08-31 00:55:43,925 - mmaction - INFO - Epoch(val) [40][3]\ttop1_acc: 0.8000, top5_acc: 1.0000, mean_class_accuracy: 0.7857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 15/15, 9.9 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 00:57:57,461 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 00:57:57,462 - mmaction - INFO - \n",
      "top1_acc\t0.8000\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 00:57:57,463 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 00:57:57,464 - mmaction - INFO - \n",
      "mean_acc\t0.7857\n",
      "2021-08-31 00:57:57,464 - mmaction - INFO - Epoch(val) [45][3]\ttop1_acc: 0.8000, top5_acc: 1.0000, mean_class_accuracy: 0.7857\n",
      "2021-08-31 01:00:09,849 - mmaction - INFO - Saving checkpoint at 50 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 15/15, 9.7 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 01:00:11,825 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 01:00:11,826 - mmaction - INFO - \n",
      "top1_acc\t0.8000\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 01:00:11,827 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 01:00:11,827 - mmaction - INFO - \n",
      "mean_acc\t0.7857\n",
      "2021-08-31 01:00:11,828 - mmaction - INFO - Epoch(val) [50][3]\ttop1_acc: 0.8000, top5_acc: 1.0000, mean_class_accuracy: 0.7857\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "from mmaction.datasets import build_dataset\n",
    "from mmaction.models import build_model\n",
    "from mmaction.apis import train_model\n",
    "\n",
    "import mmcv\n",
    "\n",
    "# Build the dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# Build the recognizer\n",
    "model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.train_cfg, test_cfg=cfg.test_cfg)\n",
    "# CUDA_LAUNCH_BLOCKING=1\n",
    "# Create work_dir\n",
    "mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
    "train_model(model, datasets, cfg, distributed=False, validate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f45f441e-714d-4272-9c1c-b5f9a95ad78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f\"{cfg.work_dir}/model50e\", 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc9098ab-0438-4050-a492-0fcbb3880b8b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 16:54:18,016 - mmaction - INFO - load model from: https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth\n",
      "2021-08-10 16:54:18,017 - mmaction - INFO - Use load_from_http loader\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "from mmaction.datasets import build_dataset\n",
    "from mmaction.models import build_model\n",
    "from mmaction.apis import train_model\n",
    "import pickle\n",
    "import mmcv\n",
    "# Build the dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# Build the recognizer\n",
    "model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "\n",
    "model = pickle.load(open(f\"{cfg.work_dir}/model50e\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b996a127-e0ce-4df9-8baa-2afa75969704",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eyY3hCMwyTct",
    "outputId": "200e37c7-0da4-421f-98da-41418c3110ca",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 10/10, 0.6 task/s, elapsed: 17s, ETA:     0s\n",
      "Evaluating top_k_accuracy ...\n",
      "\n",
      "top1_acc\t0.9000\n",
      "top5_acc\t1.0000\n",
      "\n",
      "Evaluating mean_class_accuracy ...\n",
      "\n",
      "mean_acc\t0.9167\n",
      "top1_acc: 0.9000\n",
      "top5_acc: 1.0000\n",
      "mean_class_accuracy: 0.9167\n"
     ]
    }
   ],
   "source": [
    "from mmaction.apis import single_gpu_test\n",
    "from mmaction.datasets import build_dataloader\n",
    "from mmcv.parallel import MMDataParallel\n",
    "# from mmaction.models import build_model\n",
    "# from mmaction.datasets import build_dataset\n",
    "\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# Build a test dataloader\n",
    "dataset = build_dataset(cfg.data.test, dict(test_mode=True))\n",
    "data_loader = build_dataloader(\n",
    "        dataset,\n",
    "        videos_per_gpu=1,\n",
    "        workers_per_gpu=1,\n",
    "        dist=False,\n",
    "        shuffle=False)\n",
    "model = MMDataParallel(model, device_ids=[0])\n",
    "outputs = single_gpu_test(model, data_loader)\n",
    "\n",
    "eval_config = cfg.evaluation\n",
    "eval_config.pop('interval')\n",
    "eval_res = dataset.evaluate(outputs, **eval_config)\n",
    "for name, val in eval_res.items():\n",
    "    print(f'{name}: {val:.04f}')\n",
    "    \n",
    "output_config = cfg.output_config\n",
    "dataset.dump_results(outputs, **output_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dedb1e9f-7467-48da-9481-0342b24a64f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from mmaction.core import confusion_matrix \n",
    "\n",
    "gt_labels = [ann['label'] for ann in dataset.load_annotations()]\n",
    "pred = np.argmax(outputs, axis=1)\n",
    "cf_mat = confusion_matrix(pred, gt_labels).astype(float)\n",
    "cmap = sns.cubehelix_palette(50, hue=0.05, rot=0, light=0.9, dark=0, as_cmap=True)\n",
    "# /np.sum(cf_mat), fmt='.2%',\n",
    "# sns.heatmap(cf_mat, cmap=cmap, annot=True, xticklabels = ['6yo', '7yo', '8yo'], yticklabels = ['6yo', '7yo', '8yo'])\n",
    "sns.heatmap(cf_mat, cmap=cmap, annot=True, xticklabels = ['male','female'], yticklabels = ['male','female'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fd140c-95d6-4c7f-97bb-ff72dc87ff1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c954996c-5871-494d-ac32-e4c25821c08b",
   "metadata": {},
   "source": [
    "# CSN gender GO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dfd6e51-9364-4379-994e-9a4b603f145a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/robt427nv/childact\n"
     ]
    }
   ],
   "source": [
    "cd /home/robt427nv/childact/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30910b32-d8fe-4e9b-b9f9-e3a55f13e4c5",
   "metadata": {
    "id": "LjCcmCKOjktc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mmcv import Config\n",
    "cfg = Config.fromfile('mmaction2/configs/recognition/csn/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6868c3aa-d159-4dba-b81b-21a166fa208b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "tlhu9byjjt-K",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "13d1bb01-f351-48c0-9efb-0bdf2c125d29",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "model = dict(\n",
      "    type='Recognizer3D',\n",
      "    backbone=dict(\n",
      "        type='ResNet3dCSN',\n",
      "        pretrained2d=False,\n",
      "        pretrained=\n",
      "        'https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth',\n",
      "        depth=152,\n",
      "        with_pool2=False,\n",
      "        bottleneck_mode='ir',\n",
      "        norm_eval=True,\n",
      "        zero_init_residual=False,\n",
      "        bn_frozen=True),\n",
      "    cls_head=dict(\n",
      "        type='I3DHead',\n",
      "        num_classes=2,\n",
      "        in_channels=2048,\n",
      "        spatial_type='avg',\n",
      "        dropout_ratio=0.5,\n",
      "        init_std=0.01),\n",
      "    train_cfg=None,\n",
      "    test_cfg=dict(average_clips='prob'))\n",
      "checkpoint_config = dict(interval=20)\n",
      "log_config = dict(interval=100, hooks=[dict(type='TextLoggerHook')])\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = 'checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth'\n",
      "resume_from = None\n",
      "workflow = [('train', 1)]\n",
      "dataset_type = 'RawframeDataset'\n",
      "data_root = 'age-gender-3split-rgb-frames/'\n",
      "data_root_val = 'age-gender-3split-rgb-frames/val/'\n",
      "ann_file_train = 'age-gender-3split-rgb-frames/childact_train_rgb320_gender_go.txt'\n",
      "ann_file_val = 'age-gender-3split-rgb-frames/childact_val_rgb320_gender_go.txt'\n",
      "ann_file_test = 'age-gender-3split-rgb-frames/childact_test_rgb320_gender_go.txt'\n",
      "img_norm_cfg = dict(\n",
      "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_bgr=False)\n",
      "train_pipeline = [\n",
      "    dict(type='SampleFrames', clip_len=32, frame_interval=2, num_clips=1),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='RandomResizedCrop'),\n",
      "    dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "    dict(type='Flip', flip_ratio=0.5),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "]\n",
      "val_pipeline = [\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=32,\n",
      "        frame_interval=2,\n",
      "        num_clips=1,\n",
      "        test_mode=True),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='CenterCrop', crop_size=224),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=32,\n",
      "        frame_interval=2,\n",
      "        num_clips=10,\n",
      "        test_mode=True),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='ThreeCrop', crop_size=256),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "data = dict(\n",
      "    videos_per_gpu=6,\n",
      "    workers_per_gpu=4,\n",
      "    train=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_train_rgb320_gender_go.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/train/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=1),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='RandomResizedCrop'),\n",
      "            dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "            dict(type='Flip', flip_ratio=0.5),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'),\n",
      "    val=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_val_rgb320_gender_go.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/val/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=1,\n",
      "                test_mode=True),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='CenterCrop', crop_size=224),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'),\n",
      "    test=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_test_rgb320_gender_go.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/test/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=10,\n",
      "                test_mode=True),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='ThreeCrop', crop_size=256),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'))\n",
      "evaluation = dict(\n",
      "    interval=5, metrics=['top_k_accuracy', 'mean_class_accuracy'])\n",
      "optimizer = dict(type='SGD', lr=0.000125, momentum=0.9, weight_decay=0.0001)\n",
      "optimizer_config = dict(grad_clip=dict(max_norm=40, norm_type=2))\n",
      "lr_config = dict(\n",
      "    policy='step',\n",
      "    step=[32, 48],\n",
      "    warmup='linear',\n",
      "    warmup_ratio=0.1,\n",
      "    warmup_by_epoch=True,\n",
      "    warmup_iters=16)\n",
      "total_epochs = 50\n",
      "work_dir = './childact-checkpoints/CSN-gender-go'\n",
      "find_unused_parameters = True\n",
      "omnisource = False\n",
      "seed = 42\n",
      "gpu_ids = range(0, 1)\n",
      "output_config = dict(out='./childact-checkpoints/CSN-gender-go/results.json')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mmcv.runner import set_random_seed\n",
    "\n",
    "# Modify dataset type and path\n",
    "# cfg.dataset_type = 'VideoDataset'\n",
    "cfg.data_root = 'age-gender-3split-rgb-frames/'\n",
    "cfg.data_root_val = 'age-gender-3split-rgb-frames/val/'\n",
    "cfg.ann_file_train = 'age-gender-3split-rgb-frames/childact_train_rgb320_gender_go.txt'\n",
    "cfg.ann_file_val = 'age-gender-3split-rgb-frames/childact_val_rgb320_gender_go.txt'\n",
    "cfg.ann_file_test = 'age-gender-3split-rgb-frames/childact_test_rgb320_gender_go.txt'\n",
    "\n",
    "# cfg.data.test.type = 'VideoDataset'\n",
    "cfg.data.test.ann_file = 'age-gender-3split-rgb-frames/childact_test_rgb320_gender_go.txt'\n",
    "cfg.data.test.data_prefix = 'age-gender-3split-rgb-frames/test/'\n",
    "\n",
    "# cfg.data.train.type = 'VideoDataset'\n",
    "cfg.data.train.ann_file = 'age-gender-3split-rgb-frames/childact_train_rgb320_gender_go.txt'\n",
    "cfg.data.train.data_prefix = 'age-gender-3split-rgb-frames/train/'\n",
    "\n",
    "# cfg.data.val.type = 'VideoDataset'\n",
    "cfg.data.val.ann_file = 'age-gender-3split-rgb-frames/childact_val_rgb320_gender_go.txt'\n",
    "cfg.data.val.data_prefix = 'age-gender-3split-rgb-frames/val/'\n",
    "\n",
    "# cfg.data.test.modality = 'Flow'\n",
    "# cfg.data.val.modality = 'Flow'\n",
    "# cfg.data.train.modality = 'Flow'\n",
    "\n",
    "# cfg.data.train.start_index = 0\n",
    "# cfg.data.test.start_index = 0\n",
    "# cfg.data.val.start_index = 0\n",
    "\n",
    "cfg.data.test.filename_tmpl = '{:03}.jpeg'\n",
    "cfg.data.train.filename_tmpl = '{:03}.jpeg'\n",
    "cfg.data.val.filename_tmpl = '{:03}.jpeg'\n",
    "# The flag is used to determine whether it is omnisource training\n",
    "cfg.setdefault('omnisource', False)\n",
    "# Modify num classes of the model in cls_head\n",
    "cfg.model.cls_head.num_classes = 2\n",
    "# We can use the pre-trained TSN model\n",
    "cfg.load_from = 'checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth'\n",
    "# cfg.resume_from = './childact-mm/latest.pth'\n",
    "\n",
    "# Set up working dir to save files and logs.\n",
    "cfg.work_dir = './childact-checkpoints/CSN-gender-go'\n",
    "\n",
    "cfg.total_epochs = 50\n",
    "\n",
    "# cfg.momentum_config = dict(\n",
    "#     policy='cyclic',\n",
    "#     target_ratio=(0.85 / 0.95, 1),\n",
    "#     cyclic_times=1,\n",
    "#     step_ratio_up=0.4,\n",
    "# )\n",
    "\n",
    "# We can set the checkpoint saving interval to reduce the storage cost\n",
    "cfg.checkpoint_config.interval = 20\n",
    "# We can set the log print interval to reduce the the times of printing log\n",
    "cfg.log_config.interval = 100\n",
    "\n",
    "# Set seed thus the results are more reproducible\n",
    "cfg.seed = 42\n",
    "set_random_seed(42, deterministic=False)\n",
    "cfg.gpu_ids = range(1)\n",
    "\n",
    "cfg.data.videos_per_gpu=6\n",
    "\n",
    "# cfg.model.backbone.in_channels = 2\n",
    "\n",
    "cfg.output_config = dict(out=f'{cfg.work_dir}/results.json')\n",
    "# We can initialize the logger for training and have a look\n",
    "# at the final config used for training\n",
    "# del cfg.optimizer['momentum']\n",
    "print(f'Config:\\n{cfg.pretty_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "123c41ec-9832-426e-85e4-4a58b5a7abb5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dDBWkdDRk6oz",
    "outputId": "85a52ef3-7b5c-4c52-8fef-00322a8c65e6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 01:03:38,251 - mmaction - INFO - load model from: https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth\n",
      "2021-08-31 01:03:38,252 - mmaction - INFO - Use load_from_http loader\n",
      "2021-08-31 01:03:40,329 - mmaction - INFO - load checkpoint from checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth\n",
      "2021-08-31 01:03:40,330 - mmaction - INFO - Use load_from_local loader\n",
      "2021-08-31 01:03:40,506 - mmaction - WARNING - The model and loaded state dict do not match exactly\n",
      "\n",
      "size mismatch for cls_head.fc_cls.weight: copying a param with shape torch.Size([400, 2048]) from checkpoint, the shape in current model is torch.Size([2, 2048]).\n",
      "size mismatch for cls_head.fc_cls.bias: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([2]).\n",
      "2021-08-31 01:03:40,512 - mmaction - INFO - Start running, host: robt427nv@robt427NV, work_dir: /home/robt427nv/childact/childact-checkpoints/CSN-gender-go\n",
      "2021-08-31 01:03:40,513 - mmaction - INFO - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(ABOVE_NORMAL) OptimizerHook                      \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "2021-08-31 01:03:40,514 - mmaction - INFO - workflow: [('train', 1)], max: 50 epochs\n",
      "/home/robt427nv/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/mmcv/runner/hooks/evaluation.py:190: UserWarning: runner.meta is None. Creating an empty one.\n",
      "  warnings.warn('runner.meta is None. Creating an empty one.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 18/18, 10.5 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 01:05:32,016 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 01:05:32,018 - mmaction - INFO - \n",
      "top1_acc\t1.0000\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 01:05:32,018 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 01:05:32,020 - mmaction - INFO - \n",
      "mean_acc\t1.0000\n",
      "2021-08-31 01:05:32,338 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_5.pth.\n",
      "2021-08-31 01:05:32,339 - mmaction - INFO - Best top1_acc is 1.0000 at 5 epoch.\n",
      "2021-08-31 01:05:32,339 - mmaction - INFO - Epoch(val) [5][3]\ttop1_acc: 1.0000, top5_acc: 1.0000, mean_class_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 18/18, 10.6 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 01:07:23,953 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 01:07:23,955 - mmaction - INFO - \n",
      "top1_acc\t1.0000\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 01:07:23,955 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 01:07:23,956 - mmaction - INFO - \n",
      "mean_acc\t1.0000\n",
      "2021-08-31 01:07:23,957 - mmaction - INFO - Epoch(val) [10][3]\ttop1_acc: 1.0000, top5_acc: 1.0000, mean_class_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 18/18, 10.6 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 01:09:16,193 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 01:09:16,194 - mmaction - INFO - \n",
      "top1_acc\t0.9444\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 01:09:16,195 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 01:09:16,196 - mmaction - INFO - \n",
      "mean_acc\t0.9000\n",
      "2021-08-31 01:09:16,196 - mmaction - INFO - Epoch(val) [15][3]\ttop1_acc: 0.9444, top5_acc: 1.0000, mean_class_accuracy: 0.9000\n",
      "2021-08-31 01:11:06,175 - mmaction - INFO - Saving checkpoint at 20 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 18/18, 10.8 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 01:11:08,261 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 01:11:08,262 - mmaction - INFO - \n",
      "top1_acc\t0.8889\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 01:11:08,262 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 01:11:08,263 - mmaction - INFO - \n",
      "mean_acc\t0.8000\n",
      "2021-08-31 01:11:08,263 - mmaction - INFO - Epoch(val) [20][3]\ttop1_acc: 0.8889, top5_acc: 1.0000, mean_class_accuracy: 0.8000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 18/18, 10.6 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 01:12:59,866 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 01:12:59,867 - mmaction - INFO - \n",
      "top1_acc\t0.8889\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 01:12:59,868 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 01:12:59,868 - mmaction - INFO - \n",
      "mean_acc\t0.8000\n",
      "2021-08-31 01:12:59,869 - mmaction - INFO - Epoch(val) [25][3]\ttop1_acc: 0.8889, top5_acc: 1.0000, mean_class_accuracy: 0.8000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 18/18, 10.5 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 01:14:51,130 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 01:14:51,131 - mmaction - INFO - \n",
      "top1_acc\t0.9444\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 01:14:51,132 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 01:14:51,133 - mmaction - INFO - \n",
      "mean_acc\t0.9000\n",
      "2021-08-31 01:14:51,134 - mmaction - INFO - Epoch(val) [30][3]\ttop1_acc: 0.9444, top5_acc: 1.0000, mean_class_accuracy: 0.9000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 18/18, 11.0 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 01:16:42,536 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 01:16:42,538 - mmaction - INFO - \n",
      "top1_acc\t0.9444\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 01:16:42,538 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 01:16:42,540 - mmaction - INFO - \n",
      "mean_acc\t0.9000\n",
      "2021-08-31 01:16:42,540 - mmaction - INFO - Epoch(val) [35][3]\ttop1_acc: 0.9444, top5_acc: 1.0000, mean_class_accuracy: 0.9000\n",
      "2021-08-31 01:18:32,267 - mmaction - INFO - Saving checkpoint at 40 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 18/18, 10.6 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 01:18:34,385 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 01:18:34,386 - mmaction - INFO - \n",
      "top1_acc\t0.9444\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 01:18:34,387 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 01:18:34,387 - mmaction - INFO - \n",
      "mean_acc\t0.9000\n",
      "2021-08-31 01:18:34,388 - mmaction - INFO - Epoch(val) [40][3]\ttop1_acc: 0.9444, top5_acc: 1.0000, mean_class_accuracy: 0.9000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 18/18, 10.6 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 01:20:26,112 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 01:20:26,113 - mmaction - INFO - \n",
      "top1_acc\t0.9444\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 01:20:26,114 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 01:20:26,114 - mmaction - INFO - \n",
      "mean_acc\t0.9000\n",
      "2021-08-31 01:20:26,115 - mmaction - INFO - Epoch(val) [45][3]\ttop1_acc: 0.9444, top5_acc: 1.0000, mean_class_accuracy: 0.9000\n",
      "2021-08-31 01:22:16,411 - mmaction - INFO - Saving checkpoint at 50 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 18/18, 10.3 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 01:22:18,594 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 01:22:18,595 - mmaction - INFO - \n",
      "top1_acc\t0.9444\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 01:22:18,595 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 01:22:18,596 - mmaction - INFO - \n",
      "mean_acc\t0.9000\n",
      "2021-08-31 01:22:18,597 - mmaction - INFO - Epoch(val) [50][3]\ttop1_acc: 0.9444, top5_acc: 1.0000, mean_class_accuracy: 0.9000\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "from mmaction.datasets import build_dataset\n",
    "from mmaction.models import build_model\n",
    "from mmaction.apis import train_model\n",
    "\n",
    "import mmcv\n",
    "\n",
    "# Build the dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# Build the recognizer\n",
    "model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.train_cfg, test_cfg=cfg.test_cfg)\n",
    "# CUDA_LAUNCH_BLOCKING=1\n",
    "# Create work_dir\n",
    "mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
    "train_model(model, datasets, cfg, distributed=False, validate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08c5bc17-cdfc-4e66-aaa6-56856527944c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f\"{cfg.work_dir}/model50e\", 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6a9f562-addb-4f84-92b0-add74de46501",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eyY3hCMwyTct",
    "outputId": "200e37c7-0da4-421f-98da-41418c3110ca",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 22/22, 0.6 task/s, elapsed: 36s, ETA:     0s\n",
      "Evaluating top_k_accuracy ...\n",
      "\n",
      "top1_acc\t0.9545\n",
      "top5_acc\t1.0000\n",
      "\n",
      "Evaluating mean_class_accuracy ...\n",
      "\n",
      "mean_acc\t0.9667\n",
      "top1_acc: 0.9545\n",
      "top5_acc: 1.0000\n",
      "mean_class_accuracy: 0.9667\n"
     ]
    }
   ],
   "source": [
    "from mmaction.apis import single_gpu_test\n",
    "from mmaction.datasets import build_dataloader\n",
    "from mmcv.parallel import MMDataParallel\n",
    "# from mmaction.models import build_model\n",
    "# from mmaction.datasets import build_dataset\n",
    "\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# Build a test dataloader\n",
    "dataset = build_dataset(cfg.data.test, dict(test_mode=True))\n",
    "data_loader = build_dataloader(\n",
    "        dataset,\n",
    "        videos_per_gpu=1,\n",
    "        workers_per_gpu=1,\n",
    "        dist=False,\n",
    "        shuffle=False)\n",
    "model = MMDataParallel(model, device_ids=[0])\n",
    "outputs = single_gpu_test(model, data_loader)\n",
    "\n",
    "eval_config = cfg.evaluation\n",
    "eval_config.pop('interval')\n",
    "eval_res = dataset.evaluate(outputs, **eval_config)\n",
    "for name, val in eval_res.items():\n",
    "    print(f'{name}: {val:.04f}')\n",
    "    \n",
    "output_config = cfg.output_config\n",
    "dataset.dump_results(outputs, **output_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37741934-4608-408a-9d1f-2a0a6cc4cb37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from mmaction.core import confusion_matrix \n",
    "\n",
    "gt_labels = [ann['label'] for ann in dataset.load_annotations()]\n",
    "pred = np.argmax(outputs, axis=1)\n",
    "cf_mat = confusion_matrix(pred, gt_labels).astype(float)\n",
    "cmap = sns.cubehelix_palette(50, hue=0.05, rot=0, light=0.9, dark=0, as_cmap=True)\n",
    "# /np.sum(cf_mat), fmt='.2%',\n",
    "# sns.heatmap(cf_mat, cmap=cmap, annot=True, xticklabels = ['6yo', '7yo', '8yo'], yticklabels = ['6yo', '7yo', '8yo'])\n",
    "sns.heatmap(cf_mat, cmap=cmap, annot=True, xticklabels = ['male','female'], yticklabels = ['male','female'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fea3412-c1cb-468b-94b3-ca43092748d0",
   "metadata": {},
   "source": [
    "# CSN gender RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26dd6167-6e23-4116-8ca3-7c5220f5a1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/robt427nv/childact\n"
     ]
    }
   ],
   "source": [
    "cd /home/robt427nv/childact/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac132fd6-0ca1-4256-913e-e9965b6793ae",
   "metadata": {
    "id": "LjCcmCKOjktc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mmcv import Config\n",
    "cfg = Config.fromfile('mmaction2/configs/recognition/csn/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3530e0f8-907a-43f1-8546-5741f0da7cff",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tlhu9byjjt-K",
    "outputId": "13d1bb01-f351-48c0-9efb-0bdf2c125d29",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "model = dict(\n",
      "    type='Recognizer3D',\n",
      "    backbone=dict(\n",
      "        type='ResNet3dCSN',\n",
      "        pretrained2d=False,\n",
      "        pretrained=\n",
      "        'https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth',\n",
      "        depth=152,\n",
      "        with_pool2=False,\n",
      "        bottleneck_mode='ir',\n",
      "        norm_eval=True,\n",
      "        zero_init_residual=False,\n",
      "        bn_frozen=True),\n",
      "    cls_head=dict(\n",
      "        type='I3DHead',\n",
      "        num_classes=2,\n",
      "        in_channels=2048,\n",
      "        spatial_type='avg',\n",
      "        dropout_ratio=0.5,\n",
      "        init_std=0.01),\n",
      "    train_cfg=None,\n",
      "    test_cfg=dict(average_clips='prob'))\n",
      "checkpoint_config = dict(interval=20)\n",
      "log_config = dict(interval=100, hooks=[dict(type='TextLoggerHook')])\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = 'checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth'\n",
      "resume_from = None\n",
      "workflow = [('train', 1)]\n",
      "dataset_type = 'RawframeDataset'\n",
      "data_root = 'age-gender-3split-rgb-frames/'\n",
      "data_root_val = 'age-gender-3split-rgb-frames/val/'\n",
      "ann_file_train = 'age-gender-3split-rgb-frames/childact_train_rgb320_gender_run.txt'\n",
      "ann_file_val = 'age-gender-3split-rgb-frames/childact_val_rgb320_gender_run.txt'\n",
      "ann_file_test = 'age-gender-3split-rgb-frames/childact_test_rgb320_gender_run.txt'\n",
      "img_norm_cfg = dict(\n",
      "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_bgr=False)\n",
      "train_pipeline = [\n",
      "    dict(type='SampleFrames', clip_len=32, frame_interval=2, num_clips=1),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='RandomResizedCrop'),\n",
      "    dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "    dict(type='Flip', flip_ratio=0.5),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "]\n",
      "val_pipeline = [\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=32,\n",
      "        frame_interval=2,\n",
      "        num_clips=1,\n",
      "        test_mode=True),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='CenterCrop', crop_size=224),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=32,\n",
      "        frame_interval=2,\n",
      "        num_clips=10,\n",
      "        test_mode=True),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='ThreeCrop', crop_size=256),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "data = dict(\n",
      "    videos_per_gpu=6,\n",
      "    workers_per_gpu=4,\n",
      "    train=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_train_rgb320_gender_run.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/train/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=1),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='RandomResizedCrop'),\n",
      "            dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "            dict(type='Flip', flip_ratio=0.5),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'),\n",
      "    val=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_val_rgb320_gender_run.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/val/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=1,\n",
      "                test_mode=True),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='CenterCrop', crop_size=224),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'),\n",
      "    test=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_test_rgb320_gender_run.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/test/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=10,\n",
      "                test_mode=True),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='ThreeCrop', crop_size=256),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'))\n",
      "evaluation = dict(\n",
      "    interval=5, metrics=['top_k_accuracy', 'mean_class_accuracy'])\n",
      "optimizer = dict(type='SGD', lr=0.000125, momentum=0.9, weight_decay=0.0001)\n",
      "optimizer_config = dict(grad_clip=dict(max_norm=40, norm_type=2))\n",
      "lr_config = dict(\n",
      "    policy='step',\n",
      "    step=[32, 48],\n",
      "    warmup='linear',\n",
      "    warmup_ratio=0.1,\n",
      "    warmup_by_epoch=True,\n",
      "    warmup_iters=16)\n",
      "total_epochs = 50\n",
      "work_dir = './childact-checkpoints/CSN-gender-run'\n",
      "find_unused_parameters = True\n",
      "omnisource = False\n",
      "seed = 42\n",
      "gpu_ids = range(0, 1)\n",
      "output_config = dict(out='./childact-checkpoints/CSN-gender-run/results.json')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mmcv.runner import set_random_seed\n",
    "\n",
    "# Modify dataset type and path\n",
    "# cfg.dataset_type = 'VideoDataset'\n",
    "cfg.data_root = 'age-gender-3split-rgb-frames/'\n",
    "cfg.data_root_val = 'age-gender-3split-rgb-frames/val/'\n",
    "cfg.ann_file_train = 'age-gender-3split-rgb-frames/childact_train_rgb320_gender_run.txt'\n",
    "cfg.ann_file_val = 'age-gender-3split-rgb-frames/childact_val_rgb320_gender_run.txt'\n",
    "cfg.ann_file_test = 'age-gender-3split-rgb-frames/childact_test_rgb320_gender_run.txt'\n",
    "\n",
    "# cfg.data.test.type = 'VideoDataset'\n",
    "cfg.data.test.ann_file = 'age-gender-3split-rgb-frames/childact_test_rgb320_gender_run.txt'\n",
    "cfg.data.test.data_prefix = 'age-gender-3split-rgb-frames/test/'\n",
    "\n",
    "# cfg.data.train.type = 'VideoDataset'\n",
    "cfg.data.train.ann_file = 'age-gender-3split-rgb-frames/childact_train_rgb320_gender_run.txt'\n",
    "cfg.data.train.data_prefix = 'age-gender-3split-rgb-frames/train/'\n",
    "\n",
    "# cfg.data.val.type = 'VideoDataset'\n",
    "cfg.data.val.ann_file = 'age-gender-3split-rgb-frames/childact_val_rgb320_gender_run.txt'\n",
    "cfg.data.val.data_prefix = 'age-gender-3split-rgb-frames/val/'\n",
    "\n",
    "# cfg.data.test.modality = 'Flow'\n",
    "# cfg.data.val.modality = 'Flow'\n",
    "# cfg.data.train.modality = 'Flow'\n",
    "\n",
    "# cfg.data.train.start_index = 0\n",
    "# cfg.data.test.start_index = 0\n",
    "# cfg.data.val.start_index = 0\n",
    "\n",
    "cfg.data.test.filename_tmpl = '{:03}.jpeg'\n",
    "cfg.data.train.filename_tmpl = '{:03}.jpeg'\n",
    "cfg.data.val.filename_tmpl = '{:03}.jpeg'\n",
    "# The flag is used to determine whether it is omnisource training\n",
    "cfg.setdefault('omnisource', False)\n",
    "# Modify num classes of the model in cls_head\n",
    "cfg.model.cls_head.num_classes = 2\n",
    "# We can use the pre-trained TSN model\n",
    "cfg.load_from = 'checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth'\n",
    "# cfg.resume_from = './childact-mm/latest.pth'\n",
    "\n",
    "# Set up working dir to save files and logs.\n",
    "cfg.work_dir = './childact-checkpoints/CSN-gender-run'\n",
    "\n",
    "cfg.total_epochs = 50\n",
    "\n",
    "# cfg.momentum_config = dict(\n",
    "#     policy='cyclic',\n",
    "#     target_ratio=(0.85 / 0.95, 1),\n",
    "#     cyclic_times=1,\n",
    "#     step_ratio_up=0.4,\n",
    "# )\n",
    "\n",
    "# We can set the checkpoint saving interval to reduce the storage cost\n",
    "cfg.checkpoint_config.interval = 20\n",
    "# We can set the log print interval to reduce the the times of printing log\n",
    "cfg.log_config.interval = 100\n",
    "\n",
    "# Set seed thus the results are more reproducible\n",
    "cfg.seed = 42\n",
    "set_random_seed(42, deterministic=False)\n",
    "cfg.gpu_ids = range(1)\n",
    "\n",
    "cfg.data.videos_per_gpu=6\n",
    "\n",
    "# cfg.model.backbone.in_channels = 2\n",
    "\n",
    "cfg.output_config = dict(out=f'{cfg.work_dir}/results.json')\n",
    "# We can initialize the logger for training and have a look\n",
    "# at the final config used for training\n",
    "# del cfg.optimizer['momentum']\n",
    "print(f'Config:\\n{cfg.pretty_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6bee9d0-bb4e-4cc2-b2c0-d38c1d49ec67",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "dDBWkdDRk6oz",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "85a52ef3-7b5c-4c52-8fef-00322a8c65e6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 01:25:13,172 - mmaction - INFO - load model from: https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth\n",
      "2021-08-31 01:25:13,173 - mmaction - INFO - Use load_from_http loader\n",
      "2021-08-31 01:25:15,225 - mmaction - INFO - load checkpoint from checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth\n",
      "2021-08-31 01:25:15,226 - mmaction - INFO - Use load_from_local loader\n",
      "2021-08-31 01:25:15,388 - mmaction - WARNING - The model and loaded state dict do not match exactly\n",
      "\n",
      "size mismatch for cls_head.fc_cls.weight: copying a param with shape torch.Size([400, 2048]) from checkpoint, the shape in current model is torch.Size([2, 2048]).\n",
      "size mismatch for cls_head.fc_cls.bias: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([2]).\n",
      "2021-08-31 01:25:15,394 - mmaction - INFO - Start running, host: robt427nv@robt427NV, work_dir: /home/robt427nv/childact/childact-checkpoints/CSN-gender-run\n",
      "2021-08-31 01:25:15,395 - mmaction - INFO - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(ABOVE_NORMAL) OptimizerHook                      \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "2021-08-31 01:25:15,395 - mmaction - INFO - workflow: [('train', 1)], max: 50 epochs\n",
      "/home/robt427nv/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/mmcv/runner/hooks/evaluation.py:190: UserWarning: runner.meta is None. Creating an empty one.\n",
      "  warnings.warn('runner.meta is None. Creating an empty one.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 23/23, 11.1 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 01:27:11,770 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 01:27:11,771 - mmaction - INFO - \n",
      "top1_acc\t0.9130\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 01:27:11,772 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 01:27:11,773 - mmaction - INFO - \n",
      "mean_acc\t0.9115\n",
      "2021-08-31 01:27:12,079 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_5.pth.\n",
      "2021-08-31 01:27:12,080 - mmaction - INFO - Best top1_acc is 0.9130 at 5 epoch.\n",
      "2021-08-31 01:27:12,081 - mmaction - INFO - Epoch(val) [5][4]\ttop1_acc: 0.9130, top5_acc: 1.0000, mean_class_accuracy: 0.9115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 23/23, 11.0 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 01:29:08,848 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 01:29:08,849 - mmaction - INFO - \n",
      "top1_acc\t0.7826\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 01:29:08,849 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 01:29:08,850 - mmaction - INFO - \n",
      "mean_acc\t0.7962\n",
      "2021-08-31 01:29:08,851 - mmaction - INFO - Epoch(val) [10][4]\ttop1_acc: 0.7826, top5_acc: 1.0000, mean_class_accuracy: 0.7962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 23/23, 11.0 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 01:31:05,590 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 01:31:05,591 - mmaction - INFO - \n",
      "top1_acc\t0.8696\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 01:31:05,592 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 01:31:05,594 - mmaction - INFO - \n",
      "mean_acc\t0.8615\n",
      "2021-08-31 01:31:05,595 - mmaction - INFO - Epoch(val) [15][4]\ttop1_acc: 0.8696, top5_acc: 1.0000, mean_class_accuracy: 0.8615\n",
      "2021-08-31 01:33:00,557 - mmaction - INFO - Saving checkpoint at 20 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 23/23, 11.0 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 01:33:03,021 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 01:33:03,023 - mmaction - INFO - \n",
      "top1_acc\t0.9130\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 01:33:03,024 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 01:33:03,026 - mmaction - INFO - \n",
      "mean_acc\t0.9115\n",
      "2021-08-31 01:33:03,027 - mmaction - INFO - Epoch(val) [20][4]\ttop1_acc: 0.9130, top5_acc: 1.0000, mean_class_accuracy: 0.9115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 23/23, 11.1 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 01:34:59,617 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 01:34:59,619 - mmaction - INFO - \n",
      "top1_acc\t0.8696\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 01:34:59,620 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 01:34:59,622 - mmaction - INFO - \n",
      "mean_acc\t0.8731\n",
      "2021-08-31 01:34:59,623 - mmaction - INFO - Epoch(val) [25][4]\ttop1_acc: 0.8696, top5_acc: 1.0000, mean_class_accuracy: 0.8731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 23/23, 10.5 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 01:36:56,658 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 01:36:56,659 - mmaction - INFO - \n",
      "top1_acc\t0.9130\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 01:36:56,660 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 01:36:56,660 - mmaction - INFO - \n",
      "mean_acc\t0.9115\n",
      "2021-08-31 01:36:56,661 - mmaction - INFO - Epoch(val) [30][4]\ttop1_acc: 0.9130, top5_acc: 1.0000, mean_class_accuracy: 0.9115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 23/23, 10.6 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 01:38:53,565 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 01:38:53,567 - mmaction - INFO - \n",
      "top1_acc\t0.8696\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 01:38:53,567 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 01:38:53,568 - mmaction - INFO - \n",
      "mean_acc\t0.8615\n",
      "2021-08-31 01:38:53,569 - mmaction - INFO - Epoch(val) [35][4]\ttop1_acc: 0.8696, top5_acc: 1.0000, mean_class_accuracy: 0.8615\n",
      "2021-08-31 01:40:47,962 - mmaction - INFO - Saving checkpoint at 40 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 23/23, 11.2 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 01:40:50,387 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 01:40:50,389 - mmaction - INFO - \n",
      "top1_acc\t0.8696\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 01:40:50,390 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 01:40:50,393 - mmaction - INFO - \n",
      "mean_acc\t0.8615\n",
      "2021-08-31 01:40:50,394 - mmaction - INFO - Epoch(val) [40][4]\ttop1_acc: 0.8696, top5_acc: 1.0000, mean_class_accuracy: 0.8615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 23/23, 10.9 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 01:42:46,768 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 01:42:46,770 - mmaction - INFO - \n",
      "top1_acc\t0.9130\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 01:42:46,771 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 01:42:46,772 - mmaction - INFO - \n",
      "mean_acc\t0.9115\n",
      "2021-08-31 01:42:46,772 - mmaction - INFO - Epoch(val) [45][4]\ttop1_acc: 0.9130, top5_acc: 1.0000, mean_class_accuracy: 0.9115\n",
      "2021-08-31 01:44:41,324 - mmaction - INFO - Saving checkpoint at 50 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 23/23, 10.5 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 01:44:43,885 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 01:44:43,886 - mmaction - INFO - \n",
      "top1_acc\t0.9130\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 01:44:43,887 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 01:44:43,888 - mmaction - INFO - \n",
      "mean_acc\t0.9115\n",
      "2021-08-31 01:44:43,889 - mmaction - INFO - Epoch(val) [50][4]\ttop1_acc: 0.9130, top5_acc: 1.0000, mean_class_accuracy: 0.9115\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "from mmaction.datasets import build_dataset\n",
    "from mmaction.models import build_model\n",
    "from mmaction.apis import train_model\n",
    "\n",
    "import mmcv\n",
    "\n",
    "# Build the dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# Build the recognizer\n",
    "model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.train_cfg, test_cfg=cfg.test_cfg)\n",
    "# CUDA_LAUNCH_BLOCKING=1\n",
    "# Create work_dir\n",
    "mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
    "train_model(model, datasets, cfg, distributed=False, validate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15c3347d-c15f-4198-b865-5db8e05a2c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f\"{cfg.work_dir}/model50e\", 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0db17f33-cdec-4530-b9b1-53cdfc4de765",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eyY3hCMwyTct",
    "outputId": "200e37c7-0da4-421f-98da-41418c3110ca",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 26/26, 0.6 task/s, elapsed: 42s, ETA:     0s\n",
      "Evaluating top_k_accuracy ...\n",
      "\n",
      "top1_acc\t0.8846\n",
      "top5_acc\t1.0000\n",
      "\n",
      "Evaluating mean_class_accuracy ...\n",
      "\n",
      "mean_acc\t0.8810\n",
      "top1_acc: 0.8846\n",
      "top5_acc: 1.0000\n",
      "mean_class_accuracy: 0.8810\n"
     ]
    }
   ],
   "source": [
    "from mmaction.apis import single_gpu_test\n",
    "from mmaction.datasets import build_dataloader\n",
    "from mmcv.parallel import MMDataParallel\n",
    "# from mmaction.models import build_model\n",
    "# from mmaction.datasets import build_dataset\n",
    "\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# Build a test dataloader\n",
    "dataset = build_dataset(cfg.data.test, dict(test_mode=True))\n",
    "data_loader = build_dataloader(\n",
    "        dataset,\n",
    "        videos_per_gpu=1,\n",
    "        workers_per_gpu=1,\n",
    "        dist=False,\n",
    "        shuffle=False)\n",
    "model = MMDataParallel(model, device_ids=[0])\n",
    "outputs = single_gpu_test(model, data_loader)\n",
    "\n",
    "eval_config = cfg.evaluation\n",
    "eval_config.pop('interval')\n",
    "eval_res = dataset.evaluate(outputs, **eval_config)\n",
    "for name, val in eval_res.items():\n",
    "    print(f'{name}: {val:.04f}')\n",
    "    \n",
    "output_config = cfg.output_config\n",
    "dataset.dump_results(outputs, **output_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19dd6092-675d-4e35-aa97-9988db147821",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from mmaction.core import confusion_matrix \n",
    "\n",
    "gt_labels = [ann['label'] for ann in dataset.load_annotations()]\n",
    "pred = np.argmax(outputs, axis=1)\n",
    "cf_mat = confusion_matrix(pred, gt_labels).astype(float)\n",
    "cmap = sns.cubehelix_palette(50, hue=0.05, rot=0, light=0.9, dark=0, as_cmap=True)\n",
    "# /np.sum(cf_mat), fmt='.2%',\n",
    "# sns.heatmap(cf_mat, cmap=cmap, annot=True, xticklabels = ['6yo', '7yo', '8yo'], yticklabels = ['6yo', '7yo', '8yo'])\n",
    "sns.heatmap(cf_mat, cmap=cmap, annot=True, xticklabels = ['male','female'], yticklabels = ['male','female'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd362de-add6-4bbb-98a8-e8b2b67394c7",
   "metadata": {},
   "source": [
    "# CSN gender WALK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ec544b4-1e15-4d5e-9737-8f5b9e4aed3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/robt427nv/childact\n"
     ]
    }
   ],
   "source": [
    "cd /home/robt427nv/childact/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b1a2ddf-843b-4439-8b54-14a67ab777bf",
   "metadata": {
    "id": "LjCcmCKOjktc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mmcv import Config\n",
    "cfg = Config.fromfile('mmaction2/configs/recognition/csn/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac1c9aab-666b-4692-aa31-5d605eebc7e1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "tlhu9byjjt-K",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "13d1bb01-f351-48c0-9efb-0bdf2c125d29",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "model = dict(\n",
      "    type='Recognizer3D',\n",
      "    backbone=dict(\n",
      "        type='ResNet3dCSN',\n",
      "        pretrained2d=False,\n",
      "        pretrained=\n",
      "        'https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth',\n",
      "        depth=152,\n",
      "        with_pool2=False,\n",
      "        bottleneck_mode='ir',\n",
      "        norm_eval=True,\n",
      "        zero_init_residual=False,\n",
      "        bn_frozen=True),\n",
      "    cls_head=dict(\n",
      "        type='I3DHead',\n",
      "        num_classes=2,\n",
      "        in_channels=2048,\n",
      "        spatial_type='avg',\n",
      "        dropout_ratio=0.5,\n",
      "        init_std=0.01),\n",
      "    train_cfg=None,\n",
      "    test_cfg=dict(average_clips='prob'))\n",
      "checkpoint_config = dict(interval=20)\n",
      "log_config = dict(interval=100, hooks=[dict(type='TextLoggerHook')])\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = 'checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth'\n",
      "resume_from = None\n",
      "workflow = [('train', 1)]\n",
      "dataset_type = 'RawframeDataset'\n",
      "data_root = 'age-gender-3split-rgb-frames/'\n",
      "data_root_val = 'age-gender-3split-rgb-frames/val/'\n",
      "ann_file_train = 'age-gender-3split-rgb-frames/childact_train_rgb320_gender_walk.txt'\n",
      "ann_file_val = 'age-gender-3split-rgb-frames/childact_val_rgb320_gender_walk.txt'\n",
      "ann_file_test = 'age-gender-3split-rgb-frames/childact_test_rgb320_gender_walk.txt'\n",
      "img_norm_cfg = dict(\n",
      "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_bgr=False)\n",
      "train_pipeline = [\n",
      "    dict(type='SampleFrames', clip_len=32, frame_interval=2, num_clips=1),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='RandomResizedCrop'),\n",
      "    dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "    dict(type='Flip', flip_ratio=0.5),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "]\n",
      "val_pipeline = [\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=32,\n",
      "        frame_interval=2,\n",
      "        num_clips=1,\n",
      "        test_mode=True),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='CenterCrop', crop_size=224),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=32,\n",
      "        frame_interval=2,\n",
      "        num_clips=10,\n",
      "        test_mode=True),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='ThreeCrop', crop_size=256),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "data = dict(\n",
      "    videos_per_gpu=6,\n",
      "    workers_per_gpu=4,\n",
      "    train=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_train_rgb320_gender_walk.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/train/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=1),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='RandomResizedCrop'),\n",
      "            dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "            dict(type='Flip', flip_ratio=0.5),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'),\n",
      "    val=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_val_rgb320_gender_walk.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/val/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=1,\n",
      "                test_mode=True),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='CenterCrop', crop_size=224),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'),\n",
      "    test=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_test_rgb320_gender_walk.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/test/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=10,\n",
      "                test_mode=True),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='ThreeCrop', crop_size=256),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'))\n",
      "evaluation = dict(\n",
      "    interval=5, metrics=['top_k_accuracy', 'mean_class_accuracy'])\n",
      "optimizer = dict(type='SGD', lr=0.000125, momentum=0.9, weight_decay=0.0001)\n",
      "optimizer_config = dict(grad_clip=dict(max_norm=40, norm_type=2))\n",
      "lr_config = dict(\n",
      "    policy='step',\n",
      "    step=[32, 48],\n",
      "    warmup='linear',\n",
      "    warmup_ratio=0.1,\n",
      "    warmup_by_epoch=True,\n",
      "    warmup_iters=16)\n",
      "total_epochs = 50\n",
      "work_dir = './childact-checkpoints/CSN-gender-walk'\n",
      "find_unused_parameters = True\n",
      "omnisource = False\n",
      "seed = 42\n",
      "gpu_ids = range(0, 1)\n",
      "output_config = dict(out='./childact-checkpoints/CSN-gender-walk/results.json')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mmcv.runner import set_random_seed\n",
    "\n",
    "# Modify dataset type and path\n",
    "# cfg.dataset_type = 'VideoDataset'\n",
    "cfg.data_root = 'age-gender-3split-rgb-frames/'\n",
    "cfg.data_root_val = 'age-gender-3split-rgb-frames/val/'\n",
    "cfg.ann_file_train = 'age-gender-3split-rgb-frames/childact_train_rgb320_gender_walk.txt'\n",
    "cfg.ann_file_val = 'age-gender-3split-rgb-frames/childact_val_rgb320_gender_walk.txt'\n",
    "cfg.ann_file_test = 'age-gender-3split-rgb-frames/childact_test_rgb320_gender_walk.txt'\n",
    "\n",
    "# cfg.data.test.type = 'VideoDataset'\n",
    "cfg.data.test.ann_file = 'age-gender-3split-rgb-frames/childact_test_rgb320_gender_walk.txt'\n",
    "cfg.data.test.data_prefix = 'age-gender-3split-rgb-frames/test/'\n",
    "\n",
    "# cfg.data.train.type = 'VideoDataset'\n",
    "cfg.data.train.ann_file = 'age-gender-3split-rgb-frames/childact_train_rgb320_gender_walk.txt'\n",
    "cfg.data.train.data_prefix = 'age-gender-3split-rgb-frames/train/'\n",
    "\n",
    "# cfg.data.val.type = 'VideoDataset'\n",
    "cfg.data.val.ann_file = 'age-gender-3split-rgb-frames/childact_val_rgb320_gender_walk.txt'\n",
    "cfg.data.val.data_prefix = 'age-gender-3split-rgb-frames/val/'\n",
    "\n",
    "# cfg.data.test.modality = 'Flow'\n",
    "# cfg.data.val.modality = 'Flow'\n",
    "# cfg.data.train.modality = 'Flow'\n",
    "\n",
    "# cfg.data.train.start_index = 0\n",
    "# cfg.data.test.start_index = 0\n",
    "# cfg.data.val.start_index = 0\n",
    "\n",
    "cfg.data.test.filename_tmpl = '{:03}.jpeg'\n",
    "cfg.data.train.filename_tmpl = '{:03}.jpeg'\n",
    "cfg.data.val.filename_tmpl = '{:03}.jpeg'\n",
    "# The flag is used to determine whether it is omnisource training\n",
    "cfg.setdefault('omnisource', False)\n",
    "# Modify num classes of the model in cls_head\n",
    "cfg.model.cls_head.num_classes = 2\n",
    "# We can use the pre-trained TSN model\n",
    "cfg.load_from = 'checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth'\n",
    "# cfg.resume_from = './childact-mm/latest.pth'\n",
    "\n",
    "# Set up working dir to save files and logs.\n",
    "cfg.work_dir = './childact-checkpoints/CSN-gender-walk'\n",
    "\n",
    "cfg.total_epochs = 50\n",
    "\n",
    "# cfg.momentum_config = dict(\n",
    "#     policy='cyclic',\n",
    "#     target_ratio=(0.85 / 0.95, 1),\n",
    "#     cyclic_times=1,\n",
    "#     step_ratio_up=0.4,\n",
    "# )\n",
    "\n",
    "# We can set the checkpoint saving interval to reduce the storage cost\n",
    "cfg.checkpoint_config.interval = 20\n",
    "# We can set the log print interval to reduce the the times of printing log\n",
    "cfg.log_config.interval = 100\n",
    "\n",
    "# Set seed thus the results are more reproducible\n",
    "cfg.seed = 42\n",
    "set_random_seed(42, deterministic=False)\n",
    "cfg.gpu_ids = range(1)\n",
    "\n",
    "cfg.data.videos_per_gpu=6\n",
    "\n",
    "# cfg.model.backbone.in_channels = 2\n",
    "\n",
    "cfg.output_config = dict(out=f'{cfg.work_dir}/results.json')\n",
    "# We can initialize the logger for training and have a look\n",
    "# at the final config used for training\n",
    "# del cfg.optimizer['momentum']\n",
    "print(f'Config:\\n{cfg.pretty_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3245fab-d8f0-4e13-9984-5144a8170e6d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dDBWkdDRk6oz",
    "outputId": "85a52ef3-7b5c-4c52-8fef-00322a8c65e6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 10:29:57,210 - mmaction - INFO - load model from: https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth\n",
      "2021-08-31 10:29:57,211 - mmaction - INFO - Use load_from_http loader\n",
      "2021-08-31 10:29:59,239 - mmaction - INFO - load checkpoint from checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth\n",
      "2021-08-31 10:29:59,240 - mmaction - INFO - Use load_from_local loader\n",
      "2021-08-31 10:29:59,399 - mmaction - WARNING - The model and loaded state dict do not match exactly\n",
      "\n",
      "size mismatch for cls_head.fc_cls.weight: copying a param with shape torch.Size([400, 2048]) from checkpoint, the shape in current model is torch.Size([2, 2048]).\n",
      "size mismatch for cls_head.fc_cls.bias: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([2]).\n",
      "2021-08-31 10:29:59,405 - mmaction - INFO - Start running, host: robt427nv@robt427NV, work_dir: /home/robt427nv/childact/childact-checkpoints/CSN-gender-walk\n",
      "2021-08-31 10:29:59,405 - mmaction - INFO - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(ABOVE_NORMAL) OptimizerHook                      \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "2021-08-31 10:29:59,406 - mmaction - INFO - workflow: [('train', 1)], max: 50 epochs\n",
      "/home/robt427nv/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/mmcv/runner/hooks/evaluation.py:190: UserWarning: runner.meta is None. Creating an empty one.\n",
      "  warnings.warn('runner.meta is None. Creating an empty one.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 20/20, 10.8 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 10:31:30,736 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 10:31:30,737 - mmaction - INFO - \n",
      "top1_acc\t0.5500\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 10:31:30,738 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 10:31:30,739 - mmaction - INFO - \n",
      "mean_acc\t0.5667\n",
      "2021-08-31 10:31:31,054 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_5.pth.\n",
      "2021-08-31 10:31:31,055 - mmaction - INFO - Best top1_acc is 0.5500 at 5 epoch.\n",
      "2021-08-31 10:31:31,056 - mmaction - INFO - Epoch(val) [5][4]\ttop1_acc: 0.5500, top5_acc: 1.0000, mean_class_accuracy: 0.5667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 20/20, 10.6 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 10:33:02,115 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 10:33:02,117 - mmaction - INFO - \n",
      "top1_acc\t0.7500\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 10:33:02,118 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 10:33:02,119 - mmaction - INFO - \n",
      "mean_acc\t0.6333\n",
      "2021-08-31 10:33:02,526 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_10.pth.\n",
      "2021-08-31 10:33:02,528 - mmaction - INFO - Best top1_acc is 0.7500 at 10 epoch.\n",
      "2021-08-31 10:33:02,529 - mmaction - INFO - Epoch(val) [10][4]\ttop1_acc: 0.7500, top5_acc: 1.0000, mean_class_accuracy: 0.6333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 20/20, 11.3 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 10:34:33,936 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 10:34:33,937 - mmaction - INFO - \n",
      "top1_acc\t0.7500\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 10:34:33,938 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 10:34:33,939 - mmaction - INFO - \n",
      "mean_acc\t0.6333\n",
      "2021-08-31 10:34:33,940 - mmaction - INFO - Epoch(val) [15][4]\ttop1_acc: 0.7500, top5_acc: 1.0000, mean_class_accuracy: 0.6333\n",
      "2021-08-31 10:36:03,660 - mmaction - INFO - Saving checkpoint at 20 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 20/20, 10.4 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 10:36:05,982 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 10:36:05,983 - mmaction - INFO - \n",
      "top1_acc\t0.7000\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 10:36:05,983 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 10:36:05,984 - mmaction - INFO - \n",
      "mean_acc\t0.6667\n",
      "2021-08-31 10:36:05,985 - mmaction - INFO - Epoch(val) [20][4]\ttop1_acc: 0.7000, top5_acc: 1.0000, mean_class_accuracy: 0.6667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 20/20, 10.7 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 10:37:37,841 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 10:37:37,843 - mmaction - INFO - \n",
      "top1_acc\t0.6500\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 10:37:37,843 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 10:37:37,844 - mmaction - INFO - \n",
      "mean_acc\t0.6333\n",
      "2021-08-31 10:37:37,845 - mmaction - INFO - Epoch(val) [25][4]\ttop1_acc: 0.6500, top5_acc: 1.0000, mean_class_accuracy: 0.6333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 20/20, 10.9 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 10:39:09,580 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 10:39:09,581 - mmaction - INFO - \n",
      "top1_acc\t0.7500\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 10:39:09,582 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 10:39:09,582 - mmaction - INFO - \n",
      "mean_acc\t0.6333\n",
      "2021-08-31 10:39:09,583 - mmaction - INFO - Epoch(val) [30][4]\ttop1_acc: 0.7500, top5_acc: 1.0000, mean_class_accuracy: 0.6333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 20/20, 10.5 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 10:40:41,496 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 10:40:41,498 - mmaction - INFO - \n",
      "top1_acc\t0.7500\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 10:40:41,498 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 10:40:41,500 - mmaction - INFO - \n",
      "mean_acc\t0.7000\n",
      "2021-08-31 10:40:41,500 - mmaction - INFO - Epoch(val) [35][4]\ttop1_acc: 0.7500, top5_acc: 1.0000, mean_class_accuracy: 0.7000\n",
      "2021-08-31 10:42:11,297 - mmaction - INFO - Saving checkpoint at 40 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 20/20, 10.4 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 10:42:13,628 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 10:42:13,630 - mmaction - INFO - \n",
      "top1_acc\t0.8000\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 10:42:13,631 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 10:42:13,632 - mmaction - INFO - \n",
      "mean_acc\t0.7333\n",
      "2021-08-31 10:42:13,962 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_40.pth.\n",
      "2021-08-31 10:42:13,963 - mmaction - INFO - Best top1_acc is 0.8000 at 40 epoch.\n",
      "2021-08-31 10:42:13,964 - mmaction - INFO - Epoch(val) [40][4]\ttop1_acc: 0.8000, top5_acc: 1.0000, mean_class_accuracy: 0.7333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 20/20, 10.6 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 10:43:46,024 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 10:43:46,025 - mmaction - INFO - \n",
      "top1_acc\t0.7500\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 10:43:46,026 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 10:43:46,027 - mmaction - INFO - \n",
      "mean_acc\t0.6333\n",
      "2021-08-31 10:43:46,028 - mmaction - INFO - Epoch(val) [45][4]\ttop1_acc: 0.7500, top5_acc: 1.0000, mean_class_accuracy: 0.6333\n",
      "2021-08-31 10:45:15,957 - mmaction - INFO - Saving checkpoint at 50 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 20/20, 10.4 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 10:45:18,252 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 10:45:18,254 - mmaction - INFO - \n",
      "top1_acc\t0.8000\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 10:45:18,255 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 10:45:18,256 - mmaction - INFO - \n",
      "mean_acc\t0.7333\n",
      "2021-08-31 10:45:18,257 - mmaction - INFO - Epoch(val) [50][4]\ttop1_acc: 0.8000, top5_acc: 1.0000, mean_class_accuracy: 0.7333\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "from mmaction.datasets import build_dataset\n",
    "from mmaction.models import build_model\n",
    "from mmaction.apis import train_model\n",
    "\n",
    "import mmcv\n",
    "\n",
    "# Build the dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# Build the recognizer\n",
    "model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.train_cfg, test_cfg=cfg.test_cfg)\n",
    "# CUDA_LAUNCH_BLOCKING=1\n",
    "# Create work_dir\n",
    "mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
    "train_model(model, datasets, cfg, distributed=False, validate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ed74c74-e7ae-4f75-8201-ab8958383e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f\"{cfg.work_dir}/model50e\", 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "848a23df-7700-48a0-836f-cffe8a5ae400",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eyY3hCMwyTct",
    "outputId": "200e37c7-0da4-421f-98da-41418c3110ca",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 11/11, 0.5 task/s, elapsed: 23s, ETA:     0s\n",
      "Evaluating top_k_accuracy ...\n",
      "\n",
      "top1_acc\t0.4545\n",
      "top5_acc\t1.0000\n",
      "\n",
      "Evaluating mean_class_accuracy ...\n",
      "\n",
      "mean_acc\t0.4667\n",
      "top1_acc: 0.4545\n",
      "top5_acc: 1.0000\n",
      "mean_class_accuracy: 0.4667\n"
     ]
    }
   ],
   "source": [
    "from mmaction.apis import single_gpu_test\n",
    "from mmaction.datasets import build_dataloader\n",
    "from mmcv.parallel import MMDataParallel\n",
    "# from mmaction.models import build_model\n",
    "# from mmaction.datasets import build_dataset\n",
    "\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# Build a test dataloader\n",
    "dataset = build_dataset(cfg.data.test, dict(test_mode=True))\n",
    "data_loader = build_dataloader(\n",
    "        dataset,\n",
    "        videos_per_gpu=1,\n",
    "        workers_per_gpu=1,\n",
    "        dist=False,\n",
    "        shuffle=False)\n",
    "model = MMDataParallel(model, device_ids=[0])\n",
    "outputs = single_gpu_test(model, data_loader)\n",
    "\n",
    "eval_config = cfg.evaluation\n",
    "eval_config.pop('interval')\n",
    "eval_res = dataset.evaluate(outputs, **eval_config)\n",
    "for name, val in eval_res.items():\n",
    "    print(f'{name}: {val:.04f}')\n",
    "    \n",
    "output_config = cfg.output_config\n",
    "dataset.dump_results(outputs, **output_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e6ce3c8-062d-43ec-97bc-9e000dc22065",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from mmaction.core import confusion_matrix \n",
    "\n",
    "gt_labels = [ann['label'] for ann in dataset.load_annotations()]\n",
    "pred = np.argmax(outputs, axis=1)\n",
    "cf_mat = confusion_matrix(pred, gt_labels).astype(float)\n",
    "cmap = sns.cubehelix_palette(50, hue=0.05, rot=0, light=0.9, dark=0, as_cmap=True)\n",
    "# /np.sum(cf_mat), fmt='.2%',\n",
    "# sns.heatmap(cf_mat, cmap=cmap, annot=True, xticklabels = ['6yo', '7yo', '8yo'], yticklabels = ['6yo', '7yo', '8yo'])\n",
    "sns.heatmap(cf_mat, cmap=cmap, annot=True, xticklabels = ['male','female'], yticklabels = ['male','female'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dcb5f3-c9bc-4b84-b368-4ce1fe9058b5",
   "metadata": {},
   "source": [
    "# CSN gender WAVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ad4e4ed-3aff-4668-924f-963b11eab320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/robt427nv/childact\n"
     ]
    }
   ],
   "source": [
    "cd /home/robt427nv/childact/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d64cca6-042c-4c35-97d5-8752fd88e1b8",
   "metadata": {
    "id": "LjCcmCKOjktc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mmcv import Config\n",
    "cfg = Config.fromfile('mmaction2/configs/recognition/csn/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b01e0d06-2dfa-46ca-a0c0-b3ca81df380c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "tlhu9byjjt-K",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "13d1bb01-f351-48c0-9efb-0bdf2c125d29",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "model = dict(\n",
      "    type='Recognizer3D',\n",
      "    backbone=dict(\n",
      "        type='ResNet3dCSN',\n",
      "        pretrained2d=False,\n",
      "        pretrained=\n",
      "        'https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth',\n",
      "        depth=152,\n",
      "        with_pool2=False,\n",
      "        bottleneck_mode='ir',\n",
      "        norm_eval=True,\n",
      "        zero_init_residual=False,\n",
      "        bn_frozen=True),\n",
      "    cls_head=dict(\n",
      "        type='I3DHead',\n",
      "        num_classes=2,\n",
      "        in_channels=2048,\n",
      "        spatial_type='avg',\n",
      "        dropout_ratio=0.5,\n",
      "        init_std=0.01),\n",
      "    train_cfg=None,\n",
      "    test_cfg=dict(average_clips='prob'))\n",
      "checkpoint_config = dict(interval=20)\n",
      "log_config = dict(interval=100, hooks=[dict(type='TextLoggerHook')])\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = 'checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth'\n",
      "resume_from = None\n",
      "workflow = [('train', 1)]\n",
      "dataset_type = 'RawframeDataset'\n",
      "data_root = 'age-gender-3split-rgb-frames/'\n",
      "data_root_val = 'age-gender-3split-rgb-frames/val/'\n",
      "ann_file_train = 'age-gender-3split-rgb-frames/childact_train_rgb320_gender_wawe.txt'\n",
      "ann_file_val = 'age-gender-3split-rgb-frames/childact_val_rgb320_gender_wawe.txt'\n",
      "ann_file_test = 'age-gender-3split-rgb-frames/childact_test_rgb320_gender_wawe.txt'\n",
      "img_norm_cfg = dict(\n",
      "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_bgr=False)\n",
      "train_pipeline = [\n",
      "    dict(type='SampleFrames', clip_len=32, frame_interval=2, num_clips=1),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='RandomResizedCrop'),\n",
      "    dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "    dict(type='Flip', flip_ratio=0.5),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "]\n",
      "val_pipeline = [\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=32,\n",
      "        frame_interval=2,\n",
      "        num_clips=1,\n",
      "        test_mode=True),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='CenterCrop', crop_size=224),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=32,\n",
      "        frame_interval=2,\n",
      "        num_clips=10,\n",
      "        test_mode=True),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='ThreeCrop', crop_size=256),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "data = dict(\n",
      "    videos_per_gpu=6,\n",
      "    workers_per_gpu=4,\n",
      "    train=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_train_rgb320_gender_wawe.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/train/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=1),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='RandomResizedCrop'),\n",
      "            dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "            dict(type='Flip', flip_ratio=0.5),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'),\n",
      "    val=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_val_rgb320_gender_wawe.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/val/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=1,\n",
      "                test_mode=True),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='CenterCrop', crop_size=224),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'),\n",
      "    test=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_test_rgb320_gender_wawe.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/test/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=10,\n",
      "                test_mode=True),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='ThreeCrop', crop_size=256),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'))\n",
      "evaluation = dict(\n",
      "    interval=5, metrics=['top_k_accuracy', 'mean_class_accuracy'])\n",
      "optimizer = dict(type='SGD', lr=0.000125, momentum=0.9, weight_decay=0.0001)\n",
      "optimizer_config = dict(grad_clip=dict(max_norm=40, norm_type=2))\n",
      "lr_config = dict(\n",
      "    policy='step',\n",
      "    step=[32, 48],\n",
      "    warmup='linear',\n",
      "    warmup_ratio=0.1,\n",
      "    warmup_by_epoch=True,\n",
      "    warmup_iters=16)\n",
      "total_epochs = 50\n",
      "work_dir = './childact-checkpoints/CSN-gender-wave'\n",
      "find_unused_parameters = True\n",
      "omnisource = False\n",
      "seed = 42\n",
      "gpu_ids = range(0, 1)\n",
      "output_config = dict(out='./childact-checkpoints/CSN-gender-wave/results.json')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mmcv.runner import set_random_seed\n",
    "\n",
    "# Modify dataset type and path\n",
    "# cfg.dataset_type = 'VideoDataset'\n",
    "cfg.data_root = 'age-gender-3split-rgb-frames/'\n",
    "cfg.data_root_val = 'age-gender-3split-rgb-frames/val/'\n",
    "cfg.ann_file_train = 'age-gender-3split-rgb-frames/childact_train_rgb320_gender_wawe.txt'\n",
    "cfg.ann_file_val = 'age-gender-3split-rgb-frames/childact_val_rgb320_gender_wawe.txt'\n",
    "cfg.ann_file_test = 'age-gender-3split-rgb-frames/childact_test_rgb320_gender_wawe.txt'\n",
    "\n",
    "# cfg.data.test.type = 'VideoDataset'\n",
    "cfg.data.test.ann_file = 'age-gender-3split-rgb-frames/childact_test_rgb320_gender_wawe.txt'\n",
    "cfg.data.test.data_prefix = 'age-gender-3split-rgb-frames/test/'\n",
    "\n",
    "# cfg.data.train.type = 'VideoDataset'\n",
    "cfg.data.train.ann_file = 'age-gender-3split-rgb-frames/childact_train_rgb320_gender_wawe.txt'\n",
    "cfg.data.train.data_prefix = 'age-gender-3split-rgb-frames/train/'\n",
    "\n",
    "# cfg.data.val.type = 'VideoDataset'\n",
    "cfg.data.val.ann_file = 'age-gender-3split-rgb-frames/childact_val_rgb320_gender_wawe.txt'\n",
    "cfg.data.val.data_prefix = 'age-gender-3split-rgb-frames/val/'\n",
    "\n",
    "# cfg.data.test.modality = 'Flow'\n",
    "# cfg.data.val.modality = 'Flow'\n",
    "# cfg.data.train.modality = 'Flow'\n",
    "\n",
    "# cfg.data.train.start_index = 0\n",
    "# cfg.data.test.start_index = 0\n",
    "# cfg.data.val.start_index = 0\n",
    "\n",
    "cfg.data.test.filename_tmpl = '{:03}.jpeg'\n",
    "cfg.data.train.filename_tmpl = '{:03}.jpeg'\n",
    "cfg.data.val.filename_tmpl = '{:03}.jpeg'\n",
    "# The flag is used to determine whether it is omnisource training\n",
    "cfg.setdefault('omnisource', False)\n",
    "# Modify num classes of the model in cls_head\n",
    "cfg.model.cls_head.num_classes = 2\n",
    "# We can use the pre-trained TSN model\n",
    "cfg.load_from = 'checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth'\n",
    "# cfg.resume_from = './childact-mm/latest.pth'\n",
    "\n",
    "# Set up working dir to save files and logs.\n",
    "cfg.work_dir = './childact-checkpoints/CSN-gender-wave'\n",
    "\n",
    "cfg.total_epochs = 50\n",
    "\n",
    "# cfg.momentum_config = dict(\n",
    "#     policy='cyclic',\n",
    "#     target_ratio=(0.85 / 0.95, 1),\n",
    "#     cyclic_times=1,\n",
    "#     step_ratio_up=0.4,\n",
    "# )\n",
    "\n",
    "# We can set the checkpoint saving interval to reduce the storage cost\n",
    "cfg.checkpoint_config.interval = 20\n",
    "# We can set the log print interval to reduce the the times of printing log\n",
    "cfg.log_config.interval = 100\n",
    "\n",
    "# Set seed thus the results are more reproducible\n",
    "cfg.seed = 42\n",
    "set_random_seed(42, deterministic=False)\n",
    "cfg.gpu_ids = range(1)\n",
    "\n",
    "cfg.data.videos_per_gpu=6\n",
    "\n",
    "# cfg.model.backbone.in_channels = 2\n",
    "\n",
    "cfg.output_config = dict(out=f'{cfg.work_dir}/results.json')\n",
    "# We can initialize the logger for training and have a look\n",
    "# at the final config used for training\n",
    "# del cfg.optimizer['momentum']\n",
    "print(f'Config:\\n{cfg.pretty_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd7bb9ca-31f9-44f8-a93e-c59dde096b7c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "dDBWkdDRk6oz",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "85a52ef3-7b5c-4c52-8fef-00322a8c65e6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 11:08:49,362 - mmaction - INFO - load model from: https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth\n",
      "2021-08-31 11:08:49,363 - mmaction - INFO - Use load_from_http loader\n",
      "2021-08-31 11:08:51,459 - mmaction - INFO - load checkpoint from checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth\n",
      "2021-08-31 11:08:51,460 - mmaction - INFO - Use load_from_local loader\n",
      "2021-08-31 11:08:51,617 - mmaction - WARNING - The model and loaded state dict do not match exactly\n",
      "\n",
      "size mismatch for cls_head.fc_cls.weight: copying a param with shape torch.Size([400, 2048]) from checkpoint, the shape in current model is torch.Size([2, 2048]).\n",
      "size mismatch for cls_head.fc_cls.bias: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([2]).\n",
      "2021-08-31 11:08:51,619 - mmaction - INFO - Start running, host: robt427nv@robt427NV, work_dir: /home/robt427nv/childact/childact-checkpoints/CSN-gender-wave\n",
      "2021-08-31 11:08:51,620 - mmaction - INFO - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(ABOVE_NORMAL) OptimizerHook                      \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "2021-08-31 11:08:51,620 - mmaction - INFO - workflow: [('train', 1)], max: 50 epochs\n",
      "/home/robt427nv/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/mmcv/runner/hooks/evaluation.py:190: UserWarning: runner.meta is None. Creating an empty one.\n",
      "  warnings.warn('runner.meta is None. Creating an empty one.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 17/17, 10.4 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 11:10:55,164 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 11:10:55,166 - mmaction - INFO - \n",
      "top1_acc\t0.8824\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 11:10:55,166 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 11:10:55,167 - mmaction - INFO - \n",
      "mean_acc\t0.8786\n",
      "2021-08-31 11:10:55,492 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_5.pth.\n",
      "2021-08-31 11:10:55,493 - mmaction - INFO - Best top1_acc is 0.8824 at 5 epoch.\n",
      "2021-08-31 11:10:55,494 - mmaction - INFO - Epoch(val) [5][3]\ttop1_acc: 0.8824, top5_acc: 1.0000, mean_class_accuracy: 0.8786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 17/17, 9.7 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 11:12:59,926 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 11:12:59,928 - mmaction - INFO - \n",
      "top1_acc\t0.8235\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 11:12:59,929 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 11:12:59,930 - mmaction - INFO - \n",
      "mean_acc\t0.8071\n",
      "2021-08-31 11:12:59,931 - mmaction - INFO - Epoch(val) [10][3]\ttop1_acc: 0.8235, top5_acc: 1.0000, mean_class_accuracy: 0.8071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 17/17, 9.7 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 11:15:04,540 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 11:15:04,542 - mmaction - INFO - \n",
      "top1_acc\t0.7647\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 11:15:04,543 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 11:15:04,544 - mmaction - INFO - \n",
      "mean_acc\t0.8000\n",
      "2021-08-31 11:15:04,544 - mmaction - INFO - Epoch(val) [15][3]\ttop1_acc: 0.7647, top5_acc: 1.0000, mean_class_accuracy: 0.8000\n",
      "2021-08-31 11:17:08,138 - mmaction - INFO - Saving checkpoint at 20 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 17/17, 10.0 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 11:17:10,244 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 11:17:10,246 - mmaction - INFO - \n",
      "top1_acc\t0.8824\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 11:17:10,246 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 11:17:10,247 - mmaction - INFO - \n",
      "mean_acc\t0.8786\n",
      "2021-08-31 11:17:10,248 - mmaction - INFO - Epoch(val) [20][3]\ttop1_acc: 0.8824, top5_acc: 1.0000, mean_class_accuracy: 0.8786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 17/17, 9.8 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 11:19:15,240 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 11:19:15,242 - mmaction - INFO - \n",
      "top1_acc\t0.8824\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 11:19:15,243 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 11:19:15,244 - mmaction - INFO - \n",
      "mean_acc\t0.8786\n",
      "2021-08-31 11:19:15,245 - mmaction - INFO - Epoch(val) [25][3]\ttop1_acc: 0.8824, top5_acc: 1.0000, mean_class_accuracy: 0.8786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 17/17, 10.1 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 11:21:20,778 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 11:21:20,779 - mmaction - INFO - \n",
      "top1_acc\t0.8235\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 11:21:20,780 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 11:21:20,781 - mmaction - INFO - \n",
      "mean_acc\t0.8071\n",
      "2021-08-31 11:21:20,782 - mmaction - INFO - Epoch(val) [30][3]\ttop1_acc: 0.8235, top5_acc: 1.0000, mean_class_accuracy: 0.8071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 17/17, 9.7 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 11:23:26,072 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 11:23:26,074 - mmaction - INFO - \n",
      "top1_acc\t0.9412\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 11:23:26,075 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 11:23:26,076 - mmaction - INFO - \n",
      "mean_acc\t0.9500\n",
      "2021-08-31 11:23:26,429 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_35.pth.\n",
      "2021-08-31 11:23:26,430 - mmaction - INFO - Best top1_acc is 0.9412 at 35 epoch.\n",
      "2021-08-31 11:23:26,431 - mmaction - INFO - Epoch(val) [35][3]\ttop1_acc: 0.9412, top5_acc: 1.0000, mean_class_accuracy: 0.9500\n",
      "2021-08-31 11:25:29,926 - mmaction - INFO - Saving checkpoint at 40 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 17/17, 9.8 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 11:25:32,124 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 11:25:32,126 - mmaction - INFO - \n",
      "top1_acc\t0.9412\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 11:25:32,126 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 11:25:32,128 - mmaction - INFO - \n",
      "mean_acc\t0.9500\n",
      "2021-08-31 11:25:32,128 - mmaction - INFO - Epoch(val) [40][3]\ttop1_acc: 0.9412, top5_acc: 1.0000, mean_class_accuracy: 0.9500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 17/17, 10.1 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 11:27:37,005 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 11:27:37,007 - mmaction - INFO - \n",
      "top1_acc\t0.8824\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 11:27:37,007 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 11:27:37,008 - mmaction - INFO - \n",
      "mean_acc\t0.8786\n",
      "2021-08-31 11:27:37,009 - mmaction - INFO - Epoch(val) [45][3]\ttop1_acc: 0.8824, top5_acc: 1.0000, mean_class_accuracy: 0.8786\n",
      "2021-08-31 11:29:40,119 - mmaction - INFO - Saving checkpoint at 50 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 17/17, 9.9 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 11:29:42,191 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 11:29:42,193 - mmaction - INFO - \n",
      "top1_acc\t0.8824\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 11:29:42,193 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 11:29:42,195 - mmaction - INFO - \n",
      "mean_acc\t0.8786\n",
      "2021-08-31 11:29:42,195 - mmaction - INFO - Epoch(val) [50][3]\ttop1_acc: 0.8824, top5_acc: 1.0000, mean_class_accuracy: 0.8786\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "from mmaction.datasets import build_dataset\n",
    "from mmaction.models import build_model\n",
    "from mmaction.apis import train_model\n",
    "\n",
    "import mmcv\n",
    "\n",
    "# Build the dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# Build the recognizer\n",
    "model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.train_cfg, test_cfg=cfg.test_cfg)\n",
    "# CUDA_LAUNCH_BLOCKING=1\n",
    "# Create work_dir\n",
    "mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
    "train_model(model, datasets, cfg, distributed=False, validate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe071eb7-8161-41d3-9f25-157fb2848ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f\"{cfg.work_dir}/model50e\", 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6def8ccc-c197-42f5-af13-7eeef667a65a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eyY3hCMwyTct",
    "outputId": "200e37c7-0da4-421f-98da-41418c3110ca",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 24/24, 0.2 task/s, elapsed: 112s, ETA:     0s\n",
      "Evaluating top_k_accuracy ...\n",
      "\n",
      "top1_acc\t0.9583\n",
      "top5_acc\t1.0000\n",
      "\n",
      "Evaluating mean_class_accuracy ...\n",
      "\n",
      "mean_acc\t0.9500\n",
      "top1_acc: 0.9583\n",
      "top5_acc: 1.0000\n",
      "mean_class_accuracy: 0.9500\n"
     ]
    }
   ],
   "source": [
    "from mmaction.apis import single_gpu_test\n",
    "from mmaction.datasets import build_dataloader\n",
    "from mmcv.parallel import MMDataParallel\n",
    "# from mmaction.models import build_model\n",
    "# from mmaction.datasets import build_dataset\n",
    "\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# Build a test dataloader\n",
    "dataset = build_dataset(cfg.data.test, dict(test_mode=True))\n",
    "data_loader = build_dataloader(\n",
    "        dataset,\n",
    "        videos_per_gpu=1,\n",
    "        workers_per_gpu=1,\n",
    "        dist=False,\n",
    "        shuffle=False)\n",
    "model = MMDataParallel(model, device_ids=[0])\n",
    "outputs = single_gpu_test(model, data_loader)\n",
    "\n",
    "eval_config = cfg.evaluation\n",
    "eval_config.pop('interval')\n",
    "eval_res = dataset.evaluate(outputs, **eval_config)\n",
    "for name, val in eval_res.items():\n",
    "    print(f'{name}: {val:.04f}')\n",
    "    \n",
    "output_config = cfg.output_config\n",
    "dataset.dump_results(outputs, **output_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "316415d4-b158-4b62-aa82-1756dd4959a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from mmaction.core import confusion_matrix \n",
    "\n",
    "gt_labels = [ann['label'] for ann in dataset.load_annotations()]\n",
    "pred = np.argmax(outputs, axis=1)\n",
    "cf_mat = confusion_matrix(pred, gt_labels).astype(float)\n",
    "cmap = sns.cubehelix_palette(50, hue=0.05, rot=0, light=0.9, dark=0, as_cmap=True)\n",
    "# /np.sum(cf_mat), fmt='.2%',\n",
    "# sns.heatmap(cf_mat, cmap=cmap, annot=True, xticklabels = ['6yo', '7yo', '8yo'], yticklabels = ['6yo', '7yo', '8yo'])\n",
    "sns.heatmap(cf_mat, cmap=cmap, annot=True, xticklabels = ['male','female'], yticklabels = ['male','female'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a92d7d-32f3-4e06-b03d-5fdbad1b20f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

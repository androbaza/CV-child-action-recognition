{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64b5ec5a-f125-4f1d-a286-14a5d4235ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/robt427nv/childact\n"
     ]
    }
   ],
   "source": [
    "cd /home/robt427nv/childact/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4f06af-f33e-4e37-a67e-6d85f8f39088",
   "metadata": {},
   "source": [
    "# CSN gender BOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8af680b2-4c74-4630-b539-c1e1ffa39fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/robt427nv/childact\n"
     ]
    }
   ],
   "source": [
    "cd /home/robt427nv/childact/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3c84914-7eeb-4730-a8a1-50e9ee02354e",
   "metadata": {
    "id": "LjCcmCKOjktc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mmcv import Config\n",
    "cfg = Config.fromfile('mmaction2/configs/recognition/csn/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "097ca45c-f34b-45bb-b933-e0e36d5fbb45",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "tlhu9byjjt-K",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "13d1bb01-f351-48c0-9efb-0bdf2c125d29",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "model = dict(\n",
      "    type='Recognizer3D',\n",
      "    backbone=dict(\n",
      "        type='ResNet3dCSN',\n",
      "        pretrained2d=False,\n",
      "        pretrained=\n",
      "        'https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth',\n",
      "        depth=152,\n",
      "        with_pool2=False,\n",
      "        bottleneck_mode='ir',\n",
      "        norm_eval=True,\n",
      "        zero_init_residual=False,\n",
      "        bn_frozen=True),\n",
      "    cls_head=dict(\n",
      "        type='I3DHead',\n",
      "        num_classes=2,\n",
      "        in_channels=2048,\n",
      "        spatial_type='avg',\n",
      "        dropout_ratio=0.5,\n",
      "        init_std=0.01),\n",
      "    train_cfg=None,\n",
      "    test_cfg=dict(average_clips='prob'))\n",
      "checkpoint_config = dict(interval=20)\n",
      "log_config = dict(interval=100, hooks=[dict(type='TextLoggerHook')])\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = 'checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth'\n",
      "resume_from = None\n",
      "workflow = [('train', 1)]\n",
      "dataset_type = 'RawframeDataset'\n",
      "data_root = 'age-gender-3split-rgb-frames/'\n",
      "data_root_val = 'age-gender-3split-rgb-frames/val/'\n",
      "ann_file_train = 'age-gender-3split-rgb-frames/childact_train_rgb320_gender_box.txt'\n",
      "ann_file_val = 'age-gender-3split-rgb-frames/childact_val_rgb320_gender_box.txt'\n",
      "ann_file_test = 'age-gender-3split-rgb-frames/childact_test_rgb320_gender_box.txt'\n",
      "img_norm_cfg = dict(\n",
      "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_bgr=False)\n",
      "train_pipeline = [\n",
      "    dict(type='SampleFrames', clip_len=32, frame_interval=2, num_clips=1),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='RandomResizedCrop'),\n",
      "    dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "    dict(type='Flip', flip_ratio=0.5),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "]\n",
      "val_pipeline = [\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=32,\n",
      "        frame_interval=2,\n",
      "        num_clips=1,\n",
      "        test_mode=True),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='CenterCrop', crop_size=224),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=32,\n",
      "        frame_interval=2,\n",
      "        num_clips=10,\n",
      "        test_mode=True),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='ThreeCrop', crop_size=256),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "data = dict(\n",
      "    videos_per_gpu=6,\n",
      "    workers_per_gpu=4,\n",
      "    train=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_train_rgb320_gender_box.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/train/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=1),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='RandomResizedCrop'),\n",
      "            dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "            dict(type='Flip', flip_ratio=0.5),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'),\n",
      "    val=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_val_rgb320_gender_box.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/val/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=1,\n",
      "                test_mode=True),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='CenterCrop', crop_size=224),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'),\n",
      "    test=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_test_rgb320_gender_box.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/test/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=10,\n",
      "                test_mode=True),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='ThreeCrop', crop_size=256),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'))\n",
      "evaluation = dict(\n",
      "    interval=5, metrics=['top_k_accuracy', 'mean_class_accuracy'])\n",
      "optimizer = dict(type='SGD', lr=0.000125, momentum=0.9, weight_decay=0.0001)\n",
      "optimizer_config = dict(grad_clip=dict(max_norm=40, norm_type=2))\n",
      "lr_config = dict(\n",
      "    policy='step',\n",
      "    step=[32, 48],\n",
      "    warmup='linear',\n",
      "    warmup_ratio=0.1,\n",
      "    warmup_by_epoch=True,\n",
      "    warmup_iters=16)\n",
      "total_epochs = 50\n",
      "work_dir = './childact-checkpoints/CSN-gender-box'\n",
      "find_unused_parameters = True\n",
      "omnisource = False\n",
      "seed = 42\n",
      "gpu_ids = range(0, 1)\n",
      "output_config = dict(out='./childact-checkpoints/CSN-gender-box/results.json')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mmcv.runner import set_random_seed\n",
    "\n",
    "# Modify dataset type and path\n",
    "# cfg.dataset_type = 'VideoDataset'\n",
    "cfg.data_root = 'age-gender-3split-rgb-frames/'\n",
    "cfg.data_root_val = 'age-gender-3split-rgb-frames/val/'\n",
    "cfg.ann_file_train = 'age-gender-3split-rgb-frames/childact_train_rgb320_gender_box.txt'\n",
    "cfg.ann_file_val = 'age-gender-3split-rgb-frames/childact_val_rgb320_gender_box.txt'\n",
    "cfg.ann_file_test = 'age-gender-3split-rgb-frames/childact_test_rgb320_gender_box.txt'\n",
    "\n",
    "# cfg.data.test.type = 'VideoDataset'\n",
    "cfg.data.test.ann_file = 'age-gender-3split-rgb-frames/childact_test_rgb320_gender_box.txt'\n",
    "cfg.data.test.data_prefix = 'age-gender-3split-rgb-frames/test/'\n",
    "\n",
    "# cfg.data.train.type = 'VideoDataset'\n",
    "cfg.data.train.ann_file = 'age-gender-3split-rgb-frames/childact_train_rgb320_gender_box.txt'\n",
    "cfg.data.train.data_prefix = 'age-gender-3split-rgb-frames/train/'\n",
    "\n",
    "# cfg.data.val.type = 'VideoDataset'\n",
    "cfg.data.val.ann_file = 'age-gender-3split-rgb-frames/childact_val_rgb320_gender_box.txt'\n",
    "cfg.data.val.data_prefix = 'age-gender-3split-rgb-frames/val/'\n",
    "\n",
    "# cfg.data.test.modality = 'Flow'\n",
    "# cfg.data.val.modality = 'Flow'\n",
    "# cfg.data.train.modality = 'Flow'\n",
    "\n",
    "# cfg.data.train.start_index = 0\n",
    "# cfg.data.test.start_index = 0\n",
    "# cfg.data.val.start_index = 0\n",
    "\n",
    "cfg.data.test.filename_tmpl = '{:03}.jpeg'\n",
    "cfg.data.train.filename_tmpl = '{:03}.jpeg'\n",
    "cfg.data.val.filename_tmpl = '{:03}.jpeg'\n",
    "# The flag is used to determine whether it is omnisource training\n",
    "cfg.setdefault('omnisource', False)\n",
    "# Modify num classes of the model in cls_head\n",
    "cfg.model.cls_head.num_classes = 2\n",
    "# We can use the pre-trained TSN model\n",
    "cfg.load_from = 'checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth'\n",
    "# cfg.resume_from = './childact-mm/latest.pth'\n",
    "\n",
    "# Set up working dir to save files and logs.\n",
    "cfg.work_dir = './childact-checkpoints/CSN-gender-box'\n",
    "\n",
    "cfg.total_epochs = 50\n",
    "\n",
    "# cfg.momentum_config = dict(\n",
    "#     policy='cyclic',\n",
    "#     target_ratio=(0.85 / 0.95, 1),\n",
    "#     cyclic_times=1,\n",
    "#     step_ratio_up=0.4,\n",
    "# )\n",
    "\n",
    "# We can set the checkpoint saving interval to reduce the storage cost\n",
    "cfg.checkpoint_config.interval = 20\n",
    "# We can set the log print interval to reduce the the times of printing log\n",
    "cfg.log_config.interval = 100\n",
    "\n",
    "# Set seed thus the results are more reproducible\n",
    "cfg.seed = 42\n",
    "set_random_seed(42, deterministic=False)\n",
    "cfg.gpu_ids = range(1)\n",
    "\n",
    "cfg.data.videos_per_gpu=6\n",
    "\n",
    "# cfg.model.backbone.in_channels = 2\n",
    "\n",
    "cfg.output_config = dict(out=f'{cfg.work_dir}/results.json')\n",
    "# We can initialize the logger for training and have a look\n",
    "# at the final config used for training\n",
    "# del cfg.optimizer['momentum']\n",
    "print(f'Config:\\n{cfg.pretty_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ca8d7c9-7adf-4080-9134-201eeafd2791",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "dDBWkdDRk6oz",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "85a52ef3-7b5c-4c52-8fef-00322a8c65e6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-30 23:48:31,331 - mmaction - INFO - load model from: https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth\n",
      "2021-08-30 23:48:31,332 - mmaction - INFO - Use load_from_http loader\n",
      "2021-08-30 23:48:33,419 - mmaction - INFO - load checkpoint from checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth\n",
      "2021-08-30 23:48:33,420 - mmaction - INFO - Use load_from_local loader\n",
      "2021-08-30 23:48:33,580 - mmaction - WARNING - The model and loaded state dict do not match exactly\n",
      "\n",
      "size mismatch for cls_head.fc_cls.weight: copying a param with shape torch.Size([400, 2048]) from checkpoint, the shape in current model is torch.Size([2, 2048]).\n",
      "size mismatch for cls_head.fc_cls.bias: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([2]).\n",
      "2021-08-30 23:48:33,587 - mmaction - INFO - Start running, host: robt427nv@robt427NV, work_dir: /home/robt427nv/childact/childact-checkpoints/CSN-gender-box\n",
      "2021-08-30 23:48:33,588 - mmaction - INFO - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(ABOVE_NORMAL) OptimizerHook                      \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "2021-08-30 23:48:33,588 - mmaction - INFO - workflow: [('train', 1)], max: 50 epochs\n",
      "/home/robt427nv/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/mmcv/runner/hooks/evaluation.py:190: UserWarning: runner.meta is None. Creating an empty one.\n",
      "  warnings.warn('runner.meta is None. Creating an empty one.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 11/11, 8.5 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-30 23:50:23,008 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-30 23:50:23,011 - mmaction - INFO - \n",
      "top1_acc\t0.9091\n",
      "top5_acc\t1.0000\n",
      "2021-08-30 23:50:23,011 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-30 23:50:23,013 - mmaction - INFO - \n",
      "mean_acc\t0.9286\n",
      "2021-08-30 23:50:23,339 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_5.pth.\n",
      "2021-08-30 23:50:23,340 - mmaction - INFO - Best top1_acc is 0.9091 at 5 epoch.\n",
      "2021-08-30 23:50:23,341 - mmaction - INFO - Epoch(val) [5][2]\ttop1_acc: 0.9091, top5_acc: 1.0000, mean_class_accuracy: 0.9286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 11/11, 9.0 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-30 23:52:11,356 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-30 23:52:11,357 - mmaction - INFO - \n",
      "top1_acc\t0.9091\n",
      "top5_acc\t1.0000\n",
      "2021-08-30 23:52:11,358 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-30 23:52:11,358 - mmaction - INFO - \n",
      "mean_acc\t0.9286\n",
      "2021-08-30 23:52:11,359 - mmaction - INFO - Epoch(val) [10][2]\ttop1_acc: 0.9091, top5_acc: 1.0000, mean_class_accuracy: 0.9286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 11/11, 8.9 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-30 23:53:59,660 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-30 23:53:59,661 - mmaction - INFO - \n",
      "top1_acc\t0.8182\n",
      "top5_acc\t1.0000\n",
      "2021-08-30 23:53:59,662 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-30 23:53:59,663 - mmaction - INFO - \n",
      "mean_acc\t0.8036\n",
      "2021-08-30 23:53:59,664 - mmaction - INFO - Epoch(val) [15][2]\ttop1_acc: 0.8182, top5_acc: 1.0000, mean_class_accuracy: 0.8036\n",
      "2021-08-30 23:55:47,107 - mmaction - INFO - Saving checkpoint at 20 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 11/11, 8.6 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-30 23:55:48,790 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-30 23:55:48,792 - mmaction - INFO - \n",
      "top1_acc\t0.7273\n",
      "top5_acc\t1.0000\n",
      "2021-08-30 23:55:48,792 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-30 23:55:48,794 - mmaction - INFO - \n",
      "mean_acc\t0.6786\n",
      "2021-08-30 23:55:48,794 - mmaction - INFO - Epoch(val) [20][2]\ttop1_acc: 0.7273, top5_acc: 1.0000, mean_class_accuracy: 0.6786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 11/11, 9.0 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-30 23:57:37,899 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-30 23:57:37,901 - mmaction - INFO - \n",
      "top1_acc\t0.7273\n",
      "top5_acc\t1.0000\n",
      "2021-08-30 23:57:37,902 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-30 23:57:37,903 - mmaction - INFO - \n",
      "mean_acc\t0.6786\n",
      "2021-08-30 23:57:37,903 - mmaction - INFO - Epoch(val) [25][2]\ttop1_acc: 0.7273, top5_acc: 1.0000, mean_class_accuracy: 0.6786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 11/11, 8.8 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-30 23:59:26,839 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-30 23:59:26,841 - mmaction - INFO - \n",
      "top1_acc\t0.9091\n",
      "top5_acc\t1.0000\n",
      "2021-08-30 23:59:26,842 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-30 23:59:26,843 - mmaction - INFO - \n",
      "mean_acc\t0.9286\n",
      "2021-08-30 23:59:26,844 - mmaction - INFO - Epoch(val) [30][2]\ttop1_acc: 0.9091, top5_acc: 1.0000, mean_class_accuracy: 0.9286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 11/11, 9.1 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 00:01:15,401 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 00:01:15,402 - mmaction - INFO - \n",
      "top1_acc\t0.8182\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 00:01:15,403 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 00:01:15,404 - mmaction - INFO - \n",
      "mean_acc\t0.8036\n",
      "2021-08-31 00:01:15,404 - mmaction - INFO - Epoch(val) [35][2]\ttop1_acc: 0.8182, top5_acc: 1.0000, mean_class_accuracy: 0.8036\n",
      "2021-08-31 00:03:02,825 - mmaction - INFO - Saving checkpoint at 40 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 11/11, 8.7 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 00:03:04,503 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 00:03:04,506 - mmaction - INFO - \n",
      "top1_acc\t0.9091\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 00:03:04,507 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 00:03:04,509 - mmaction - INFO - \n",
      "mean_acc\t0.9286\n",
      "2021-08-31 00:03:04,510 - mmaction - INFO - Epoch(val) [40][2]\ttop1_acc: 0.9091, top5_acc: 1.0000, mean_class_accuracy: 0.9286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 11/11, 8.9 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 00:04:53,694 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 00:04:53,696 - mmaction - INFO - \n",
      "top1_acc\t0.9091\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 00:04:53,697 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 00:04:53,697 - mmaction - INFO - \n",
      "mean_acc\t0.9286\n",
      "2021-08-31 00:04:53,698 - mmaction - INFO - Epoch(val) [45][2]\ttop1_acc: 0.9091, top5_acc: 1.0000, mean_class_accuracy: 0.9286\n",
      "2021-08-31 00:06:41,217 - mmaction - INFO - Saving checkpoint at 50 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 11/11, 8.5 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 00:06:42,902 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 00:06:42,903 - mmaction - INFO - \n",
      "top1_acc\t0.9091\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 00:06:42,903 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 00:06:42,904 - mmaction - INFO - \n",
      "mean_acc\t0.9286\n",
      "2021-08-31 00:06:42,905 - mmaction - INFO - Epoch(val) [50][2]\ttop1_acc: 0.9091, top5_acc: 1.0000, mean_class_accuracy: 0.9286\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "from mmaction.datasets import build_dataset\n",
    "from mmaction.models import build_model\n",
    "from mmaction.apis import train_model\n",
    "\n",
    "import mmcv\n",
    "\n",
    "# Build the dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# Build the recognizer\n",
    "model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.train_cfg, test_cfg=cfg.test_cfg)\n",
    "# CUDA_LAUNCH_BLOCKING=1\n",
    "# Create work_dir\n",
    "mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
    "train_model(model, datasets, cfg, distributed=False, validate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28671ef7-0902-4c5c-b137-0261341d6b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f\"{cfg.work_dir}/model50e\", 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3776a3ca-9d38-4efa-8c6c-b20e907da4ae",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 16:54:18,016 - mmaction - INFO - load model from: https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth\n",
      "2021-08-10 16:54:18,017 - mmaction - INFO - Use load_from_http loader\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "from mmaction.datasets import build_dataset\n",
    "from mmaction.models import build_model\n",
    "from mmaction.apis import train_model\n",
    "import pickle\n",
    "import mmcv\n",
    "# Build the dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# Build the recognizer\n",
    "model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "\n",
    "model = pickle.load(open(f\"{cfg.work_dir}/model50e\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a14875a1-8a85-4af0-a54a-6d54cdeb7c9e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eyY3hCMwyTct",
    "outputId": "200e37c7-0da4-421f-98da-41418c3110ca",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 8/8, 0.5 task/s, elapsed: 15s, ETA:     0s\n",
      "Evaluating top_k_accuracy ...\n",
      "\n",
      "top1_acc\t1.0000\n",
      "top5_acc\t1.0000\n",
      "\n",
      "Evaluating mean_class_accuracy ...\n",
      "\n",
      "mean_acc\t1.0000\n",
      "top1_acc: 1.0000\n",
      "top5_acc: 1.0000\n",
      "mean_class_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from mmaction.apis import single_gpu_test\n",
    "from mmaction.datasets import build_dataloader\n",
    "from mmcv.parallel import MMDataParallel\n",
    "# from mmaction.models import build_model\n",
    "# from mmaction.datasets import build_dataset\n",
    "\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# Build a test dataloader\n",
    "dataset = build_dataset(cfg.data.test, dict(test_mode=True))\n",
    "data_loader = build_dataloader(\n",
    "        dataset,\n",
    "        videos_per_gpu=1,\n",
    "        workers_per_gpu=1,\n",
    "        dist=False,\n",
    "        shuffle=False)\n",
    "model = MMDataParallel(model, device_ids=[0])\n",
    "outputs = single_gpu_test(model, data_loader)\n",
    "\n",
    "eval_config = cfg.evaluation\n",
    "eval_config.pop('interval')\n",
    "eval_res = dataset.evaluate(outputs, **eval_config)\n",
    "for name, val in eval_res.items():\n",
    "    print(f'{name}: {val:.04f}')\n",
    "    \n",
    "output_config = cfg.output_config\n",
    "dataset.dump_results(outputs, **output_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0994ae1f-adaa-4118-a0c9-709514bda2a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from mmaction.core import confusion_matrix \n",
    "\n",
    "gt_labels = [ann['label'] for ann in dataset.load_annotations()]\n",
    "pred = np.argmax(outputs, axis=1)\n",
    "cf_mat = confusion_matrix(pred, gt_labels).astype(float)\n",
    "cmap = sns.cubehelix_palette(50, hue=0.05, rot=0, light=0.9, dark=0, as_cmap=True)\n",
    "# /np.sum(cf_mat), fmt='.2%',\n",
    "# sns.heatmap(cf_mat, cmap=cmap, annot=True, xticklabels = ['6yo', '7yo', '8yo'], yticklabels = ['6yo', '7yo', '8yo'])\n",
    "sns.heatmap(cf_mat, cmap=cmap, annot=True, xticklabels = ['male','female'], yticklabels = ['male','female'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00b8100-8cbd-46ea-86d4-1a4743c197d6",
   "metadata": {},
   "source": [
    "# CSN gender CLAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "766557c8-a1e4-4df6-896f-872b7c659fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/robt427nv/childact\n"
     ]
    }
   ],
   "source": [
    "cd /home/robt427nv/childact/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b15232f-8ad3-4b69-819f-e8bdd4c8c435",
   "metadata": {
    "id": "LjCcmCKOjktc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mmcv import Config\n",
    "cfg = Config.fromfile('mmaction2/configs/recognition/csn/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a73424d-0824-4d90-9aad-f86f6c8fb0fd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "tlhu9byjjt-K",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "13d1bb01-f351-48c0-9efb-0bdf2c125d29",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "model = dict(\n",
      "    type='Recognizer3D',\n",
      "    backbone=dict(\n",
      "        type='ResNet3dCSN',\n",
      "        pretrained2d=False,\n",
      "        pretrained=\n",
      "        'https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth',\n",
      "        depth=152,\n",
      "        with_pool2=False,\n",
      "        bottleneck_mode='ir',\n",
      "        norm_eval=True,\n",
      "        zero_init_residual=False,\n",
      "        bn_frozen=True),\n",
      "    cls_head=dict(\n",
      "        type='I3DHead',\n",
      "        num_classes=2,\n",
      "        in_channels=2048,\n",
      "        spatial_type='avg',\n",
      "        dropout_ratio=0.5,\n",
      "        init_std=0.01),\n",
      "    train_cfg=None,\n",
      "    test_cfg=dict(average_clips='prob'))\n",
      "checkpoint_config = dict(interval=20)\n",
      "log_config = dict(interval=100, hooks=[dict(type='TextLoggerHook')])\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = 'checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth'\n",
      "resume_from = None\n",
      "workflow = [('train', 1)]\n",
      "dataset_type = 'RawframeDataset'\n",
      "data_root = 'age-gender-3split-rgb-frames/'\n",
      "data_root_val = 'age-gender-3split-rgb-frames/val/'\n",
      "ann_file_train = 'age-gender-3split-rgb-frames/childact_train_rgb320_gender_clap.txt'\n",
      "ann_file_val = 'age-gender-3split-rgb-frames/childact_val_rgb320_gender_clap.txt'\n",
      "ann_file_test = 'age-gender-3split-rgb-frames/childact_test_rgb320_gender_clap.txt'\n",
      "img_norm_cfg = dict(\n",
      "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_bgr=False)\n",
      "train_pipeline = [\n",
      "    dict(type='SampleFrames', clip_len=32, frame_interval=2, num_clips=1),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='RandomResizedCrop'),\n",
      "    dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "    dict(type='Flip', flip_ratio=0.5),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "]\n",
      "val_pipeline = [\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=32,\n",
      "        frame_interval=2,\n",
      "        num_clips=1,\n",
      "        test_mode=True),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='CenterCrop', crop_size=224),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=32,\n",
      "        frame_interval=2,\n",
      "        num_clips=10,\n",
      "        test_mode=True),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='ThreeCrop', crop_size=256),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "data = dict(\n",
      "    videos_per_gpu=6,\n",
      "    workers_per_gpu=4,\n",
      "    train=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_train_rgb320_gender_clap.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/train/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=1),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='RandomResizedCrop'),\n",
      "            dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "            dict(type='Flip', flip_ratio=0.5),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'),\n",
      "    val=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_val_rgb320_gender_clap.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/val/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=1,\n",
      "                test_mode=True),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='CenterCrop', crop_size=224),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'),\n",
      "    test=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_test_rgb320_gender_clap.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/test/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=10,\n",
      "                test_mode=True),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='ThreeCrop', crop_size=256),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'))\n",
      "evaluation = dict(\n",
      "    interval=5, metrics=['top_k_accuracy', 'mean_class_accuracy'])\n",
      "optimizer = dict(type='SGD', lr=0.000125, momentum=0.9, weight_decay=0.0001)\n",
      "optimizer_config = dict(grad_clip=dict(max_norm=40, norm_type=2))\n",
      "lr_config = dict(\n",
      "    policy='step',\n",
      "    step=[32, 48],\n",
      "    warmup='linear',\n",
      "    warmup_ratio=0.1,\n",
      "    warmup_by_epoch=True,\n",
      "    warmup_iters=16)\n",
      "total_epochs = 50\n",
      "work_dir = './childact-checkpoints/CSN-gender-clap'\n",
      "find_unused_parameters = True\n",
      "omnisource = False\n",
      "seed = 42\n",
      "gpu_ids = range(0, 1)\n",
      "output_config = dict(out='./childact-checkpoints/CSN-gender-clap/results.json')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mmcv.runner import set_random_seed\n",
    "\n",
    "# Modify dataset type and path\n",
    "# cfg.dataset_type = 'VideoDataset'\n",
    "cfg.data_root = 'age-gender-3split-rgb-frames/'\n",
    "cfg.data_root_val = 'age-gender-3split-rgb-frames/val/'\n",
    "cfg.ann_file_train = 'age-gender-3split-rgb-frames/childact_train_rgb320_gender_clap.txt'\n",
    "cfg.ann_file_val = 'age-gender-3split-rgb-frames/childact_val_rgb320_gender_clap.txt'\n",
    "cfg.ann_file_test = 'age-gender-3split-rgb-frames/childact_test_rgb320_gender_clap.txt'\n",
    "\n",
    "# cfg.data.test.type = 'VideoDataset'\n",
    "cfg.data.test.ann_file = 'age-gender-3split-rgb-frames/childact_test_rgb320_gender_clap.txt'\n",
    "cfg.data.test.data_prefix = 'age-gender-3split-rgb-frames/test/'\n",
    "\n",
    "# cfg.data.train.type = 'VideoDataset'\n",
    "cfg.data.train.ann_file = 'age-gender-3split-rgb-frames/childact_train_rgb320_gender_clap.txt'\n",
    "cfg.data.train.data_prefix = 'age-gender-3split-rgb-frames/train/'\n",
    "\n",
    "# cfg.data.val.type = 'VideoDataset'\n",
    "cfg.data.val.ann_file = 'age-gender-3split-rgb-frames/childact_val_rgb320_gender_clap.txt'\n",
    "cfg.data.val.data_prefix = 'age-gender-3split-rgb-frames/val/'\n",
    "\n",
    "# cfg.data.test.modality = 'Flow'\n",
    "# cfg.data.val.modality = 'Flow'\n",
    "# cfg.data.train.modality = 'Flow'\n",
    "\n",
    "# cfg.data.train.start_index = 0\n",
    "# cfg.data.test.start_index = 0\n",
    "# cfg.data.val.start_index = 0\n",
    "\n",
    "cfg.data.test.filename_tmpl = '{:03}.jpeg'\n",
    "cfg.data.train.filename_tmpl = '{:03}.jpeg'\n",
    "cfg.data.val.filename_tmpl = '{:03}.jpeg'\n",
    "# The flag is used to determine whether it is omnisource training\n",
    "cfg.setdefault('omnisource', False)\n",
    "# Modify num classes of the model in cls_head\n",
    "cfg.model.cls_head.num_classes = 2\n",
    "# We can use the pre-trained TSN model\n",
    "cfg.load_from = 'checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth'\n",
    "# cfg.resume_from = './childact-mm/latest.pth'\n",
    "\n",
    "# Set up working dir to save files and logs.\n",
    "cfg.work_dir = './childact-checkpoints/CSN-gender-clap'\n",
    "\n",
    "cfg.total_epochs = 50\n",
    "\n",
    "# cfg.momentum_config = dict(\n",
    "#     policy='cyclic',\n",
    "#     target_ratio=(0.85 / 0.95, 1),\n",
    "#     cyclic_times=1,\n",
    "#     step_ratio_up=0.4,\n",
    "# )\n",
    "\n",
    "# We can set the checkpoint saving interval to reduce the storage cost\n",
    "cfg.checkpoint_config.interval = 20\n",
    "# We can set the log print interval to reduce the the times of printing log\n",
    "cfg.log_config.interval = 100\n",
    "\n",
    "# Set seed thus the results are more reproducible\n",
    "cfg.seed = 42\n",
    "set_random_seed(42, deterministic=False)\n",
    "cfg.gpu_ids = range(1)\n",
    "\n",
    "cfg.data.videos_per_gpu=6\n",
    "\n",
    "# cfg.model.backbone.in_channels = 2\n",
    "\n",
    "cfg.output_config = dict(out=f'{cfg.work_dir}/results.json')\n",
    "# We can initialize the logger for training and have a look\n",
    "# at the final config used for training\n",
    "# del cfg.optimizer['momentum']\n",
    "print(f'Config:\\n{cfg.pretty_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b895d9-4649-4bb0-9b72-0e08f66f30f7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dDBWkdDRk6oz",
    "outputId": "85a52ef3-7b5c-4c52-8fef-00322a8c65e6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 00:11:25,322 - mmaction - INFO - load model from: https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth\n",
      "2021-08-31 00:11:25,323 - mmaction - INFO - Use load_from_http loader\n",
      "2021-08-31 00:11:27,490 - mmaction - INFO - load checkpoint from checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth\n",
      "2021-08-31 00:11:27,491 - mmaction - INFO - Use load_from_local loader\n",
      "2021-08-31 00:11:27,634 - mmaction - WARNING - The model and loaded state dict do not match exactly\n",
      "\n",
      "size mismatch for cls_head.fc_cls.weight: copying a param with shape torch.Size([400, 2048]) from checkpoint, the shape in current model is torch.Size([2, 2048]).\n",
      "size mismatch for cls_head.fc_cls.bias: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([2]).\n",
      "2021-08-31 00:11:27,640 - mmaction - INFO - Start running, host: robt427nv@robt427NV, work_dir: /home/robt427nv/childact/childact-checkpoints/CSN-gender-clap\n",
      "2021-08-31 00:11:27,641 - mmaction - INFO - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(ABOVE_NORMAL) OptimizerHook                      \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "2021-08-31 00:11:27,641 - mmaction - INFO - workflow: [('train', 1)], max: 50 epochs\n",
      "/home/robt427nv/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/mmcv/runner/hooks/evaluation.py:190: UserWarning: runner.meta is None. Creating an empty one.\n",
      "  warnings.warn('runner.meta is None. Creating an empty one.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 14/14, 9.5 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 00:13:39,626 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 00:13:39,627 - mmaction - INFO - \n",
      "top1_acc\t0.7857\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 00:13:39,628 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 00:13:39,629 - mmaction - INFO - \n",
      "mean_acc\t0.8125\n",
      "2021-08-31 00:13:39,942 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_5.pth.\n",
      "2021-08-31 00:13:39,943 - mmaction - INFO - Best top1_acc is 0.7857 at 5 epoch.\n",
      "2021-08-31 00:13:39,944 - mmaction - INFO - Epoch(val) [5][3]\ttop1_acc: 0.7857, top5_acc: 1.0000, mean_class_accuracy: 0.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 14/14, 10.1 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 00:15:51,861 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 00:15:51,862 - mmaction - INFO - \n",
      "top1_acc\t0.7857\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 00:15:51,863 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 00:15:51,864 - mmaction - INFO - \n",
      "mean_acc\t0.7917\n",
      "2021-08-31 00:15:51,865 - mmaction - INFO - Epoch(val) [10][3]\ttop1_acc: 0.7857, top5_acc: 1.0000, mean_class_accuracy: 0.7917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 14/14, 9.7 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 00:18:00,494 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 00:18:00,496 - mmaction - INFO - \n",
      "top1_acc\t0.7857\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 00:18:00,497 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 00:18:00,498 - mmaction - INFO - \n",
      "mean_acc\t0.7917\n",
      "2021-08-31 00:18:00,500 - mmaction - INFO - Epoch(val) [15][3]\ttop1_acc: 0.7857, top5_acc: 1.0000, mean_class_accuracy: 0.7917\n",
      "2021-08-31 00:20:07,685 - mmaction - INFO - Saving checkpoint at 20 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 14/14, 9.1 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 00:20:09,620 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 00:20:09,621 - mmaction - INFO - \n",
      "top1_acc\t0.7143\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 00:20:09,623 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 00:20:09,624 - mmaction - INFO - \n",
      "mean_acc\t0.7500\n",
      "2021-08-31 00:20:09,625 - mmaction - INFO - Epoch(val) [20][3]\ttop1_acc: 0.7143, top5_acc: 1.0000, mean_class_accuracy: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 14/14, 9.5 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 00:22:18,429 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 00:22:18,430 - mmaction - INFO - \n",
      "top1_acc\t0.7857\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 00:22:18,431 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 00:22:18,432 - mmaction - INFO - \n",
      "mean_acc\t0.8125\n",
      "2021-08-31 00:22:18,433 - mmaction - INFO - Epoch(val) [25][3]\ttop1_acc: 0.7857, top5_acc: 1.0000, mean_class_accuracy: 0.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 14/14, 10.0 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 00:24:27,050 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-31 00:24:27,051 - mmaction - INFO - \n",
      "top1_acc\t0.7857\n",
      "top5_acc\t1.0000\n",
      "2021-08-31 00:24:27,052 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-31 00:24:27,052 - mmaction - INFO - \n",
      "mean_acc\t0.7917\n",
      "2021-08-31 00:24:27,053 - mmaction - INFO - Epoch(val) [30][3]\ttop1_acc: 0.7857, top5_acc: 1.0000, mean_class_accuracy: 0.7917\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "from mmaction.datasets import build_dataset\n",
    "from mmaction.models import build_model\n",
    "from mmaction.apis import train_model\n",
    "\n",
    "import mmcv\n",
    "\n",
    "# Build the dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# Build the recognizer\n",
    "model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.train_cfg, test_cfg=cfg.test_cfg)\n",
    "# CUDA_LAUNCH_BLOCKING=1\n",
    "# Create work_dir\n",
    "mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
    "train_model(model, datasets, cfg, distributed=False, validate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced646b8-25d1-4c42-92b2-7cf01e98084b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f\"{cfg.work_dir}/model50e\", 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db180f14-773a-4743-8097-6f82f13d3758",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 16:54:18,016 - mmaction - INFO - load model from: https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth\n",
      "2021-08-10 16:54:18,017 - mmaction - INFO - Use load_from_http loader\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "from mmaction.datasets import build_dataset\n",
    "from mmaction.models import build_model\n",
    "from mmaction.apis import train_model\n",
    "import pickle\n",
    "import mmcv\n",
    "# Build the dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# Build the recognizer\n",
    "model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "\n",
    "model = pickle.load(open(f\"{cfg.work_dir}/model50e\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1601075-e8af-4c00-9dab-0dddab540448",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eyY3hCMwyTct",
    "outputId": "200e37c7-0da4-421f-98da-41418c3110ca",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mmaction.apis import single_gpu_test\n",
    "from mmaction.datasets import build_dataloader\n",
    "from mmcv.parallel import MMDataParallel\n",
    "# from mmaction.models import build_model\n",
    "# from mmaction.datasets import build_dataset\n",
    "\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# Build a test dataloader\n",
    "dataset = build_dataset(cfg.data.test, dict(test_mode=True))\n",
    "data_loader = build_dataloader(\n",
    "        dataset,\n",
    "        videos_per_gpu=1,\n",
    "        workers_per_gpu=1,\n",
    "        dist=False,\n",
    "        shuffle=False)\n",
    "model = MMDataParallel(model, device_ids=[0])\n",
    "outputs = single_gpu_test(model, data_loader)\n",
    "\n",
    "eval_config = cfg.evaluation\n",
    "eval_config.pop('interval')\n",
    "eval_res = dataset.evaluate(outputs, **eval_config)\n",
    "for name, val in eval_res.items():\n",
    "    print(f'{name}: {val:.04f}')\n",
    "    \n",
    "output_config = cfg.output_config\n",
    "dataset.dump_results(outputs, **output_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd251b4-b332-43e9-bc1d-12d847bb7a11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from mmaction.core import confusion_matrix \n",
    "\n",
    "gt_labels = [ann['label'] for ann in dataset.load_annotations()]\n",
    "pred = np.argmax(outputs, axis=1)\n",
    "cf_mat = confusion_matrix(pred, gt_labels).astype(float)\n",
    "cmap = sns.cubehelix_palette(50, hue=0.05, rot=0, light=0.9, dark=0, as_cmap=True)\n",
    "# /np.sum(cf_mat), fmt='.2%',\n",
    "# sns.heatmap(cf_mat, cmap=cmap, annot=True, xticklabels = ['6yo', '7yo', '8yo'], yticklabels = ['6yo', '7yo', '8yo'])\n",
    "sns.heatmap(cf_mat, cmap=cmap, annot=True, xticklabels = ['male','female'], yticklabels = ['male','female'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7099c023-5e2a-4a8e-aa21-fe135c36a630",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79a865c-51bd-4e5a-8f62-2eb8ad59e64a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

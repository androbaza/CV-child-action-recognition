{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44ef7d31-79f4-4b8d-84b7-03a87ce489d9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bf8PpPXtVvmg",
    "outputId": "2c685a33-474b-4e71-8f98-c2533c66095e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2019 NVIDIA Corporation\n",
      "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
      "Cuda compilation tools, release 10.1, V10.1.243\n",
      "gcc (Ubuntu 8.4.0-3ubuntu2) 8.4.0\n",
      "Copyright (C) 2018 Free Software Foundation, Inc.\n",
      "This is free software; see the source for copying conditions.  There is NO\n",
      "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check nvcc version\n",
    "!nvcc -V\n",
    "# Check GCC version\n",
    "!gcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bb1a682-5796-45c9-be5f-570b0f780a26",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "No_zZAFpWC-a",
    "outputId": "1d425eea-d44e-434a-991c-01eb15abaab2",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0 True\n",
      "0.16.0\n",
      "10.2\n",
      "GCC 7.3\n"
     ]
    }
   ],
   "source": [
    "# Check Pytorch installation\n",
    "import torch, torchvision\n",
    "print(torch.__version__, torch.cuda.is_available())\n",
    "\n",
    "# Check MMAction2 installation\n",
    "import mmaction\n",
    "print(mmaction.__version__)\n",
    "\n",
    "# Check MMCV installation\n",
    "from mmcv.ops import get_compiling_cuda_version, get_compiler_version\n",
    "print(get_compiling_cuda_version())\n",
    "print(get_compiler_version())\n",
    "\n",
    "import sys\n",
    "# sys.path.append('/home/actrec/.local/lib/python3.6/site-packages/decord-0.5.3-py3.6-linux-x86_64.egg')\n",
    "# import decord\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "916b8b4d-5611-4ec9-9411-451c41a2f4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/robt427nv/childact\n"
     ]
    }
   ],
   "source": [
    "cd /home/robt427nv/childact/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981efb1d-74e1-464e-aaef-6ce83a034aea",
   "metadata": {},
   "source": [
    "# CSN age BOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec97f7fb-9aae-4f6b-9238-706fd00a86ec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "64CW6d_AaT-Q",
    "outputId": "3b284fd8-4ee7-4a34-90d7-5023cd123a04",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-07-21 17:39:20--  https://download.openmmlab.com/mmaction/recognition/csn/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth\n",
      "Resolving download.openmmlab.com (download.openmmlab.com)... 47.75.20.25\n",
      "Connecting to download.openmmlab.com (download.openmmlab.com)|47.75.20.25|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 119580180 (114M) [application/octet-stream]\n",
      "Saving to: ‘checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth’\n",
      "\n",
      "checkpoints/ircsn_i 100%[===================>] 114,04M  6,78MB/s    in 17s     \n",
      "\n",
      "2021-07-21 17:39:39 (6,87 MB/s) - ‘checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth’ saved [119580180/119580180]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir checkpoints\n",
    "!wget -c https://download.openmmlab.com/mmaction/recognition/csn/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth \\\n",
    "      -O checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "775b5624-5baf-41d7-b8e6-e9a274bb317b",
   "metadata": {
    "id": "LjCcmCKOjktc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mmcv import Config\n",
    "cfg = Config.fromfile('mmaction2/configs/recognition/csn/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1cdd7df-c256-4d1a-b47b-1a027c33d315",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "tlhu9byjjt-K",
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "outputId": "13d1bb01-f351-48c0-9efb-0bdf2c125d29",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "model = dict(\n",
      "    type='Recognizer3D',\n",
      "    backbone=dict(\n",
      "        type='ResNet3dCSN',\n",
      "        pretrained2d=False,\n",
      "        pretrained=\n",
      "        'https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth',\n",
      "        depth=152,\n",
      "        with_pool2=False,\n",
      "        bottleneck_mode='ir',\n",
      "        norm_eval=True,\n",
      "        zero_init_residual=False,\n",
      "        bn_frozen=True),\n",
      "    cls_head=dict(\n",
      "        type='I3DHead',\n",
      "        num_classes=6,\n",
      "        in_channels=2048,\n",
      "        spatial_type='avg',\n",
      "        dropout_ratio=0.5,\n",
      "        init_std=0.01),\n",
      "    train_cfg=None,\n",
      "    test_cfg=dict(average_clips='prob'))\n",
      "checkpoint_config = dict(interval=20)\n",
      "log_config = dict(interval=100, hooks=[dict(type='TextLoggerHook')])\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = 'checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth'\n",
      "resume_from = None\n",
      "workflow = [('train', 1)]\n",
      "dataset_type = 'RawframeDataset'\n",
      "data_root = 'age-gender-3split-rgb-frames/'\n",
      "data_root_val = 'age-gender-3split-rgb-frames/val/'\n",
      "ann_file_train = 'age-gender-3split-rgb-frames/childact_train_rgb320_age_box.txt'\n",
      "ann_file_val = 'age-gender-3split-rgb-frames/childact_val_rgb320_age_box.txt'\n",
      "ann_file_test = 'age-gender-3split-rgb-frames/childact_test_rgb320_age_box.txt'\n",
      "img_norm_cfg = dict(\n",
      "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_bgr=False)\n",
      "train_pipeline = [\n",
      "    dict(type='SampleFrames', clip_len=32, frame_interval=2, num_clips=1),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='RandomResizedCrop'),\n",
      "    dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "    dict(type='Flip', flip_ratio=0.5),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "]\n",
      "val_pipeline = [\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=32,\n",
      "        frame_interval=2,\n",
      "        num_clips=1,\n",
      "        test_mode=True),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='CenterCrop', crop_size=224),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=32,\n",
      "        frame_interval=2,\n",
      "        num_clips=10,\n",
      "        test_mode=True),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='ThreeCrop', crop_size=256),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "data = dict(\n",
      "    videos_per_gpu=4,\n",
      "    workers_per_gpu=4,\n",
      "    train=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_train_rgb320_age_box.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/train/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=1),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='RandomResizedCrop'),\n",
      "            dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "            dict(type='Flip', flip_ratio=0.5),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'),\n",
      "    val=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file='age-gender-3split-rgb-frames/childact_val_rgb320_age_box.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/val/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=1,\n",
      "                test_mode=True),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='CenterCrop', crop_size=224),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'),\n",
      "    test=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_test_rgb320_age_box.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/test/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=10,\n",
      "                test_mode=True),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='ThreeCrop', crop_size=256),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'))\n",
      "evaluation = dict(\n",
      "    interval=5, metrics=['top_k_accuracy', 'mean_class_accuracy'])\n",
      "optimizer = dict(type='SGD', lr=0.000125, momentum=0.9, weight_decay=0.0001)\n",
      "optimizer_config = dict(grad_clip=dict(max_norm=40, norm_type=2))\n",
      "lr_config = dict(\n",
      "    policy='step',\n",
      "    step=[32, 48],\n",
      "    warmup='linear',\n",
      "    warmup_ratio=0.1,\n",
      "    warmup_by_epoch=True,\n",
      "    warmup_iters=16)\n",
      "total_epochs = 50\n",
      "work_dir = './childact-checkpoints/CSN-age-box'\n",
      "find_unused_parameters = True\n",
      "omnisource = False\n",
      "seed = 42\n",
      "gpu_ids = range(0, 1)\n",
      "output_config = dict(out='./childact-checkpoints/CSN-age-box/results.json')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mmcv.runner import set_random_seed\n",
    "\n",
    "# Modify dataset type and path\n",
    "# cfg.dataset_type = 'VideoDataset'\n",
    "cfg.data_root = 'age-gender-3split-rgb-frames/'\n",
    "cfg.data_root_val = 'age-gender-3split-rgb-frames/val/'\n",
    "cfg.ann_file_train = 'age-gender-3split-rgb-frames/childact_train_rgb320_age_box.txt'\n",
    "cfg.ann_file_val = 'age-gender-3split-rgb-frames/childact_val_rgb320_age_box.txt'\n",
    "cfg.ann_file_test = 'age-gender-3split-rgb-frames/childact_test_rgb320_age_box.txt'\n",
    "\n",
    "# cfg.data.test.type = 'VideoDataset'\n",
    "cfg.data.test.ann_file = 'age-gender-3split-rgb-frames/childact_test_rgb320_age_box.txt'\n",
    "cfg.data.test.data_prefix = 'age-gender-3split-rgb-frames/test/'\n",
    "\n",
    "# cfg.data.train.type = 'VideoDataset'\n",
    "cfg.data.train.ann_file = 'age-gender-3split-rgb-frames/childact_train_rgb320_age_box.txt'\n",
    "cfg.data.train.data_prefix = 'age-gender-3split-rgb-frames/train/'\n",
    "\n",
    "# cfg.data.val.type = 'VideoDataset'\n",
    "cfg.data.val.ann_file = 'age-gender-3split-rgb-frames/childact_val_rgb320_age_box.txt'\n",
    "cfg.data.val.data_prefix = 'age-gender-3split-rgb-frames/val/'\n",
    "\n",
    "# cfg.data.test.modality = 'Flow'\n",
    "# cfg.data.val.modality = 'Flow'\n",
    "# cfg.data.train.modality = 'Flow'\n",
    "\n",
    "# cfg.data.train.start_index = 0\n",
    "# cfg.data.test.start_index = 0\n",
    "# cfg.data.val.start_index = 0\n",
    "\n",
    "cfg.data.test.filename_tmpl = '{:03}.jpeg'\n",
    "cfg.data.train.filename_tmpl = '{:03}.jpeg'\n",
    "cfg.data.val.filename_tmpl = '{:03}.jpeg'\n",
    "# The flag is used to determine whether it is omnisource training\n",
    "cfg.setdefault('omnisource', False)\n",
    "# Modify num classes of the model in cls_head\n",
    "cfg.model.cls_head.num_classes = 6\n",
    "# We can use the pre-trained TSN model\n",
    "cfg.load_from = 'checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth'\n",
    "# cfg.resume_from = './childact-mm/latest.pth'\n",
    "\n",
    "# Set up working dir to save files and logs.\n",
    "cfg.work_dir = './childact-checkpoints/CSN-age-box'\n",
    "\n",
    "cfg.total_epochs = 50\n",
    "\n",
    "# cfg.momentum_config = dict(\n",
    "#     policy='cyclic',\n",
    "#     target_ratio=(0.85 / 0.95, 1),\n",
    "#     cyclic_times=1,\n",
    "#     step_ratio_up=0.4,\n",
    "# )\n",
    "\n",
    "# We can set the checkpoint saving interval to reduce the storage cost\n",
    "cfg.checkpoint_config.interval = 20\n",
    "# We can set the log print interval to reduce the the times of printing log\n",
    "cfg.log_config.interval = 100\n",
    "\n",
    "# Set seed thus the results are more reproducible\n",
    "cfg.seed = 42\n",
    "set_random_seed(42, deterministic=False)\n",
    "cfg.gpu_ids = range(1)\n",
    "\n",
    "cfg.data.videos_per_gpu=4\n",
    "\n",
    "# cfg.model.backbone.in_channels = 2\n",
    "\n",
    "cfg.output_config = dict(out=f'{cfg.work_dir}/results.json')\n",
    "# We can initialize the logger for training and have a look\n",
    "# at the final config used for training\n",
    "# del cfg.optimizer['momentum']\n",
    "print(f'Config:\\n{cfg.pretty_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b2dfc99-ac49-4b23-be8d-379dc9fb4d2b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "dDBWkdDRk6oz",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "85a52ef3-7b5c-4c52-8fef-00322a8c65e6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 08:19:33,116 - mmaction - INFO - load model from: https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth\n",
      "2021-08-10 08:19:33,117 - mmaction - INFO - Use load_from_http loader\n",
      "2021-08-10 08:19:35,230 - mmaction - INFO - load checkpoint from checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth\n",
      "2021-08-10 08:19:35,231 - mmaction - INFO - Use load_from_local loader\n",
      "2021-08-10 08:19:35,407 - mmaction - WARNING - The model and loaded state dict do not match exactly\n",
      "\n",
      "size mismatch for cls_head.fc_cls.weight: copying a param with shape torch.Size([400, 2048]) from checkpoint, the shape in current model is torch.Size([6, 2048]).\n",
      "size mismatch for cls_head.fc_cls.bias: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([6]).\n",
      "2021-08-10 08:19:35,411 - mmaction - INFO - Start running, host: robt427nv@robt427NV, work_dir: /home/robt427nv/childact/childact-checkpoints/CSN-age-box\n",
      "2021-08-10 08:19:35,412 - mmaction - INFO - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(ABOVE_NORMAL) OptimizerHook                      \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "2021-08-10 08:19:35,413 - mmaction - INFO - workflow: [('train', 1)], max: 50 epochs\n",
      "/home/robt427nv/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/mmcv/runner/hooks/evaluation.py:190: UserWarning: runner.meta is None. Creating an empty one.\n",
      "  warnings.warn('runner.meta is None. Creating an empty one.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 11/11, 9.7 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 08:21:45,103 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 08:21:45,105 - mmaction - INFO - \n",
      "top1_acc\t0.2727\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 08:21:45,105 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 08:21:45,107 - mmaction - INFO - \n",
      "mean_acc\t0.2500\n",
      "2021-08-10 08:21:45,432 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_5.pth.\n",
      "2021-08-10 08:21:45,433 - mmaction - INFO - Best top1_acc is 0.2727 at 5 epoch.\n",
      "2021-08-10 08:21:45,433 - mmaction - INFO - Epoch(val) [5][3]\ttop1_acc: 0.2727, top5_acc: 1.0000, mean_class_accuracy: 0.2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 11/11, 9.0 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 08:23:56,335 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 08:23:56,337 - mmaction - INFO - \n",
      "top1_acc\t0.1818\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 08:23:56,337 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 08:23:56,338 - mmaction - INFO - \n",
      "mean_acc\t0.1667\n",
      "2021-08-10 08:23:56,338 - mmaction - INFO - Epoch(val) [10][3]\ttop1_acc: 0.1818, top5_acc: 1.0000, mean_class_accuracy: 0.1667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 11/11, 9.7 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 08:26:01,773 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 08:26:01,775 - mmaction - INFO - \n",
      "top1_acc\t0.2727\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 08:26:01,775 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 08:26:01,777 - mmaction - INFO - \n",
      "mean_acc\t0.2500\n",
      "2021-08-10 08:26:01,777 - mmaction - INFO - Epoch(val) [15][3]\ttop1_acc: 0.2727, top5_acc: 1.0000, mean_class_accuracy: 0.2500\n",
      "2021-08-10 08:28:08,388 - mmaction - INFO - Saving checkpoint at 20 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 11/11, 9.5 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 08:28:09,906 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 08:28:09,907 - mmaction - INFO - \n",
      "top1_acc\t0.1818\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 08:28:09,908 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 08:28:09,909 - mmaction - INFO - \n",
      "mean_acc\t0.1333\n",
      "2021-08-10 08:28:09,909 - mmaction - INFO - Epoch(val) [20][3]\ttop1_acc: 0.1818, top5_acc: 1.0000, mean_class_accuracy: 0.1333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 11/11, 9.1 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 08:30:15,204 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 08:30:15,206 - mmaction - INFO - \n",
      "top1_acc\t0.2727\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 08:30:15,207 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 08:30:15,208 - mmaction - INFO - \n",
      "mean_acc\t0.1833\n",
      "2021-08-10 08:30:15,209 - mmaction - INFO - Epoch(val) [25][3]\ttop1_acc: 0.2727, top5_acc: 1.0000, mean_class_accuracy: 0.1833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 11/11, 9.6 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 08:32:21,612 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 08:32:21,614 - mmaction - INFO - \n",
      "top1_acc\t0.3636\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 08:32:21,615 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 08:32:21,617 - mmaction - INFO - \n",
      "mean_acc\t0.3125\n",
      "2021-08-10 08:32:21,940 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_30.pth.\n",
      "2021-08-10 08:32:21,941 - mmaction - INFO - Best top1_acc is 0.3636 at 30 epoch.\n",
      "2021-08-10 08:32:21,942 - mmaction - INFO - Epoch(val) [30][3]\ttop1_acc: 0.3636, top5_acc: 1.0000, mean_class_accuracy: 0.3125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 11/11, 9.6 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 08:34:28,553 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 08:34:28,554 - mmaction - INFO - \n",
      "top1_acc\t0.2727\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 08:34:28,555 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 08:34:28,555 - mmaction - INFO - \n",
      "mean_acc\t0.1833\n",
      "2021-08-10 08:34:28,556 - mmaction - INFO - Epoch(val) [35][3]\ttop1_acc: 0.2727, top5_acc: 1.0000, mean_class_accuracy: 0.1833\n",
      "2021-08-10 08:36:37,867 - mmaction - INFO - Saving checkpoint at 40 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 11/11, 9.3 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 08:36:39,421 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 08:36:39,423 - mmaction - INFO - \n",
      "top1_acc\t0.2727\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 08:36:39,423 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 08:36:39,424 - mmaction - INFO - \n",
      "mean_acc\t0.1833\n",
      "2021-08-10 08:36:39,426 - mmaction - INFO - Epoch(val) [40][3]\ttop1_acc: 0.2727, top5_acc: 1.0000, mean_class_accuracy: 0.1833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 11/11, 9.1 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 08:38:50,814 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 08:38:50,816 - mmaction - INFO - \n",
      "top1_acc\t0.2727\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 08:38:50,817 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 08:38:50,819 - mmaction - INFO - \n",
      "mean_acc\t0.1833\n",
      "2021-08-10 08:38:50,820 - mmaction - INFO - Epoch(val) [45][3]\ttop1_acc: 0.2727, top5_acc: 1.0000, mean_class_accuracy: 0.1833\n",
      "2021-08-10 08:41:01,028 - mmaction - INFO - Saving checkpoint at 50 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 11/11, 9.5 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 08:41:02,547 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 08:41:02,549 - mmaction - INFO - \n",
      "top1_acc\t0.1818\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 08:41:02,550 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 08:41:02,553 - mmaction - INFO - \n",
      "mean_acc\t0.1333\n",
      "2021-08-10 08:41:02,554 - mmaction - INFO - Epoch(val) [50][3]\ttop1_acc: 0.1818, top5_acc: 1.0000, mean_class_accuracy: 0.1333\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "from mmaction.datasets import build_dataset\n",
    "from mmaction.models import build_model\n",
    "from mmaction.apis import train_model\n",
    "\n",
    "import mmcv\n",
    "\n",
    "# Build the dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# Build the recognizer\n",
    "model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.train_cfg, test_cfg=cfg.test_cfg)\n",
    "# CUDA_LAUNCH_BLOCKING=1\n",
    "# Create work_dir\n",
    "mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
    "train_model(model, datasets, cfg, distributed=False, validate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f5b739c-1b26-40ec-b6d6-74ee4d0b8a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f\"{cfg.work_dir}/model50e\", 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03c849b7-ddd1-4743-97f2-f98f7186016c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eyY3hCMwyTct",
    "outputId": "200e37c7-0da4-421f-98da-41418c3110ca",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 8/8, 0.4 task/s, elapsed: 18s, ETA:     0s\n",
      "Evaluating top_k_accuracy ...\n",
      "\n",
      "top1_acc\t0.0000\n",
      "top5_acc\t0.8750\n",
      "\n",
      "Evaluating mean_class_accuracy ...\n",
      "\n",
      "mean_acc\t0.0000\n",
      "top1_acc: 0.0000\n",
      "top5_acc: 0.8750\n",
      "mean_class_accuracy: 0.0000\n"
     ]
    }
   ],
   "source": [
    "from mmaction.apis import single_gpu_test\n",
    "from mmaction.datasets import build_dataloader\n",
    "from mmcv.parallel import MMDataParallel\n",
    "# from mmaction.models import build_model\n",
    "# from mmaction.datasets import build_dataset\n",
    "\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# Build a test dataloader\n",
    "dataset = build_dataset(cfg.data.test, dict(test_mode=True))\n",
    "data_loader = build_dataloader(\n",
    "        dataset,\n",
    "        videos_per_gpu=1,\n",
    "        workers_per_gpu=2,\n",
    "        dist=False,\n",
    "        shuffle=False)\n",
    "model = MMDataParallel(model, device_ids=[0])\n",
    "outputs = single_gpu_test(model, data_loader)\n",
    "\n",
    "eval_config = cfg.evaluation\n",
    "eval_config.pop('interval')\n",
    "eval_res = dataset.evaluate(outputs, **eval_config)\n",
    "for name, val in eval_res.items():\n",
    "    print(f'{name}: {val:.04f}')\n",
    "    \n",
    "output_config = cfg.output_config\n",
    "dataset.dump_results(outputs, **output_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35c36b81-8229-4d07-80c1-521fc7dae107",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from mmaction.core import confusion_matrix \n",
    "\n",
    "gt_labels = [ann['label'] for ann in dataset.load_annotations()]\n",
    "pred = np.argmax(outputs, axis=1)\n",
    "cf_mat = confusion_matrix(pred, gt_labels).astype(float)\n",
    "cmap = sns.cubehelix_palette(50, hue=0.05, rot=0, light=0.9, dark=0, as_cmap=True)\n",
    "# /np.sum(cf_mat), fmt='.2%',\n",
    "sns.heatmap(cf_mat, cmap=cmap, annot=True, xticklabels = ['6yo', '7yo', '8yo', '9yo', '10yo', '11yo'], yticklabels = ['6yo', '7yo', '8yo', '9yo', '10yo', '11yo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6944e37-59d8-437c-a112-a147ce045619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "mean_error = (1+5+1+1)/8 #number of all tested videos\n",
    "print(mean_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c47bde-6314-4b20-a9f5-2bd279e7868a",
   "metadata": {},
   "source": [
    "# CSN age clap "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "261aa6f1-84a1-457d-95ee-d807eee5ddc4",
   "metadata": {
    "id": "LjCcmCKOjktc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mmcv import Config\n",
    "cfg = Config.fromfile('mmaction2/configs/recognition/csn/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27880464-a44d-4f37-9223-27f0dc2fc8bf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "tlhu9byjjt-K",
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "outputId": "13d1bb01-f351-48c0-9efb-0bdf2c125d29",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "model = dict(\n",
      "    type='Recognizer3D',\n",
      "    backbone=dict(\n",
      "        type='ResNet3dCSN',\n",
      "        pretrained2d=False,\n",
      "        pretrained=\n",
      "        'https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth',\n",
      "        depth=152,\n",
      "        with_pool2=False,\n",
      "        bottleneck_mode='ir',\n",
      "        norm_eval=True,\n",
      "        zero_init_residual=False,\n",
      "        bn_frozen=True),\n",
      "    cls_head=dict(\n",
      "        type='I3DHead',\n",
      "        num_classes=6,\n",
      "        in_channels=2048,\n",
      "        spatial_type='avg',\n",
      "        dropout_ratio=0.5,\n",
      "        init_std=0.01),\n",
      "    train_cfg=None,\n",
      "    test_cfg=dict(average_clips='prob'))\n",
      "checkpoint_config = dict(interval=20)\n",
      "log_config = dict(interval=100, hooks=[dict(type='TextLoggerHook')])\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = 'checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth'\n",
      "resume_from = None\n",
      "workflow = [('train', 1)]\n",
      "dataset_type = 'RawframeDataset'\n",
      "data_root = 'age-gender-3split-rgb-frames/'\n",
      "data_root_val = 'age-gender-3split-rgb-frames/val/'\n",
      "ann_file_train = 'age-gender-3split-rgb-frames/childact_train_rgb320_age_clap.txt'\n",
      "ann_file_val = 'age-gender-3split-rgb-frames/childact_val_rgb320_age_clap.txt'\n",
      "ann_file_test = 'age-gender-3split-rgb-frames/childact_test_rgb320_age_clap.txt'\n",
      "img_norm_cfg = dict(\n",
      "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_bgr=False)\n",
      "train_pipeline = [\n",
      "    dict(type='SampleFrames', clip_len=32, frame_interval=2, num_clips=1),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='RandomResizedCrop'),\n",
      "    dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "    dict(type='Flip', flip_ratio=0.5),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "]\n",
      "val_pipeline = [\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=32,\n",
      "        frame_interval=2,\n",
      "        num_clips=1,\n",
      "        test_mode=True),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='CenterCrop', crop_size=224),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=32,\n",
      "        frame_interval=2,\n",
      "        num_clips=10,\n",
      "        test_mode=True),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='ThreeCrop', crop_size=256),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "data = dict(\n",
      "    videos_per_gpu=4,\n",
      "    workers_per_gpu=4,\n",
      "    train=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_train_rgb320_age_clap.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/train/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=1),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='RandomResizedCrop'),\n",
      "            dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "            dict(type='Flip', flip_ratio=0.5),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'),\n",
      "    val=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_val_rgb320_age_clap.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/val/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=1,\n",
      "                test_mode=True),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='CenterCrop', crop_size=224),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'),\n",
      "    test=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_test_rgb320_age_clap.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/test/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=10,\n",
      "                test_mode=True),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='ThreeCrop', crop_size=256),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'))\n",
      "evaluation = dict(\n",
      "    interval=5, metrics=['top_k_accuracy', 'mean_class_accuracy'])\n",
      "optimizer = dict(type='SGD', lr=0.000125, momentum=0.9, weight_decay=0.0001)\n",
      "optimizer_config = dict(grad_clip=dict(max_norm=40, norm_type=2))\n",
      "lr_config = dict(\n",
      "    policy='step',\n",
      "    step=[32, 48],\n",
      "    warmup='linear',\n",
      "    warmup_ratio=0.1,\n",
      "    warmup_by_epoch=True,\n",
      "    warmup_iters=16)\n",
      "total_epochs = 50\n",
      "work_dir = './childact-checkpoints/CSN-age-box'\n",
      "find_unused_parameters = True\n",
      "omnisource = False\n",
      "seed = 42\n",
      "gpu_ids = range(0, 1)\n",
      "output_config = dict(out='./childact-checkpoints/CSN-age-box/results.json')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mmcv.runner import set_random_seed\n",
    "\n",
    "# Modify dataset type and path\n",
    "# cfg.dataset_type = 'VideoDataset'\n",
    "cfg.data_root = 'age-gender-3split-rgb-frames/'\n",
    "cfg.data_root_val = 'age-gender-3split-rgb-frames/val/'\n",
    "cfg.ann_file_train = 'age-gender-3split-rgb-frames/childact_train_rgb320_age_clap.txt'\n",
    "cfg.ann_file_val = 'age-gender-3split-rgb-frames/childact_val_rgb320_age_clap.txt'\n",
    "cfg.ann_file_test = 'age-gender-3split-rgb-frames/childact_test_rgb320_age_clap.txt'\n",
    "\n",
    "# cfg.data.test.type = 'VideoDataset'\n",
    "cfg.data.test.ann_file = 'age-gender-3split-rgb-frames/childact_test_rgb320_age_clap.txt'\n",
    "cfg.data.test.data_prefix = 'age-gender-3split-rgb-frames/test/'\n",
    "\n",
    "# cfg.data.train.type = 'VideoDataset'\n",
    "cfg.data.train.ann_file = 'age-gender-3split-rgb-frames/childact_train_rgb320_age_clap.txt'\n",
    "cfg.data.train.data_prefix = 'age-gender-3split-rgb-frames/train/'\n",
    "\n",
    "# cfg.data.val.type = 'VideoDataset'\n",
    "cfg.data.val.ann_file = 'age-gender-3split-rgb-frames/childact_val_rgb320_age_clap.txt'\n",
    "cfg.data.val.data_prefix = 'age-gender-3split-rgb-frames/val/'\n",
    "\n",
    "# cfg.data.test.modality = 'Flow'\n",
    "# cfg.data.val.modality = 'Flow'\n",
    "# cfg.data.train.modality = 'Flow'\n",
    "\n",
    "# cfg.data.train.start_index = 0\n",
    "# cfg.data.test.start_index = 0\n",
    "# cfg.data.val.start_index = 0\n",
    "\n",
    "cfg.data.test.filename_tmpl = '{:03}.jpeg'\n",
    "cfg.data.train.filename_tmpl = '{:03}.jpeg'\n",
    "cfg.data.val.filename_tmpl = '{:03}.jpeg'\n",
    "# The flag is used to determine whether it is omnisource training\n",
    "cfg.setdefault('omnisource', False)\n",
    "# Modify num classes of the model in cls_head\n",
    "cfg.model.cls_head.num_classes = 6\n",
    "# We can use the pre-trained TSN model\n",
    "cfg.load_from = 'checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth'\n",
    "# cfg.resume_from = './childact-mm/latest.pth'\n",
    "\n",
    "# Set up working dir to save files and logs.\n",
    "cfg.work_dir = './childact-checkpoints/CSN-age-box'\n",
    "\n",
    "cfg.total_epochs = 50\n",
    "\n",
    "# cfg.momentum_config = dict(\n",
    "#     policy='cyclic',\n",
    "#     target_ratio=(0.85 / 0.95, 1),\n",
    "#     cyclic_times=1,\n",
    "#     step_ratio_up=0.4,\n",
    "# )\n",
    "\n",
    "# We can set the checkpoint saving interval to reduce the storage cost\n",
    "cfg.checkpoint_config.interval = 20\n",
    "# We can set the log print interval to reduce the the times of printing log\n",
    "cfg.log_config.interval = 100\n",
    "\n",
    "# Set seed thus the results are more reproducible\n",
    "cfg.seed = 42\n",
    "set_random_seed(42, deterministic=False)\n",
    "cfg.gpu_ids = range(1)\n",
    "\n",
    "cfg.data.videos_per_gpu=4\n",
    "\n",
    "# cfg.model.backbone.in_channels = 2\n",
    "\n",
    "cfg.output_config = dict(out=f'{cfg.work_dir}/results.json')\n",
    "# We can initialize the logger for training and have a look\n",
    "# at the final config used for training\n",
    "# del cfg.optimizer['momentum']\n",
    "print(f'Config:\\n{cfg.pretty_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd758729-843c-4476-b478-97d77772b037",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "dDBWkdDRk6oz",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "85a52ef3-7b5c-4c52-8fef-00322a8c65e6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 08:52:09,598 - mmaction - INFO - load model from: https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth\n",
      "2021-08-10 08:52:09,599 - mmaction - INFO - Use load_from_http loader\n",
      "2021-08-10 08:52:11,641 - mmaction - INFO - load checkpoint from checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth\n",
      "2021-08-10 08:52:11,642 - mmaction - INFO - Use load_from_local loader\n",
      "2021-08-10 08:52:11,809 - mmaction - WARNING - The model and loaded state dict do not match exactly\n",
      "\n",
      "size mismatch for cls_head.fc_cls.weight: copying a param with shape torch.Size([400, 2048]) from checkpoint, the shape in current model is torch.Size([6, 2048]).\n",
      "size mismatch for cls_head.fc_cls.bias: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([6]).\n",
      "2021-08-10 08:52:11,815 - mmaction - INFO - Start running, host: robt427nv@robt427NV, work_dir: /home/robt427nv/childact/childact-checkpoints/CSN-age-box\n",
      "2021-08-10 08:52:11,816 - mmaction - INFO - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(ABOVE_NORMAL) OptimizerHook                      \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "2021-08-10 08:52:11,816 - mmaction - INFO - workflow: [('train', 1)], max: 50 epochs\n",
      "/home/robt427nv/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/mmcv/runner/hooks/evaluation.py:190: UserWarning: runner.meta is None. Creating an empty one.\n",
      "  warnings.warn('runner.meta is None. Creating an empty one.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 14/14, 9.8 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 08:54:38,069 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 08:54:38,070 - mmaction - INFO - \n",
      "top1_acc\t0.2143\n",
      "top5_acc\t0.9286\n",
      "2021-08-10 08:54:38,071 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 08:54:38,073 - mmaction - INFO - \n",
      "mean_acc\t0.1905\n",
      "2021-08-10 08:54:38,394 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_5.pth.\n",
      "2021-08-10 08:54:38,396 - mmaction - INFO - Best top1_acc is 0.2143 at 5 epoch.\n",
      "2021-08-10 08:54:38,396 - mmaction - INFO - Epoch(val) [5][4]\ttop1_acc: 0.2143, top5_acc: 0.9286, mean_class_accuracy: 0.1905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 14/14, 9.8 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 08:57:05,509 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 08:57:05,511 - mmaction - INFO - \n",
      "top1_acc\t0.5000\n",
      "top5_acc\t0.9286\n",
      "2021-08-10 08:57:05,511 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 08:57:05,512 - mmaction - INFO - \n",
      "mean_acc\t0.1667\n",
      "2021-08-10 08:57:05,849 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_10.pth.\n",
      "2021-08-10 08:57:05,850 - mmaction - INFO - Best top1_acc is 0.5000 at 10 epoch.\n",
      "2021-08-10 08:57:05,851 - mmaction - INFO - Epoch(val) [10][4]\ttop1_acc: 0.5000, top5_acc: 0.9286, mean_class_accuracy: 0.1667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 14/14, 10.0 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 08:59:33,436 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 08:59:33,438 - mmaction - INFO - \n",
      "top1_acc\t0.4286\n",
      "top5_acc\t0.9286\n",
      "2021-08-10 08:59:33,439 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 08:59:33,441 - mmaction - INFO - \n",
      "mean_acc\t0.2857\n",
      "2021-08-10 08:59:33,442 - mmaction - INFO - Epoch(val) [15][4]\ttop1_acc: 0.4286, top5_acc: 0.9286, mean_class_accuracy: 0.2857\n",
      "2021-08-10 09:01:59,151 - mmaction - INFO - Saving checkpoint at 20 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 14/14, 10.5 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 09:02:00,960 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 09:02:00,962 - mmaction - INFO - \n",
      "top1_acc\t0.5000\n",
      "top5_acc\t0.8571\n",
      "2021-08-10 09:02:00,963 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 09:02:00,964 - mmaction - INFO - \n",
      "mean_acc\t0.4286\n",
      "2021-08-10 09:02:00,965 - mmaction - INFO - Epoch(val) [20][4]\ttop1_acc: 0.5000, top5_acc: 0.8571, mean_class_accuracy: 0.4286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 14/14, 10.6 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 09:04:28,396 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 09:04:28,398 - mmaction - INFO - \n",
      "top1_acc\t0.3571\n",
      "top5_acc\t0.9286\n",
      "2021-08-10 09:04:28,398 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 09:04:28,399 - mmaction - INFO - \n",
      "mean_acc\t0.3214\n",
      "2021-08-10 09:04:28,399 - mmaction - INFO - Epoch(val) [25][4]\ttop1_acc: 0.3571, top5_acc: 0.9286, mean_class_accuracy: 0.3214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 14/14, 10.2 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 09:06:55,330 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 09:06:55,331 - mmaction - INFO - \n",
      "top1_acc\t0.3571\n",
      "top5_acc\t0.9286\n",
      "2021-08-10 09:06:55,332 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 09:06:55,333 - mmaction - INFO - \n",
      "mean_acc\t0.3810\n",
      "2021-08-10 09:06:55,334 - mmaction - INFO - Epoch(val) [30][4]\ttop1_acc: 0.3571, top5_acc: 0.9286, mean_class_accuracy: 0.3810\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 14/14, 10.1 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 09:09:21,996 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 09:09:21,998 - mmaction - INFO - \n",
      "top1_acc\t0.3571\n",
      "top5_acc\t0.9286\n",
      "2021-08-10 09:09:21,998 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 09:09:22,000 - mmaction - INFO - \n",
      "mean_acc\t0.3214\n",
      "2021-08-10 09:09:22,001 - mmaction - INFO - Epoch(val) [35][4]\ttop1_acc: 0.3571, top5_acc: 0.9286, mean_class_accuracy: 0.3214\n",
      "2021-08-10 09:11:48,431 - mmaction - INFO - Saving checkpoint at 40 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 14/14, 10.0 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 09:11:50,333 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 09:11:50,334 - mmaction - INFO - \n",
      "top1_acc\t0.2857\n",
      "top5_acc\t0.9286\n",
      "2021-08-10 09:11:50,335 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 09:11:50,336 - mmaction - INFO - \n",
      "mean_acc\t0.2976\n",
      "2021-08-10 09:11:50,336 - mmaction - INFO - Epoch(val) [40][4]\ttop1_acc: 0.2857, top5_acc: 0.9286, mean_class_accuracy: 0.2976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 14/14, 10.1 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 09:14:17,482 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 09:14:17,483 - mmaction - INFO - \n",
      "top1_acc\t0.3571\n",
      "top5_acc\t0.9286\n",
      "2021-08-10 09:14:17,483 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 09:14:17,484 - mmaction - INFO - \n",
      "mean_acc\t0.3810\n",
      "2021-08-10 09:14:17,484 - mmaction - INFO - Epoch(val) [45][4]\ttop1_acc: 0.3571, top5_acc: 0.9286, mean_class_accuracy: 0.3810\n",
      "2021-08-10 09:16:43,161 - mmaction - INFO - Saving checkpoint at 50 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 14/14, 10.0 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 09:16:45,035 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 09:16:45,037 - mmaction - INFO - \n",
      "top1_acc\t0.3571\n",
      "top5_acc\t0.9286\n",
      "2021-08-10 09:16:45,037 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 09:16:45,038 - mmaction - INFO - \n",
      "mean_acc\t0.3810\n",
      "2021-08-10 09:16:45,039 - mmaction - INFO - Epoch(val) [50][4]\ttop1_acc: 0.3571, top5_acc: 0.9286, mean_class_accuracy: 0.3810\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "from mmaction.datasets import build_dataset\n",
    "from mmaction.models import build_model\n",
    "from mmaction.apis import train_model\n",
    "\n",
    "import mmcv\n",
    "\n",
    "# Build the dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# Build the recognizer\n",
    "model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.train_cfg, test_cfg=cfg.test_cfg)\n",
    "# CUDA_LAUNCH_BLOCKING=1\n",
    "# Create work_dir\n",
    "mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
    "train_model(model, datasets, cfg, distributed=False, validate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e16f52ee-f73c-439a-b579-637a777dcf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f\"{cfg.work_dir}/model50e\", 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "293f078b-0dd0-4912-82a8-7a3bfdd1ea91",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eyY3hCMwyTct",
    "outputId": "200e37c7-0da4-421f-98da-41418c3110ca",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 18/18, 0.6 task/s, elapsed: 29s, ETA:     0s\n",
      "Evaluating top_k_accuracy ...\n",
      "\n",
      "top1_acc\t0.5000\n",
      "top5_acc\t0.9444\n",
      "\n",
      "Evaluating mean_class_accuracy ...\n",
      "\n",
      "mean_acc\t0.2569\n",
      "top1_acc: 0.5000\n",
      "top5_acc: 0.9444\n",
      "mean_class_accuracy: 0.2569\n"
     ]
    }
   ],
   "source": [
    "from mmaction.apis import single_gpu_test\n",
    "from mmaction.datasets import build_dataloader\n",
    "from mmcv.parallel import MMDataParallel\n",
    "# from mmaction.models import build_model\n",
    "# from mmaction.datasets import build_dataset\n",
    "\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# Build a test dataloader\n",
    "dataset = build_dataset(cfg.data.test, dict(test_mode=True))\n",
    "data_loader = build_dataloader(\n",
    "        dataset,\n",
    "        videos_per_gpu=1,\n",
    "        workers_per_gpu=2,\n",
    "        dist=False,\n",
    "        shuffle=False)\n",
    "model = MMDataParallel(model, device_ids=[0])\n",
    "outputs = single_gpu_test(model, data_loader)\n",
    "\n",
    "eval_config = cfg.evaluation\n",
    "eval_config.pop('interval')\n",
    "eval_res = dataset.evaluate(outputs, **eval_config)\n",
    "for name, val in eval_res.items():\n",
    "    print(f'{name}: {val:.04f}')\n",
    "    \n",
    "output_config = cfg.output_config\n",
    "dataset.dump_results(outputs, **output_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f5f6358-f472-459d-8b27-7b51bb8e18ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from mmaction.core import confusion_matrix \n",
    "\n",
    "gt_labels = [ann['label'] for ann in dataset.load_annotations()]\n",
    "pred = np.argmax(outputs, axis=1)\n",
    "cf_mat = confusion_matrix(pred, gt_labels).astype(float)\n",
    "cmap = sns.cubehelix_palette(50, hue=0.05, rot=0, light=0.9, dark=0, as_cmap=True)\n",
    "# /np.sum(cf_mat), fmt='.2%',\n",
    "sns.heatmap(cf_mat, cmap=cmap, annot=True, xticklabels = ['6yo', '7yo', '8yo', '9yo', '10yo', '11yo'], yticklabels = ['6yo', '7yo', '8yo', '9yo', '10yo', '11yo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65a7f8bc-5787-4ea2-88c0-f8f35ae396bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "mean_error = (1+1+1+1+3+1+1)/18 #number of all tested videos\n",
    "print(mean_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0d3c5c-7c01-473b-8c2e-1fe84570c1ef",
   "metadata": {},
   "source": [
    "# CSN age go "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9532f2b7-9f58-44ce-9119-6f4ec27a7457",
   "metadata": {
    "id": "LjCcmCKOjktc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mmcv import Config\n",
    "cfg = Config.fromfile('mmaction2/configs/recognition/csn/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a49c91d-d2a5-44dd-96ed-e38f1616e463",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "tlhu9byjjt-K",
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "outputId": "13d1bb01-f351-48c0-9efb-0bdf2c125d29",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "model = dict(\n",
      "    type='Recognizer3D',\n",
      "    backbone=dict(\n",
      "        type='ResNet3dCSN',\n",
      "        pretrained2d=False,\n",
      "        pretrained=\n",
      "        'https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth',\n",
      "        depth=152,\n",
      "        with_pool2=False,\n",
      "        bottleneck_mode='ir',\n",
      "        norm_eval=True,\n",
      "        zero_init_residual=False,\n",
      "        bn_frozen=True),\n",
      "    cls_head=dict(\n",
      "        type='I3DHead',\n",
      "        num_classes=6,\n",
      "        in_channels=2048,\n",
      "        spatial_type='avg',\n",
      "        dropout_ratio=0.5,\n",
      "        init_std=0.01),\n",
      "    train_cfg=None,\n",
      "    test_cfg=dict(average_clips='prob'))\n",
      "checkpoint_config = dict(interval=20)\n",
      "log_config = dict(interval=100, hooks=[dict(type='TextLoggerHook')])\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = 'checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth'\n",
      "resume_from = None\n",
      "workflow = [('train', 1)]\n",
      "dataset_type = 'RawframeDataset'\n",
      "data_root = 'age-gender-3split-rgb-frames/'\n",
      "data_root_val = 'age-gender-3split-rgb-frames/val/'\n",
      "ann_file_train = 'age-gender-3split-rgb-frames/childact_train_rgb320_age_go.txt'\n",
      "ann_file_val = 'age-gender-3split-rgb-frames/childact_val_rgb320_age_go.txt'\n",
      "ann_file_test = 'age-gender-3split-rgb-frames/childact_test_rgb320_age_go.txt'\n",
      "img_norm_cfg = dict(\n",
      "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_bgr=False)\n",
      "train_pipeline = [\n",
      "    dict(type='SampleFrames', clip_len=32, frame_interval=2, num_clips=1),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='RandomResizedCrop'),\n",
      "    dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "    dict(type='Flip', flip_ratio=0.5),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "]\n",
      "val_pipeline = [\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=32,\n",
      "        frame_interval=2,\n",
      "        num_clips=1,\n",
      "        test_mode=True),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='CenterCrop', crop_size=224),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=32,\n",
      "        frame_interval=2,\n",
      "        num_clips=10,\n",
      "        test_mode=True),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='ThreeCrop', crop_size=256),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "data = dict(\n",
      "    videos_per_gpu=4,\n",
      "    workers_per_gpu=4,\n",
      "    train=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_train_rgb320_age_go.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/train/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=1),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='RandomResizedCrop'),\n",
      "            dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "            dict(type='Flip', flip_ratio=0.5),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'),\n",
      "    val=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file='age-gender-3split-rgb-frames/childact_val_rgb320_age_go.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/val/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=1,\n",
      "                test_mode=True),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='CenterCrop', crop_size=224),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'),\n",
      "    test=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file='age-gender-3split-rgb-frames/childact_test_rgb320_age_go.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/test/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=10,\n",
      "                test_mode=True),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='ThreeCrop', crop_size=256),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'))\n",
      "evaluation = dict(\n",
      "    interval=5, metrics=['top_k_accuracy', 'mean_class_accuracy'])\n",
      "optimizer = dict(type='SGD', lr=0.000125, momentum=0.9, weight_decay=0.0001)\n",
      "optimizer_config = dict(grad_clip=dict(max_norm=40, norm_type=2))\n",
      "lr_config = dict(\n",
      "    policy='step',\n",
      "    step=[32, 48],\n",
      "    warmup='linear',\n",
      "    warmup_ratio=0.1,\n",
      "    warmup_by_epoch=True,\n",
      "    warmup_iters=16)\n",
      "total_epochs = 50\n",
      "work_dir = './childact-checkpoints/CSN-age-go'\n",
      "find_unused_parameters = True\n",
      "omnisource = False\n",
      "seed = 42\n",
      "gpu_ids = range(0, 1)\n",
      "output_config = dict(out='./childact-checkpoints/CSN-age-go/results.json')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mmcv.runner import set_random_seed\n",
    "\n",
    "# Modify dataset type and path\n",
    "# cfg.dataset_type = 'VideoDataset'\n",
    "cfg.data_root = 'age-gender-3split-rgb-frames/'\n",
    "cfg.data_root_val = 'age-gender-3split-rgb-frames/val/'\n",
    "cfg.ann_file_train = 'age-gender-3split-rgb-frames/childact_train_rgb320_age_go.txt'\n",
    "cfg.ann_file_val = 'age-gender-3split-rgb-frames/childact_val_rgb320_age_go.txt'\n",
    "cfg.ann_file_test = 'age-gender-3split-rgb-frames/childact_test_rgb320_age_go.txt'\n",
    "\n",
    "# cfg.data.test.type = 'VideoDataset'\n",
    "cfg.data.test.ann_file = 'age-gender-3split-rgb-frames/childact_test_rgb320_age_go.txt'\n",
    "cfg.data.test.data_prefix = 'age-gender-3split-rgb-frames/test/'\n",
    "\n",
    "# cfg.data.train.type = 'VideoDataset'\n",
    "cfg.data.train.ann_file = 'age-gender-3split-rgb-frames/childact_train_rgb320_age_go.txt'\n",
    "cfg.data.train.data_prefix = 'age-gender-3split-rgb-frames/train/'\n",
    "\n",
    "# cfg.data.val.type = 'VideoDataset'\n",
    "cfg.data.val.ann_file = 'age-gender-3split-rgb-frames/childact_val_rgb320_age_go.txt'\n",
    "cfg.data.val.data_prefix = 'age-gender-3split-rgb-frames/val/'\n",
    "\n",
    "# cfg.data.test.modality = 'Flow'\n",
    "# cfg.data.val.modality = 'Flow'\n",
    "# cfg.data.train.modality = 'Flow'\n",
    "\n",
    "# cfg.data.train.start_index = 0\n",
    "# cfg.data.test.start_index = 0\n",
    "# cfg.data.val.start_index = 0\n",
    "\n",
    "cfg.data.test.filename_tmpl = '{:03}.jpeg'\n",
    "cfg.data.train.filename_tmpl = '{:03}.jpeg'\n",
    "cfg.data.val.filename_tmpl = '{:03}.jpeg'\n",
    "# The flag is used to determine whether it is omnisource training\n",
    "cfg.setdefault('omnisource', False)\n",
    "# Modify num classes of the model in cls_head\n",
    "cfg.model.cls_head.num_classes = 6\n",
    "# We can use the pre-trained TSN model\n",
    "cfg.load_from = 'checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth'\n",
    "# cfg.resume_from = './childact-mm/latest.pth'\n",
    "\n",
    "# Set up working dir to save files and logs.\n",
    "cfg.work_dir = './childact-checkpoints/CSN-age-go'\n",
    "\n",
    "cfg.total_epochs = 50\n",
    "\n",
    "# cfg.momentum_config = dict(\n",
    "#     policy='cyclic',\n",
    "#     target_ratio=(0.85 / 0.95, 1),\n",
    "#     cyclic_times=1,\n",
    "#     step_ratio_up=0.4,\n",
    "# )\n",
    "\n",
    "# We can set the checkpoint saving interval to reduce the storage cost\n",
    "cfg.checkpoint_config.interval = 20\n",
    "# We can set the log print interval to reduce the the times of printing log\n",
    "cfg.log_config.interval = 100\n",
    "\n",
    "# Set seed thus the results are more reproducible\n",
    "cfg.seed = 42\n",
    "set_random_seed(42, deterministic=False)\n",
    "cfg.gpu_ids = range(1)\n",
    "\n",
    "cfg.data.videos_per_gpu=4\n",
    "\n",
    "# cfg.model.backbone.in_channels = 2\n",
    "\n",
    "cfg.output_config = dict(out=f'{cfg.work_dir}/results.json')\n",
    "# We can initialize the logger for training and have a look\n",
    "# at the final config used for training\n",
    "# del cfg.optimizer['momentum']\n",
    "print(f'Config:\\n{cfg.pretty_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15612c13-8dea-4515-b084-43d185f78fe7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "dDBWkdDRk6oz",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "85a52ef3-7b5c-4c52-8fef-00322a8c65e6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 09:40:44,136 - mmaction - INFO - load model from: https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth\n",
      "2021-08-10 09:40:44,137 - mmaction - INFO - Use load_from_http loader\n",
      "2021-08-10 09:40:44,381 - mmaction - INFO - load checkpoint from checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth\n",
      "2021-08-10 09:40:44,382 - mmaction - INFO - Use load_from_local loader\n",
      "2021-08-10 09:40:44,546 - mmaction - WARNING - The model and loaded state dict do not match exactly\n",
      "\n",
      "size mismatch for cls_head.fc_cls.weight: copying a param with shape torch.Size([400, 2048]) from checkpoint, the shape in current model is torch.Size([6, 2048]).\n",
      "size mismatch for cls_head.fc_cls.bias: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([6]).\n",
      "2021-08-10 09:40:44,549 - mmaction - INFO - Start running, host: robt427nv@robt427NV, work_dir: /home/robt427nv/childact/childact-checkpoints/CSN-age-go\n",
      "2021-08-10 09:40:44,551 - mmaction - INFO - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(ABOVE_NORMAL) OptimizerHook                      \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "2021-08-10 09:40:44,552 - mmaction - INFO - workflow: [('train', 1)], max: 50 epochs\n",
      "/home/robt427nv/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/mmcv/runner/hooks/evaluation.py:190: UserWarning: runner.meta is None. Creating an empty one.\n",
      "  warnings.warn('runner.meta is None. Creating an empty one.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 18/18, 10.1 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 09:42:50,544 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 09:42:50,545 - mmaction - INFO - \n",
      "top1_acc\t0.3889\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 09:42:50,546 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 09:42:50,547 - mmaction - INFO - \n",
      "mean_acc\t0.2500\n",
      "2021-08-10 09:42:50,846 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_5.pth.\n",
      "2021-08-10 09:42:50,847 - mmaction - INFO - Best top1_acc is 0.3889 at 5 epoch.\n",
      "2021-08-10 09:42:50,848 - mmaction - INFO - Epoch(val) [5][5]\ttop1_acc: 0.3889, top5_acc: 1.0000, mean_class_accuracy: 0.2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 18/18, 10.3 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 09:44:57,985 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 09:44:57,988 - mmaction - INFO - \n",
      "top1_acc\t0.5000\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 09:44:57,988 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 09:44:57,990 - mmaction - INFO - \n",
      "mean_acc\t0.3000\n",
      "2021-08-10 09:44:58,325 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_10.pth.\n",
      "2021-08-10 09:44:58,326 - mmaction - INFO - Best top1_acc is 0.5000 at 10 epoch.\n",
      "2021-08-10 09:44:58,327 - mmaction - INFO - Epoch(val) [10][5]\ttop1_acc: 0.5000, top5_acc: 1.0000, mean_class_accuracy: 0.3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 18/18, 10.7 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 09:47:05,944 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 09:47:05,946 - mmaction - INFO - \n",
      "top1_acc\t0.6111\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 09:47:05,946 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 09:47:05,948 - mmaction - INFO - \n",
      "mean_acc\t0.4550\n",
      "2021-08-10 09:47:06,291 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_15.pth.\n",
      "2021-08-10 09:47:06,292 - mmaction - INFO - Best top1_acc is 0.6111 at 15 epoch.\n",
      "2021-08-10 09:47:06,293 - mmaction - INFO - Epoch(val) [15][5]\ttop1_acc: 0.6111, top5_acc: 1.0000, mean_class_accuracy: 0.4550\n",
      "2021-08-10 09:49:12,101 - mmaction - INFO - Saving checkpoint at 20 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 18/18, 10.9 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 09:49:14,144 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 09:49:14,146 - mmaction - INFO - \n",
      "top1_acc\t0.4444\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 09:49:14,146 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 09:49:14,147 - mmaction - INFO - \n",
      "mean_acc\t0.2300\n",
      "2021-08-10 09:49:14,147 - mmaction - INFO - Epoch(val) [20][5]\ttop1_acc: 0.4444, top5_acc: 1.0000, mean_class_accuracy: 0.2300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 18/18, 10.3 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 09:51:21,538 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 09:51:21,539 - mmaction - INFO - \n",
      "top1_acc\t0.6667\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 09:51:21,539 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 09:51:21,540 - mmaction - INFO - \n",
      "mean_acc\t0.5100\n",
      "2021-08-10 09:51:21,858 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_25.pth.\n",
      "2021-08-10 09:51:21,859 - mmaction - INFO - Best top1_acc is 0.6667 at 25 epoch.\n",
      "2021-08-10 09:51:21,860 - mmaction - INFO - Epoch(val) [25][5]\ttop1_acc: 0.6667, top5_acc: 1.0000, mean_class_accuracy: 0.5100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 18/18, 10.7 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 09:53:29,780 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 09:53:29,782 - mmaction - INFO - \n",
      "top1_acc\t0.3333\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 09:53:29,783 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 09:53:29,784 - mmaction - INFO - \n",
      "mean_acc\t0.2250\n",
      "2021-08-10 09:53:29,785 - mmaction - INFO - Epoch(val) [30][5]\ttop1_acc: 0.3333, top5_acc: 1.0000, mean_class_accuracy: 0.2250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 18/18, 10.5 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 09:55:37,088 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 09:55:37,090 - mmaction - INFO - \n",
      "top1_acc\t0.6667\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 09:55:37,090 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 09:55:37,091 - mmaction - INFO - \n",
      "mean_acc\t0.6450\n",
      "2021-08-10 09:55:37,092 - mmaction - INFO - Epoch(val) [35][5]\ttop1_acc: 0.6667, top5_acc: 1.0000, mean_class_accuracy: 0.6450\n",
      "2021-08-10 09:57:42,632 - mmaction - INFO - Saving checkpoint at 40 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 18/18, 10.6 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 09:57:44,741 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 09:57:44,742 - mmaction - INFO - \n",
      "top1_acc\t0.6667\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 09:57:44,743 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 09:57:44,744 - mmaction - INFO - \n",
      "mean_acc\t0.6450\n",
      "2021-08-10 09:57:44,745 - mmaction - INFO - Epoch(val) [40][5]\ttop1_acc: 0.6667, top5_acc: 1.0000, mean_class_accuracy: 0.6450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 18/18, 10.6 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 09:59:51,790 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 09:59:51,792 - mmaction - INFO - \n",
      "top1_acc\t0.6667\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 09:59:51,793 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 09:59:51,794 - mmaction - INFO - \n",
      "mean_acc\t0.6450\n",
      "2021-08-10 09:59:51,795 - mmaction - INFO - Epoch(val) [45][5]\ttop1_acc: 0.6667, top5_acc: 1.0000, mean_class_accuracy: 0.6450\n",
      "2021-08-10 10:01:57,274 - mmaction - INFO - Saving checkpoint at 50 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 18/18, 11.0 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 10:01:59,299 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 10:01:59,301 - mmaction - INFO - \n",
      "top1_acc\t0.6667\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 10:01:59,302 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 10:01:59,303 - mmaction - INFO - \n",
      "mean_acc\t0.5850\n",
      "2021-08-10 10:01:59,304 - mmaction - INFO - Epoch(val) [50][5]\ttop1_acc: 0.6667, top5_acc: 1.0000, mean_class_accuracy: 0.5850\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "from mmaction.datasets import build_dataset\n",
    "from mmaction.models import build_model\n",
    "from mmaction.apis import train_model\n",
    "\n",
    "import mmcv\n",
    "\n",
    "# Build the dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# Build the recognizer\n",
    "model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.train_cfg, test_cfg=cfg.test_cfg)\n",
    "# CUDA_LAUNCH_BLOCKING=1\n",
    "# Create work_dir\n",
    "mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
    "train_model(model, datasets, cfg, distributed=False, validate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3fa717db-1fa7-4a43-abe7-3da42cd8d3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f\"{cfg.work_dir}/model50e\", 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22098c4d-b3a1-4c46-abad-e4ef9f36332c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 10:10:19,285 - mmaction - INFO - load model from: https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth\n",
      "2021-08-10 10:10:19,286 - mmaction - INFO - Use load_from_http loader\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "from mmaction.datasets import build_dataset\n",
    "from mmaction.models import build_model\n",
    "from mmaction.apis import train_model\n",
    "import pickle\n",
    "import mmcv\n",
    "# Build the dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# Build the recognizer\n",
    "model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "\n",
    "model = pickle.load(open(f\"{cfg.work_dir}/model50e\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d475d2b-9e9b-4016-ac3a-511eaf41e2c4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eyY3hCMwyTct",
    "outputId": "200e37c7-0da4-421f-98da-41418c3110ca",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 22/22, 0.7 task/s, elapsed: 33s, ETA:     0s\n",
      "Evaluating top_k_accuracy ...\n",
      "\n",
      "top1_acc\t0.5455\n",
      "top5_acc\t0.9091\n",
      "\n",
      "Evaluating mean_class_accuracy ...\n",
      "\n",
      "mean_acc\t0.3847\n",
      "top1_acc: 0.5455\n",
      "top5_acc: 0.9091\n",
      "mean_class_accuracy: 0.3847\n"
     ]
    }
   ],
   "source": [
    "from mmaction.apis import single_gpu_test\n",
    "from mmaction.datasets import build_dataloader\n",
    "from mmcv.parallel import MMDataParallel\n",
    "# from mmaction.models import build_model\n",
    "# from mmaction.datasets import build_dataset\n",
    "\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# Build a test dataloader\n",
    "dataset = build_dataset(cfg.data.test, dict(test_mode=True))\n",
    "data_loader = build_dataloader(\n",
    "        dataset,\n",
    "        videos_per_gpu=1,\n",
    "        workers_per_gpu=2,\n",
    "        dist=False,\n",
    "        shuffle=False)\n",
    "model = MMDataParallel(model, device_ids=[0])\n",
    "outputs = single_gpu_test(model, data_loader)\n",
    "\n",
    "eval_config = cfg.evaluation\n",
    "eval_config.pop('interval')\n",
    "eval_res = dataset.evaluate(outputs, **eval_config)\n",
    "for name, val in eval_res.items():\n",
    "    print(f'{name}: {val:.04f}')\n",
    "    \n",
    "output_config = cfg.output_config\n",
    "dataset.dump_results(outputs, **output_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0bd7db4-ae6b-4f08-8164-bfef79f6e59c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from mmaction.core import confusion_matrix \n",
    "\n",
    "gt_labels = [ann['label'] for ann in dataset.load_annotations()]\n",
    "pred = np.argmax(outputs, axis=1)\n",
    "cf_mat = confusion_matrix(pred, gt_labels).astype(float)\n",
    "cmap = sns.cubehelix_palette(50, hue=0.05, rot=0, light=0.9, dark=0, as_cmap=True)\n",
    "# /np.sum(cf_mat), fmt='.2%',\n",
    "sns.heatmap(cf_mat, cmap=cmap, annot=True, xticklabels = ['6yo', '7yo', '8yo', '9yo', '10yo', '11yo'], yticklabels = ['6yo', '7yo', '8yo', '9yo', '10yo', '11yo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5a001d4-df1e-46d5-9f5e-805438ba858d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5909090909090909\n"
     ]
    }
   ],
   "source": [
    "mean_error = (4+2+1+2+2+1+1)/22 #number of all tested videos\n",
    "print(mean_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650c7218-bd9f-4399-996b-98206ce1b259",
   "metadata": {},
   "source": [
    "# CSN age jog "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "177ce2db-e34c-4ebf-b704-60eb60dd370f",
   "metadata": {
    "id": "LjCcmCKOjktc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mmcv import Config\n",
    "cfg = Config.fromfile('mmaction2/configs/recognition/csn/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21f37bad-181b-4687-aeb7-c7b4cb202f52",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "tlhu9byjjt-K",
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "outputId": "13d1bb01-f351-48c0-9efb-0bdf2c125d29",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "model = dict(\n",
      "    type='Recognizer3D',\n",
      "    backbone=dict(\n",
      "        type='ResNet3dCSN',\n",
      "        pretrained2d=False,\n",
      "        pretrained=\n",
      "        'https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth',\n",
      "        depth=152,\n",
      "        with_pool2=False,\n",
      "        bottleneck_mode='ir',\n",
      "        norm_eval=True,\n",
      "        zero_init_residual=False,\n",
      "        bn_frozen=True),\n",
      "    cls_head=dict(\n",
      "        type='I3DHead',\n",
      "        num_classes=6,\n",
      "        in_channels=2048,\n",
      "        spatial_type='avg',\n",
      "        dropout_ratio=0.5,\n",
      "        init_std=0.01),\n",
      "    train_cfg=None,\n",
      "    test_cfg=dict(average_clips='prob'))\n",
      "checkpoint_config = dict(interval=20)\n",
      "log_config = dict(interval=100, hooks=[dict(type='TextLoggerHook')])\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = 'checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth'\n",
      "resume_from = None\n",
      "workflow = [('train', 1)]\n",
      "dataset_type = 'RawframeDataset'\n",
      "data_root = 'age-gender-3split-rgb-frames/'\n",
      "data_root_val = 'age-gender-3split-rgb-frames/val/'\n",
      "ann_file_train = 'age-gender-3split-rgb-frames/childact_train_rgb320_age_jog.txt'\n",
      "ann_file_val = 'age-gender-3split-rgb-frames/childact_val_rgb320_age_jog.txt'\n",
      "ann_file_test = 'age-gender-3split-rgb-frames/childact_test_rgb320_age_jog.txt'\n",
      "img_norm_cfg = dict(\n",
      "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_bgr=False)\n",
      "train_pipeline = [\n",
      "    dict(type='SampleFrames', clip_len=32, frame_interval=2, num_clips=1),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='RandomResizedCrop'),\n",
      "    dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "    dict(type='Flip', flip_ratio=0.5),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "]\n",
      "val_pipeline = [\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=32,\n",
      "        frame_interval=2,\n",
      "        num_clips=1,\n",
      "        test_mode=True),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='CenterCrop', crop_size=224),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=32,\n",
      "        frame_interval=2,\n",
      "        num_clips=10,\n",
      "        test_mode=True),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='ThreeCrop', crop_size=256),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "data = dict(\n",
      "    videos_per_gpu=4,\n",
      "    workers_per_gpu=4,\n",
      "    train=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_train_rgb320_age_jog.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/train/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=1),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='RandomResizedCrop'),\n",
      "            dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "            dict(type='Flip', flip_ratio=0.5),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'),\n",
      "    val=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file='age-gender-3split-rgb-frames/childact_val_rgb320_age_jog.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/val/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=1,\n",
      "                test_mode=True),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='CenterCrop', crop_size=224),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'),\n",
      "    test=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_test_rgb320_age_jog.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/test/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=10,\n",
      "                test_mode=True),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='ThreeCrop', crop_size=256),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'))\n",
      "evaluation = dict(\n",
      "    interval=5, metrics=['top_k_accuracy', 'mean_class_accuracy'])\n",
      "optimizer = dict(type='SGD', lr=0.000125, momentum=0.9, weight_decay=0.0001)\n",
      "optimizer_config = dict(grad_clip=dict(max_norm=40, norm_type=2))\n",
      "lr_config = dict(\n",
      "    policy='step',\n",
      "    step=[32, 48],\n",
      "    warmup='linear',\n",
      "    warmup_ratio=0.1,\n",
      "    warmup_by_epoch=True,\n",
      "    warmup_iters=16)\n",
      "total_epochs = 50\n",
      "work_dir = './childact-checkpoints/CSN-age-jog'\n",
      "find_unused_parameters = True\n",
      "omnisource = False\n",
      "seed = 42\n",
      "gpu_ids = range(0, 1)\n",
      "output_config = dict(out='./childact-checkpoints/CSN-age-jog/results.json')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mmcv.runner import set_random_seed\n",
    "\n",
    "# Modify dataset type and path\n",
    "# cfg.dataset_type = 'VideoDataset'\n",
    "cfg.data_root = 'age-gender-3split-rgb-frames/'\n",
    "cfg.data_root_val = 'age-gender-3split-rgb-frames/val/'\n",
    "cfg.ann_file_train = 'age-gender-3split-rgb-frames/childact_train_rgb320_age_jog.txt'\n",
    "cfg.ann_file_val = 'age-gender-3split-rgb-frames/childact_val_rgb320_age_jog.txt'\n",
    "cfg.ann_file_test = 'age-gender-3split-rgb-frames/childact_test_rgb320_age_jog.txt'\n",
    "\n",
    "# cfg.data.test.type = 'VideoDataset'\n",
    "cfg.data.test.ann_file = 'age-gender-3split-rgb-frames/childact_test_rgb320_age_jog.txt'\n",
    "cfg.data.test.data_prefix = 'age-gender-3split-rgb-frames/test/'\n",
    "\n",
    "# cfg.data.train.type = 'VideoDataset'\n",
    "cfg.data.train.ann_file = 'age-gender-3split-rgb-frames/childact_train_rgb320_age_jog.txt'\n",
    "cfg.data.train.data_prefix = 'age-gender-3split-rgb-frames/train/'\n",
    "\n",
    "# cfg.data.val.type = 'VideoDataset'\n",
    "cfg.data.val.ann_file = 'age-gender-3split-rgb-frames/childact_val_rgb320_age_jog.txt'\n",
    "cfg.data.val.data_prefix = 'age-gender-3split-rgb-frames/val/'\n",
    "\n",
    "# cfg.data.test.modality = 'Flow'\n",
    "# cfg.data.val.modality = 'Flow'\n",
    "# cfg.data.train.modality = 'Flow'\n",
    "\n",
    "# cfg.data.train.start_index = 0\n",
    "# cfg.data.test.start_index = 0\n",
    "# cfg.data.val.start_index = 0\n",
    "\n",
    "cfg.data.test.filename_tmpl = '{:03}.jpeg'\n",
    "cfg.data.train.filename_tmpl = '{:03}.jpeg'\n",
    "cfg.data.val.filename_tmpl = '{:03}.jpeg'\n",
    "# The flag is used to determine whether it is omnisource training\n",
    "cfg.setdefault('omnisource', False)\n",
    "# Modify num classes of the model in cls_head\n",
    "cfg.model.cls_head.num_classes = 6\n",
    "# We can use the pre-trained TSN model\n",
    "cfg.load_from = 'checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth'\n",
    "# cfg.resume_from = './childact-mm/latest.pth'\n",
    "\n",
    "# Set up working dir to save files and logs.\n",
    "cfg.work_dir = './childact-checkpoints/CSN-age-jog'\n",
    "\n",
    "cfg.total_epochs = 50\n",
    "\n",
    "# cfg.momentum_config = dict(\n",
    "#     policy='cyclic',\n",
    "#     target_ratio=(0.85 / 0.95, 1),\n",
    "#     cyclic_times=1,\n",
    "#     step_ratio_up=0.4,\n",
    "# )\n",
    "\n",
    "# We can set the checkpoint saving interval to reduce the storage cost\n",
    "cfg.checkpoint_config.interval = 20\n",
    "# We can set the log print interval to reduce the the times of printing log\n",
    "cfg.log_config.interval = 100\n",
    "\n",
    "# Set seed thus the results are more reproducible\n",
    "cfg.seed = 42\n",
    "set_random_seed(42, deterministic=False)\n",
    "cfg.gpu_ids = range(1)\n",
    "\n",
    "cfg.data.videos_per_gpu=4\n",
    "\n",
    "# cfg.model.backbone.in_channels = 2\n",
    "\n",
    "cfg.output_config = dict(out=f'{cfg.work_dir}/results.json')\n",
    "# We can initialize the logger for training and have a look\n",
    "# at the final config used for training\n",
    "# del cfg.optimizer['momentum']\n",
    "print(f'Config:\\n{cfg.pretty_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ba29a54-96ed-4dee-a29e-8fc6715f5dbd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "dDBWkdDRk6oz",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "85a52ef3-7b5c-4c52-8fef-00322a8c65e6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 16:16:38,766 - mmaction - INFO - load model from: https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth\n",
      "2021-08-10 16:16:38,767 - mmaction - INFO - Use load_from_http loader\n",
      "2021-08-10 16:16:39,003 - mmaction - INFO - load checkpoint from checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth\n",
      "2021-08-10 16:16:39,003 - mmaction - INFO - Use load_from_local loader\n",
      "2021-08-10 16:16:39,149 - mmaction - WARNING - The model and loaded state dict do not match exactly\n",
      "\n",
      "size mismatch for cls_head.fc_cls.weight: copying a param with shape torch.Size([400, 2048]) from checkpoint, the shape in current model is torch.Size([6, 2048]).\n",
      "size mismatch for cls_head.fc_cls.bias: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([6]).\n",
      "2021-08-10 16:16:39,150 - mmaction - INFO - Start running, host: robt427nv@robt427NV, work_dir: /home/robt427nv/childact/childact-checkpoints/CSN-age-jog\n",
      "2021-08-10 16:16:39,151 - mmaction - INFO - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(ABOVE_NORMAL) OptimizerHook                      \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "2021-08-10 16:16:39,151 - mmaction - INFO - workflow: [('train', 1)], max: 50 epochs\n",
      "/home/robt427nv/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/mmcv/runner/hooks/evaluation.py:190: UserWarning: runner.meta is None. Creating an empty one.\n",
      "  warnings.warn('runner.meta is None. Creating an empty one.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 15/15, 9.2 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 16:19:18,896 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 16:19:18,898 - mmaction - INFO - \n",
      "top1_acc\t0.7333\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 16:19:18,899 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 16:19:18,900 - mmaction - INFO - \n",
      "mean_acc\t0.3750\n",
      "2021-08-10 16:19:19,209 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_5.pth.\n",
      "2021-08-10 16:19:19,210 - mmaction - INFO - Best top1_acc is 0.7333 at 5 epoch.\n",
      "2021-08-10 16:19:19,210 - mmaction - INFO - Epoch(val) [5][4]\ttop1_acc: 0.7333, top5_acc: 1.0000, mean_class_accuracy: 0.3750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 15/15, 9.7 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 16:22:00,722 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 16:22:00,724 - mmaction - INFO - \n",
      "top1_acc\t0.6000\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 16:22:00,724 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 16:22:00,726 - mmaction - INFO - \n",
      "mean_acc\t0.2250\n",
      "2021-08-10 16:22:00,726 - mmaction - INFO - Epoch(val) [10][4]\ttop1_acc: 0.6000, top5_acc: 1.0000, mean_class_accuracy: 0.2250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 15/15, 9.5 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 16:24:39,911 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 16:24:39,914 - mmaction - INFO - \n",
      "top1_acc\t0.4000\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 16:24:39,914 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 16:24:39,917 - mmaction - INFO - \n",
      "mean_acc\t0.2500\n",
      "2021-08-10 16:24:39,918 - mmaction - INFO - Epoch(val) [15][4]\ttop1_acc: 0.4000, top5_acc: 1.0000, mean_class_accuracy: 0.2500\n",
      "2021-08-10 16:27:12,426 - mmaction - INFO - Saving checkpoint at 20 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 15/15, 9.5 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 16:27:14,392 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 16:27:14,394 - mmaction - INFO - \n",
      "top1_acc\t0.5333\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 16:27:14,394 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 16:27:14,396 - mmaction - INFO - \n",
      "mean_acc\t0.3000\n",
      "2021-08-10 16:27:14,396 - mmaction - INFO - Epoch(val) [20][4]\ttop1_acc: 0.5333, top5_acc: 1.0000, mean_class_accuracy: 0.3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 15/15, 9.8 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 16:29:49,037 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 16:29:49,039 - mmaction - INFO - \n",
      "top1_acc\t0.6000\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 16:29:49,039 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 16:29:49,040 - mmaction - INFO - \n",
      "mean_acc\t0.3250\n",
      "2021-08-10 16:29:49,041 - mmaction - INFO - Epoch(val) [25][4]\ttop1_acc: 0.6000, top5_acc: 1.0000, mean_class_accuracy: 0.3250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 15/15, 9.0 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 16:32:24,040 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 16:32:24,085 - mmaction - INFO - \n",
      "top1_acc\t0.2667\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 16:32:24,103 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 16:32:24,196 - mmaction - INFO - \n",
      "mean_acc\t0.4250\n",
      "2021-08-10 16:32:24,237 - mmaction - INFO - Epoch(val) [30][4]\ttop1_acc: 0.2667, top5_acc: 1.0000, mean_class_accuracy: 0.4250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 15/15, 9.6 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 16:34:59,790 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 16:34:59,792 - mmaction - INFO - \n",
      "top1_acc\t0.4667\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 16:34:59,793 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 16:34:59,795 - mmaction - INFO - \n",
      "mean_acc\t0.2750\n",
      "2021-08-10 16:34:59,796 - mmaction - INFO - Epoch(val) [35][4]\ttop1_acc: 0.4667, top5_acc: 1.0000, mean_class_accuracy: 0.2750\n",
      "2021-08-10 16:37:32,558 - mmaction - INFO - Saving checkpoint at 40 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 15/15, 9.2 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 16:37:34,605 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 16:37:34,606 - mmaction - INFO - \n",
      "top1_acc\t0.4667\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 16:37:34,607 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 16:37:34,608 - mmaction - INFO - \n",
      "mean_acc\t0.2750\n",
      "2021-08-10 16:37:34,609 - mmaction - INFO - Epoch(val) [40][4]\ttop1_acc: 0.4667, top5_acc: 1.0000, mean_class_accuracy: 0.2750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 15/15, 9.5 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 16:40:09,047 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 16:40:09,049 - mmaction - INFO - \n",
      "top1_acc\t0.4667\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 16:40:09,049 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 16:40:09,050 - mmaction - INFO - \n",
      "mean_acc\t0.2750\n",
      "2021-08-10 16:40:09,051 - mmaction - INFO - Epoch(val) [45][4]\ttop1_acc: 0.4667, top5_acc: 1.0000, mean_class_accuracy: 0.2750\n",
      "2021-08-10 16:42:41,388 - mmaction - INFO - Saving checkpoint at 50 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 15/15, 9.9 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 16:42:43,314 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 16:42:43,316 - mmaction - INFO - \n",
      "top1_acc\t0.5333\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 16:42:43,317 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 16:42:43,319 - mmaction - INFO - \n",
      "mean_acc\t0.5250\n",
      "2021-08-10 16:42:43,320 - mmaction - INFO - Epoch(val) [50][4]\ttop1_acc: 0.5333, top5_acc: 1.0000, mean_class_accuracy: 0.5250\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "from mmaction.datasets import build_dataset\n",
    "from mmaction.models import build_model\n",
    "from mmaction.apis import train_model\n",
    "\n",
    "import mmcv\n",
    "\n",
    "# Build the dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# Build the recognizer\n",
    "model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.train_cfg, test_cfg=cfg.test_cfg)\n",
    "# CUDA_LAUNCH_BLOCKING=1\n",
    "# Create work_dir\n",
    "mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
    "train_model(model, datasets, cfg, distributed=False, validate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a41036b7-3d48-4055-b643-c3660bf5763e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f\"{cfg.work_dir}/model50e\", 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc25eda3-6a08-41d9-908c-8b016de5c947",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 16:54:18,016 - mmaction - INFO - load model from: https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth\n",
      "2021-08-10 16:54:18,017 - mmaction - INFO - Use load_from_http loader\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "from mmaction.datasets import build_dataset\n",
    "from mmaction.models import build_model\n",
    "from mmaction.apis import train_model\n",
    "import pickle\n",
    "import mmcv\n",
    "# Build the dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# Build the recognizer\n",
    "model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "\n",
    "model = pickle.load(open(f\"{cfg.work_dir}/model50e\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d24a99fd-5a76-492b-98b9-8ee6427e00b9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eyY3hCMwyTct",
    "outputId": "200e37c7-0da4-421f-98da-41418c3110ca",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 10/10, 0.6 task/s, elapsed: 17s, ETA:     0s\n",
      "Evaluating top_k_accuracy ...\n",
      "\n",
      "top1_acc\t0.4000\n",
      "top5_acc\t1.0000\n",
      "\n",
      "Evaluating mean_class_accuracy ...\n",
      "\n",
      "mean_acc\t0.5000\n",
      "top1_acc: 0.4000\n",
      "top5_acc: 1.0000\n",
      "mean_class_accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "from mmaction.apis import single_gpu_test\n",
    "from mmaction.datasets import build_dataloader\n",
    "from mmcv.parallel import MMDataParallel\n",
    "# from mmaction.models import build_model\n",
    "# from mmaction.datasets import build_dataset\n",
    "\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# Build a test dataloader\n",
    "dataset = build_dataset(cfg.data.test, dict(test_mode=True))\n",
    "data_loader = build_dataloader(\n",
    "        dataset,\n",
    "        videos_per_gpu=1,\n",
    "        workers_per_gpu=1,\n",
    "        dist=False,\n",
    "        shuffle=False)\n",
    "model = MMDataParallel(model, device_ids=[0])\n",
    "outputs = single_gpu_test(model, data_loader)\n",
    "\n",
    "eval_config = cfg.evaluation\n",
    "eval_config.pop('interval')\n",
    "eval_res = dataset.evaluate(outputs, **eval_config)\n",
    "for name, val in eval_res.items():\n",
    "    print(f'{name}: {val:.04f}')\n",
    "    \n",
    "output_config = cfg.output_config\n",
    "dataset.dump_results(outputs, **output_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd2bb12b-8dba-4586-a0f3-b0b61985ba00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from mmaction.core import confusion_matrix \n",
    "\n",
    "gt_labels = [ann['label'] for ann in dataset.load_annotations()]\n",
    "pred = np.argmax(outputs, axis=1)\n",
    "cf_mat = confusion_matrix(pred, gt_labels).astype(float)\n",
    "cmap = sns.cubehelix_palette(50, hue=0.05, rot=0, light=0.9, dark=0, as_cmap=True)\n",
    "# /np.sum(cf_mat), fmt='.2%',\n",
    "sns.heatmap(cf_mat, cmap=cmap, annot=True, xticklabels = ['6yo', '7yo', '8yo', '9yo'], yticklabels = ['6yo', '7yo', '8yo', '9yo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7230d4f-ba5e-4c7f-ac0a-cbf11a679a8f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n"
     ]
    }
   ],
   "source": [
    "mean_error = (1+4+3)/10 #number of all tested videos\n",
    "print(mean_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef536a9f-1d69-40dc-aa45-66373942e6aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed0ef921-f91a-4eb4-8434-23c61aa04646",
   "metadata": {},
   "source": [
    "# CSN age wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1df6aaf-c0ef-4d6d-8db0-6ff71344c370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/robt427nv/childact\n"
     ]
    }
   ],
   "source": [
    "cd /home/robt427nv/childact/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d8ca5e9-46c8-4fe5-a41e-d22359ef9dd3",
   "metadata": {
    "id": "LjCcmCKOjktc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mmcv import Config\n",
    "cfg = Config.fromfile('mmaction2/configs/recognition/csn/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c82d6fcb-4f24-4912-b0b8-d1fe792a94cf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "tlhu9byjjt-K",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "13d1bb01-f351-48c0-9efb-0bdf2c125d29",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "model = dict(\n",
      "    type='Recognizer3D',\n",
      "    backbone=dict(\n",
      "        type='ResNet3dCSN',\n",
      "        pretrained2d=False,\n",
      "        pretrained=\n",
      "        'https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth',\n",
      "        depth=152,\n",
      "        with_pool2=False,\n",
      "        bottleneck_mode='ir',\n",
      "        norm_eval=True,\n",
      "        zero_init_residual=False,\n",
      "        bn_frozen=True),\n",
      "    cls_head=dict(\n",
      "        type='I3DHead',\n",
      "        num_classes=6,\n",
      "        in_channels=2048,\n",
      "        spatial_type='avg',\n",
      "        dropout_ratio=0.5,\n",
      "        init_std=0.01),\n",
      "    train_cfg=None,\n",
      "    test_cfg=dict(average_clips='prob'))\n",
      "checkpoint_config = dict(interval=20)\n",
      "log_config = dict(interval=100, hooks=[dict(type='TextLoggerHook')])\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = 'checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth'\n",
      "resume_from = None\n",
      "workflow = [('train', 1)]\n",
      "dataset_type = 'RawframeDataset'\n",
      "data_root = 'age-gender-3split-rgb-frames/'\n",
      "data_root_val = 'age-gender-3split-rgb-frames/val/'\n",
      "ann_file_train = 'age-gender-3split-rgb-frames/childact_train_rgb320_age_wave.txt'\n",
      "ann_file_val = 'age-gender-3split-rgb-frames/childact_val_rgb320_age_wave.txt'\n",
      "ann_file_test = 'age-gender-3split-rgb-frames/childact_test_rgb320_age_wave.txt'\n",
      "img_norm_cfg = dict(\n",
      "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_bgr=False)\n",
      "train_pipeline = [\n",
      "    dict(type='SampleFrames', clip_len=32, frame_interval=2, num_clips=1),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='RandomResizedCrop'),\n",
      "    dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "    dict(type='Flip', flip_ratio=0.5),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "]\n",
      "val_pipeline = [\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=32,\n",
      "        frame_interval=2,\n",
      "        num_clips=1,\n",
      "        test_mode=True),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='CenterCrop', crop_size=224),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=32,\n",
      "        frame_interval=2,\n",
      "        num_clips=10,\n",
      "        test_mode=True),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='ThreeCrop', crop_size=256),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "data = dict(\n",
      "    videos_per_gpu=6,\n",
      "    workers_per_gpu=4,\n",
      "    train=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_train_rgb320_age_wave.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/train/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=1),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='RandomResizedCrop'),\n",
      "            dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "            dict(type='Flip', flip_ratio=0.5),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'),\n",
      "    val=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_val_rgb320_age_wave.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/val/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=1,\n",
      "                test_mode=True),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='CenterCrop', crop_size=224),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'),\n",
      "    test=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_test_rgb320_age_wave.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/test/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=10,\n",
      "                test_mode=True),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='ThreeCrop', crop_size=256),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'))\n",
      "evaluation = dict(\n",
      "    interval=5, metrics=['top_k_accuracy', 'mean_class_accuracy'])\n",
      "optimizer = dict(type='SGD', lr=0.000125, momentum=0.9, weight_decay=0.0001)\n",
      "optimizer_config = dict(grad_clip=dict(max_norm=40, norm_type=2))\n",
      "lr_config = dict(\n",
      "    policy='step',\n",
      "    step=[32, 48],\n",
      "    warmup='linear',\n",
      "    warmup_ratio=0.1,\n",
      "    warmup_by_epoch=True,\n",
      "    warmup_iters=16)\n",
      "total_epochs = 50\n",
      "work_dir = './childact-checkpoints/CSN-age-wave'\n",
      "find_unused_parameters = True\n",
      "omnisource = False\n",
      "seed = 42\n",
      "gpu_ids = range(0, 1)\n",
      "output_config = dict(out='./childact-checkpoints/CSN-age-wave/results.json')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mmcv.runner import set_random_seed\n",
    "\n",
    "# Modify dataset type and path\n",
    "# cfg.dataset_type = 'VideoDataset'\n",
    "cfg.data_root = 'age-gender-3split-rgb-frames/'\n",
    "cfg.data_root_val = 'age-gender-3split-rgb-frames/val/'\n",
    "cfg.ann_file_train = 'age-gender-3split-rgb-frames/childact_train_rgb320_age_wave.txt'\n",
    "cfg.ann_file_val = 'age-gender-3split-rgb-frames/childact_val_rgb320_age_wave.txt'\n",
    "cfg.ann_file_test = 'age-gender-3split-rgb-frames/childact_test_rgb320_age_wave.txt'\n",
    "\n",
    "# cfg.data.test.type = 'VideoDataset'\n",
    "cfg.data.test.ann_file = 'age-gender-3split-rgb-frames/childact_test_rgb320_age_wave.txt'\n",
    "cfg.data.test.data_prefix = 'age-gender-3split-rgb-frames/test/'\n",
    "\n",
    "# cfg.data.train.type = 'VideoDataset'\n",
    "cfg.data.train.ann_file = 'age-gender-3split-rgb-frames/childact_train_rgb320_age_wave.txt'\n",
    "cfg.data.train.data_prefix = 'age-gender-3split-rgb-frames/train/'\n",
    "\n",
    "# cfg.data.val.type = 'VideoDataset'\n",
    "cfg.data.val.ann_file = 'age-gender-3split-rgb-frames/childact_val_rgb320_age_wave.txt'\n",
    "cfg.data.val.data_prefix = 'age-gender-3split-rgb-frames/val/'\n",
    "\n",
    "# cfg.data.test.modality = 'Flow'\n",
    "# cfg.data.val.modality = 'Flow'\n",
    "# cfg.data.train.modality = 'Flow'\n",
    "\n",
    "# cfg.data.train.start_index = 0\n",
    "# cfg.data.test.start_index = 0\n",
    "# cfg.data.val.start_index = 0\n",
    "\n",
    "cfg.data.test.filename_tmpl = '{:03}.jpeg'\n",
    "cfg.data.train.filename_tmpl = '{:03}.jpeg'\n",
    "cfg.data.val.filename_tmpl = '{:03}.jpeg'\n",
    "# The flag is used to determine whether it is omnisource training\n",
    "cfg.setdefault('omnisource', False)\n",
    "# Modify num classes of the model in cls_head\n",
    "cfg.model.cls_head.num_classes = 6\n",
    "# We can use the pre-trained TSN model\n",
    "cfg.load_from = 'checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth'\n",
    "# cfg.resume_from = './childact-mm/latest.pth'\n",
    "\n",
    "# Set up working dir to save files and logs.\n",
    "cfg.work_dir = './childact-checkpoints/CSN-age-wave'\n",
    "\n",
    "cfg.total_epochs = 50\n",
    "\n",
    "# cfg.momentum_config = dict(\n",
    "#     policy='cyclic',\n",
    "#     target_ratio=(0.85 / 0.95, 1),\n",
    "#     cyclic_times=1,\n",
    "#     step_ratio_up=0.4,\n",
    "# )\n",
    "\n",
    "# We can set the checkpoint saving interval to reduce the storage cost\n",
    "cfg.checkpoint_config.interval = 20\n",
    "# We can set the log print interval to reduce the the times of printing log\n",
    "cfg.log_config.interval = 100\n",
    "\n",
    "# Set seed thus the results are more reproducible\n",
    "cfg.seed = 42\n",
    "set_random_seed(42, deterministic=False)\n",
    "cfg.gpu_ids = range(1)\n",
    "\n",
    "cfg.data.videos_per_gpu=6\n",
    "\n",
    "# cfg.model.backbone.in_channels = 2\n",
    "\n",
    "cfg.output_config = dict(out=f'{cfg.work_dir}/results.json')\n",
    "# We can initialize the logger for training and have a look\n",
    "# at the final config used for training\n",
    "# del cfg.optimizer['momentum']\n",
    "print(f'Config:\\n{cfg.pretty_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "241f22d5-8cb5-40d8-a59d-739ab6886dd8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "dDBWkdDRk6oz",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "85a52ef3-7b5c-4c52-8fef-00322a8c65e6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-23 17:04:58,879 - mmaction - INFO - load model from: https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth\n",
      "2021-08-23 17:04:58,880 - mmaction - INFO - Use load_from_http loader\n",
      "2021-08-23 17:05:00,934 - mmaction - INFO - load checkpoint from checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth\n",
      "2021-08-23 17:05:00,935 - mmaction - INFO - Use load_from_local loader\n",
      "2021-08-23 17:05:01,112 - mmaction - WARNING - The model and loaded state dict do not match exactly\n",
      "\n",
      "size mismatch for cls_head.fc_cls.weight: copying a param with shape torch.Size([400, 2048]) from checkpoint, the shape in current model is torch.Size([6, 2048]).\n",
      "size mismatch for cls_head.fc_cls.bias: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([6]).\n",
      "2021-08-23 17:05:01,121 - mmaction - INFO - Start running, host: robt427nv@robt427NV, work_dir: /home/robt427nv/childact/childact-checkpoints/CSN-age-wave\n",
      "2021-08-23 17:05:01,122 - mmaction - INFO - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(ABOVE_NORMAL) OptimizerHook                      \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "2021-08-23 17:05:01,122 - mmaction - INFO - workflow: [('train', 1)], max: 50 epochs\n",
      "/home/robt427nv/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/mmcv/runner/hooks/evaluation.py:190: UserWarning: runner.meta is None. Creating an empty one.\n",
      "  warnings.warn('runner.meta is None. Creating an empty one.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 17/17, 10.2 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-23 17:07:02,674 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-23 17:07:02,675 - mmaction - INFO - \n",
      "top1_acc\t0.4118\n",
      "top5_acc\t0.9412\n",
      "2021-08-23 17:07:02,676 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-23 17:07:02,677 - mmaction - INFO - \n",
      "mean_acc\t0.2000\n",
      "2021-08-23 17:07:03,002 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_5.pth.\n",
      "2021-08-23 17:07:03,003 - mmaction - INFO - Best top1_acc is 0.4118 at 5 epoch.\n",
      "2021-08-23 17:07:03,004 - mmaction - INFO - Epoch(val) [5][3]\ttop1_acc: 0.4118, top5_acc: 0.9412, mean_class_accuracy: 0.2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 17/17, 9.9 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-23 17:09:04,382 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-23 17:09:04,384 - mmaction - INFO - \n",
      "top1_acc\t0.2941\n",
      "top5_acc\t0.9412\n",
      "2021-08-23 17:09:04,385 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-23 17:09:04,387 - mmaction - INFO - \n",
      "mean_acc\t0.1556\n",
      "2021-08-23 17:09:04,388 - mmaction - INFO - Epoch(val) [10][3]\ttop1_acc: 0.2941, top5_acc: 0.9412, mean_class_accuracy: 0.1556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 17/17, 10.0 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-23 17:11:06,176 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-23 17:11:06,178 - mmaction - INFO - \n",
      "top1_acc\t0.3529\n",
      "top5_acc\t1.0000\n",
      "2021-08-23 17:11:06,178 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-23 17:11:06,180 - mmaction - INFO - \n",
      "mean_acc\t0.3000\n",
      "2021-08-23 17:11:06,180 - mmaction - INFO - Epoch(val) [15][3]\ttop1_acc: 0.3529, top5_acc: 1.0000, mean_class_accuracy: 0.3000\n",
      "2021-08-23 17:13:06,673 - mmaction - INFO - Saving checkpoint at 20 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 17/17, 10.0 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-23 17:13:08,833 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-23 17:13:08,835 - mmaction - INFO - \n",
      "top1_acc\t0.2941\n",
      "top5_acc\t1.0000\n",
      "2021-08-23 17:13:08,835 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-23 17:13:08,838 - mmaction - INFO - \n",
      "mean_acc\t0.2111\n",
      "2021-08-23 17:13:08,839 - mmaction - INFO - Epoch(val) [20][3]\ttop1_acc: 0.2941, top5_acc: 1.0000, mean_class_accuracy: 0.2111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 17/17, 10.3 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-23 17:15:10,789 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-23 17:15:10,791 - mmaction - INFO - \n",
      "top1_acc\t0.4706\n",
      "top5_acc\t0.9412\n",
      "2021-08-23 17:15:10,791 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-23 17:15:10,793 - mmaction - INFO - \n",
      "mean_acc\t0.3000\n",
      "2021-08-23 17:15:11,107 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_25.pth.\n",
      "2021-08-23 17:15:11,108 - mmaction - INFO - Best top1_acc is 0.4706 at 25 epoch.\n",
      "2021-08-23 17:15:11,109 - mmaction - INFO - Epoch(val) [25][3]\ttop1_acc: 0.4706, top5_acc: 0.9412, mean_class_accuracy: 0.3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 17/17, 10.2 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-23 17:17:12,889 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-23 17:17:12,890 - mmaction - INFO - \n",
      "top1_acc\t0.4706\n",
      "top5_acc\t0.9412\n",
      "2021-08-23 17:17:12,891 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-23 17:17:12,893 - mmaction - INFO - \n",
      "mean_acc\t0.2833\n",
      "2021-08-23 17:17:12,894 - mmaction - INFO - Epoch(val) [30][3]\ttop1_acc: 0.4706, top5_acc: 0.9412, mean_class_accuracy: 0.2833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 17/17, 10.2 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-23 17:19:14,504 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-23 17:19:14,506 - mmaction - INFO - \n",
      "top1_acc\t0.3529\n",
      "top5_acc\t1.0000\n",
      "2021-08-23 17:19:14,507 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-23 17:19:14,508 - mmaction - INFO - \n",
      "mean_acc\t0.2333\n",
      "2021-08-23 17:19:14,509 - mmaction - INFO - Epoch(val) [35][3]\ttop1_acc: 0.3529, top5_acc: 1.0000, mean_class_accuracy: 0.2333\n",
      "2021-08-23 17:21:14,616 - mmaction - INFO - Saving checkpoint at 40 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 17/17, 10.6 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-23 17:21:16,604 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-23 17:21:16,605 - mmaction - INFO - \n",
      "top1_acc\t0.4118\n",
      "top5_acc\t1.0000\n",
      "2021-08-23 17:21:16,606 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-23 17:21:16,607 - mmaction - INFO - \n",
      "mean_acc\t0.2667\n",
      "2021-08-23 17:21:16,607 - mmaction - INFO - Epoch(val) [40][3]\ttop1_acc: 0.4118, top5_acc: 1.0000, mean_class_accuracy: 0.2667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 17/17, 10.2 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-23 17:23:18,403 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-23 17:23:18,405 - mmaction - INFO - \n",
      "top1_acc\t0.4706\n",
      "top5_acc\t1.0000\n",
      "2021-08-23 17:23:18,405 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-23 17:23:18,406 - mmaction - INFO - \n",
      "mean_acc\t0.2944\n",
      "2021-08-23 17:23:18,407 - mmaction - INFO - Epoch(val) [45][3]\ttop1_acc: 0.4706, top5_acc: 1.0000, mean_class_accuracy: 0.2944\n",
      "2021-08-23 17:25:18,120 - mmaction - INFO - Saving checkpoint at 50 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 17/17, 10.2 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-23 17:25:20,187 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-23 17:25:20,189 - mmaction - INFO - \n",
      "top1_acc\t0.4118\n",
      "top5_acc\t1.0000\n",
      "2021-08-23 17:25:20,190 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-23 17:25:20,191 - mmaction - INFO - \n",
      "mean_acc\t0.2667\n",
      "2021-08-23 17:25:20,192 - mmaction - INFO - Epoch(val) [50][3]\ttop1_acc: 0.4118, top5_acc: 1.0000, mean_class_accuracy: 0.2667\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "from mmaction.datasets import build_dataset\n",
    "from mmaction.models import build_model\n",
    "from mmaction.apis import train_model\n",
    "\n",
    "import mmcv\n",
    "\n",
    "# Build the dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# Build the recognizer\n",
    "model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.train_cfg, test_cfg=cfg.test_cfg)\n",
    "# CUDA_LAUNCH_BLOCKING=1\n",
    "# Create work_dir\n",
    "mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
    "train_model(model, datasets, cfg, distributed=False, validate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d6de14c-7f8d-4397-b11c-4bdcb42bb850",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f\"{cfg.work_dir}/model50e\", 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce03ef4d-608d-4108-b6da-5768cfc5b83c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 16:54:18,016 - mmaction - INFO - load model from: https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth\n",
      "2021-08-10 16:54:18,017 - mmaction - INFO - Use load_from_http loader\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "from mmaction.datasets import build_dataset\n",
    "from mmaction.models import build_model\n",
    "from mmaction.apis import train_model\n",
    "import pickle\n",
    "import mmcv\n",
    "# Build the dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# Build the recognizer\n",
    "model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "\n",
    "model = pickle.load(open(f\"{cfg.work_dir}/model50e\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39c23b5e-3e29-444f-beff-579b92ec9f1b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eyY3hCMwyTct",
    "outputId": "200e37c7-0da4-421f-98da-41418c3110ca",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 24/24, 0.6 task/s, elapsed: 39s, ETA:     0s\n",
      "Evaluating top_k_accuracy ...\n",
      "\n",
      "top1_acc\t0.4167\n",
      "top5_acc\t0.9583\n",
      "\n",
      "Evaluating mean_class_accuracy ...\n",
      "\n",
      "mean_acc\t0.2533\n",
      "top1_acc: 0.4167\n",
      "top5_acc: 0.9583\n",
      "mean_class_accuracy: 0.2533\n"
     ]
    }
   ],
   "source": [
    "from mmaction.apis import single_gpu_test\n",
    "from mmaction.datasets import build_dataloader\n",
    "from mmcv.parallel import MMDataParallel\n",
    "# from mmaction.models import build_model\n",
    "# from mmaction.datasets import build_dataset\n",
    "\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# Build a test dataloader\n",
    "dataset = build_dataset(cfg.data.test, dict(test_mode=True))\n",
    "data_loader = build_dataloader(\n",
    "        dataset,\n",
    "        videos_per_gpu=1,\n",
    "        workers_per_gpu=1,\n",
    "        dist=False,\n",
    "        shuffle=False)\n",
    "model = MMDataParallel(model, device_ids=[0])\n",
    "outputs = single_gpu_test(model, data_loader)\n",
    "\n",
    "eval_config = cfg.evaluation\n",
    "eval_config.pop('interval')\n",
    "eval_res = dataset.evaluate(outputs, **eval_config)\n",
    "for name, val in eval_res.items():\n",
    "    print(f'{name}: {val:.04f}')\n",
    "    \n",
    "output_config = cfg.output_config\n",
    "dataset.dump_results(outputs, **output_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f01ead32-8af1-42bb-b420-17b49f9eed24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from mmaction.core import confusion_matrix \n",
    "\n",
    "gt_labels = [ann['label'] for ann in dataset.load_annotations()]\n",
    "pred = np.argmax(outputs, axis=1)\n",
    "cf_mat = confusion_matrix(pred, gt_labels).astype(float)\n",
    "cmap = sns.cubehelix_palette(50, hue=0.05, rot=0, light=0.9, dark=0, as_cmap=True)\n",
    "# /np.sum(cf_mat), fmt='.2%',\n",
    "sns.heatmap(cf_mat, cmap=cmap, annot=True, xticklabels = ['6yo', '7yo', '8yo', '9yo', '10yo'], yticklabels = ['6yo', '7yo', '8yo', '9yo', '10yo'])\n",
    "# sns.heatmap(cf_mat, cmap=cmap, annot=True, xticklabels = ['7yo', '8yo', '9yo', '10yo'], yticklabels = ['7yo', '8yo', '9yo', '10yo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "857793bd-40c8-41c9-9ad8-01c3e9099da5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7083333333333334\n"
     ]
    }
   ],
   "source": [
    "mean_error = (2+3+4+4+2+1+1)/24 #number of all tested videos\n",
    "print(mean_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad56099-6810-496c-9b39-e7e3422a358d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75489caa-817b-4aad-9d27-bb7115885f7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4417965-41aa-4932-afa3-f53de8399825",
   "metadata": {},
   "source": [
    "# CSN age walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee0fe8f4-bf21-4251-b1b1-91b693d4f7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/robt427nv/childact\n"
     ]
    }
   ],
   "source": [
    "cd /home/robt427nv/childact/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc1ea81c-ac38-4c9f-b280-ce8960ae3216",
   "metadata": {
    "id": "LjCcmCKOjktc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mmcv import Config\n",
    "cfg = Config.fromfile('mmaction2/configs/recognition/csn/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ee47121-b6be-4ea8-b9ea-a55ddf1d520f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "tlhu9byjjt-K",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "13d1bb01-f351-48c0-9efb-0bdf2c125d29",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "model = dict(\n",
      "    type='Recognizer3D',\n",
      "    backbone=dict(\n",
      "        type='ResNet3dCSN',\n",
      "        pretrained2d=False,\n",
      "        pretrained=\n",
      "        'https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth',\n",
      "        depth=152,\n",
      "        with_pool2=False,\n",
      "        bottleneck_mode='ir',\n",
      "        norm_eval=True,\n",
      "        zero_init_residual=False,\n",
      "        bn_frozen=True),\n",
      "    cls_head=dict(\n",
      "        type='I3DHead',\n",
      "        num_classes=6,\n",
      "        in_channels=2048,\n",
      "        spatial_type='avg',\n",
      "        dropout_ratio=0.5,\n",
      "        init_std=0.01),\n",
      "    train_cfg=None,\n",
      "    test_cfg=dict(average_clips='prob'))\n",
      "checkpoint_config = dict(interval=20)\n",
      "log_config = dict(interval=100, hooks=[dict(type='TextLoggerHook')])\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = 'checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth'\n",
      "resume_from = None\n",
      "workflow = [('train', 1)]\n",
      "dataset_type = 'RawframeDataset'\n",
      "data_root = 'age-gender-3split-rgb-frames/'\n",
      "data_root_val = 'age-gender-3split-rgb-frames/val/'\n",
      "ann_file_train = 'age-gender-3split-rgb-frames/childact_train_rgb320_age_walk.txt'\n",
      "ann_file_val = 'age-gender-3split-rgb-frames/childact_val_rgb320_age_walk.txt'\n",
      "ann_file_test = 'age-gender-3split-rgb-frames/childact_test_rgb320_age_walk.txt'\n",
      "img_norm_cfg = dict(\n",
      "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_bgr=False)\n",
      "train_pipeline = [\n",
      "    dict(type='SampleFrames', clip_len=32, frame_interval=2, num_clips=1),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='RandomResizedCrop'),\n",
      "    dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "    dict(type='Flip', flip_ratio=0.5),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "]\n",
      "val_pipeline = [\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=32,\n",
      "        frame_interval=2,\n",
      "        num_clips=1,\n",
      "        test_mode=True),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='CenterCrop', crop_size=224),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=32,\n",
      "        frame_interval=2,\n",
      "        num_clips=10,\n",
      "        test_mode=True),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='ThreeCrop', crop_size=256),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "data = dict(\n",
      "    videos_per_gpu=6,\n",
      "    workers_per_gpu=4,\n",
      "    train=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_train_rgb320_age_walk.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/train/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=1),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='RandomResizedCrop'),\n",
      "            dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "            dict(type='Flip', flip_ratio=0.5),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'),\n",
      "    val=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_val_rgb320_age_walk.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/val/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=1,\n",
      "                test_mode=True),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='CenterCrop', crop_size=224),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'),\n",
      "    test=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_test_rgb320_age_walk.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/test/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=10,\n",
      "                test_mode=True),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='ThreeCrop', crop_size=256),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'))\n",
      "evaluation = dict(\n",
      "    interval=5, metrics=['top_k_accuracy', 'mean_class_accuracy'])\n",
      "optimizer = dict(type='SGD', lr=0.000125, momentum=0.9, weight_decay=0.0001)\n",
      "optimizer_config = dict(grad_clip=dict(max_norm=40, norm_type=2))\n",
      "lr_config = dict(\n",
      "    policy='step',\n",
      "    step=[32, 48],\n",
      "    warmup='linear',\n",
      "    warmup_ratio=0.1,\n",
      "    warmup_by_epoch=True,\n",
      "    warmup_iters=16)\n",
      "total_epochs = 50\n",
      "work_dir = './childact-checkpoints/CSN-age-walk'\n",
      "find_unused_parameters = True\n",
      "omnisource = False\n",
      "seed = 42\n",
      "gpu_ids = range(0, 1)\n",
      "output_config = dict(out='./childact-checkpoints/CSN-age-walk/results.json')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mmcv.runner import set_random_seed\n",
    "\n",
    "# Modify dataset type and path\n",
    "# cfg.dataset_type = 'VideoDataset'\n",
    "cfg.data_root = 'age-gender-3split-rgb-frames/'\n",
    "cfg.data_root_val = 'age-gender-3split-rgb-frames/val/'\n",
    "cfg.ann_file_train = 'age-gender-3split-rgb-frames/childact_train_rgb320_age_walk.txt'\n",
    "cfg.ann_file_val = 'age-gender-3split-rgb-frames/childact_val_rgb320_age_walk.txt'\n",
    "cfg.ann_file_test = 'age-gender-3split-rgb-frames/childact_test_rgb320_age_walk.txt'\n",
    "\n",
    "# cfg.data.test.type = 'VideoDataset'\n",
    "cfg.data.test.ann_file = 'age-gender-3split-rgb-frames/childact_test_rgb320_age_walk.txt'\n",
    "cfg.data.test.data_prefix = 'age-gender-3split-rgb-frames/test/'\n",
    "\n",
    "# cfg.data.train.type = 'VideoDataset'\n",
    "cfg.data.train.ann_file = 'age-gender-3split-rgb-frames/childact_train_rgb320_age_walk.txt'\n",
    "cfg.data.train.data_prefix = 'age-gender-3split-rgb-frames/train/'\n",
    "\n",
    "# cfg.data.val.type = 'VideoDataset'\n",
    "cfg.data.val.ann_file = 'age-gender-3split-rgb-frames/childact_val_rgb320_age_walk.txt'\n",
    "cfg.data.val.data_prefix = 'age-gender-3split-rgb-frames/val/'\n",
    "\n",
    "# cfg.data.test.modality = 'Flow'\n",
    "# cfg.data.val.modality = 'Flow'\n",
    "# cfg.data.train.modality = 'Flow'\n",
    "\n",
    "# cfg.data.train.start_index = 0\n",
    "# cfg.data.test.start_index = 0\n",
    "# cfg.data.val.start_index = 0\n",
    "\n",
    "cfg.data.test.filename_tmpl = '{:03}.jpeg'\n",
    "cfg.data.train.filename_tmpl = '{:03}.jpeg'\n",
    "cfg.data.val.filename_tmpl = '{:03}.jpeg'\n",
    "# The flag is used to determine whether it is omnisource training\n",
    "cfg.setdefault('omnisource', False)\n",
    "# Modify num classes of the model in cls_head\n",
    "cfg.model.cls_head.num_classes = 6\n",
    "# We can use the pre-trained TSN model\n",
    "cfg.load_from = 'checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth'\n",
    "# cfg.resume_from = './childact-mm/latest.pth'\n",
    "\n",
    "# Set up working dir to save files and logs.\n",
    "cfg.work_dir = './childact-checkpoints/CSN-age-walk'\n",
    "\n",
    "cfg.total_epochs = 50\n",
    "\n",
    "# cfg.momentum_config = dict(\n",
    "#     policy='cyclic',\n",
    "#     target_ratio=(0.85 / 0.95, 1),\n",
    "#     cyclic_times=1,\n",
    "#     step_ratio_up=0.4,\n",
    "# )\n",
    "\n",
    "# We can set the checkpoint saving interval to reduce the storage cost\n",
    "cfg.checkpoint_config.interval = 20\n",
    "# We can set the log print interval to reduce the the times of printing log\n",
    "cfg.log_config.interval = 100\n",
    "\n",
    "# Set seed thus the results are more reproducible\n",
    "cfg.seed = 42\n",
    "set_random_seed(42, deterministic=False)\n",
    "cfg.gpu_ids = range(1)\n",
    "\n",
    "cfg.data.videos_per_gpu=6\n",
    "\n",
    "# cfg.model.backbone.in_channels = 2\n",
    "\n",
    "cfg.output_config = dict(out=f'{cfg.work_dir}/results.json')\n",
    "# We can initialize the logger for training and have a look\n",
    "# at the final config used for training\n",
    "# del cfg.optimizer['momentum']\n",
    "print(f'Config:\\n{cfg.pretty_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5980b85-cadc-45c3-a583-87d2ea87bb6f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dDBWkdDRk6oz",
    "outputId": "85a52ef3-7b5c-4c52-8fef-00322a8c65e6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-23 22:56:23,515 - mmaction - INFO - load model from: https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth\n",
      "2021-08-23 22:56:23,516 - mmaction - INFO - Use load_from_http loader\n",
      "2021-08-23 22:56:25,627 - mmaction - INFO - load checkpoint from checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth\n",
      "2021-08-23 22:56:25,628 - mmaction - INFO - Use load_from_local loader\n",
      "2021-08-23 22:56:25,821 - mmaction - WARNING - The model and loaded state dict do not match exactly\n",
      "\n",
      "size mismatch for cls_head.fc_cls.weight: copying a param with shape torch.Size([400, 2048]) from checkpoint, the shape in current model is torch.Size([6, 2048]).\n",
      "size mismatch for cls_head.fc_cls.bias: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([6]).\n",
      "2021-08-23 22:56:25,827 - mmaction - INFO - Start running, host: robt427nv@robt427NV, work_dir: /home/robt427nv/childact/childact-checkpoints/CSN-age-walk\n",
      "2021-08-23 22:56:25,828 - mmaction - INFO - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(ABOVE_NORMAL) OptimizerHook                      \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "2021-08-23 22:56:25,829 - mmaction - INFO - workflow: [('train', 1)], max: 50 epochs\n",
      "/home/robt427nv/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/mmcv/runner/hooks/evaluation.py:190: UserWarning: runner.meta is None. Creating an empty one.\n",
      "  warnings.warn('runner.meta is None. Creating an empty one.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 20/20, 10.7 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-23 22:57:58,971 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-23 22:57:58,973 - mmaction - INFO - \n",
      "top1_acc\t0.2500\n",
      "top5_acc\t0.9500\n",
      "2021-08-23 22:57:58,973 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-23 22:57:58,974 - mmaction - INFO - \n",
      "mean_acc\t0.1905\n",
      "2021-08-23 22:57:59,297 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_5.pth.\n",
      "2021-08-23 22:57:59,298 - mmaction - INFO - Best top1_acc is 0.2500 at 5 epoch.\n",
      "2021-08-23 22:57:59,299 - mmaction - INFO - Epoch(val) [5][4]\ttop1_acc: 0.2500, top5_acc: 0.9500, mean_class_accuracy: 0.1905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 20/20, 10.7 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-23 22:59:31,058 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-23 22:59:31,059 - mmaction - INFO - \n",
      "top1_acc\t0.2500\n",
      "top5_acc\t0.9500\n",
      "2021-08-23 22:59:31,060 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-23 22:59:31,061 - mmaction - INFO - \n",
      "mean_acc\t0.1726\n",
      "2021-08-23 22:59:31,062 - mmaction - INFO - Epoch(val) [10][4]\ttop1_acc: 0.2500, top5_acc: 0.9500, mean_class_accuracy: 0.1726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 20/20, 11.2 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-23 23:01:02,645 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-23 23:01:02,646 - mmaction - INFO - \n",
      "top1_acc\t0.5000\n",
      "top5_acc\t0.9500\n",
      "2021-08-23 23:01:02,647 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-23 23:01:02,648 - mmaction - INFO - \n",
      "mean_acc\t0.3202\n",
      "2021-08-23 23:01:03,080 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_15.pth.\n",
      "2021-08-23 23:01:03,081 - mmaction - INFO - Best top1_acc is 0.5000 at 15 epoch.\n",
      "2021-08-23 23:01:03,082 - mmaction - INFO - Epoch(val) [15][4]\ttop1_acc: 0.5000, top5_acc: 0.9500, mean_class_accuracy: 0.3202\n",
      "2021-08-23 23:02:33,152 - mmaction - INFO - Saving checkpoint at 20 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 20/20, 10.7 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-23 23:02:35,443 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-23 23:02:35,444 - mmaction - INFO - \n",
      "top1_acc\t0.5000\n",
      "top5_acc\t0.9500\n",
      "2021-08-23 23:02:35,444 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-23 23:02:35,444 - mmaction - INFO - \n",
      "mean_acc\t0.3024\n",
      "2021-08-23 23:02:35,445 - mmaction - INFO - Epoch(val) [20][4]\ttop1_acc: 0.5000, top5_acc: 0.9500, mean_class_accuracy: 0.3024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 20/20, 10.7 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-23 23:04:07,604 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-23 23:04:07,606 - mmaction - INFO - \n",
      "top1_acc\t0.5000\n",
      "top5_acc\t0.9500\n",
      "2021-08-23 23:04:07,606 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-23 23:04:07,607 - mmaction - INFO - \n",
      "mean_acc\t0.3202\n",
      "2021-08-23 23:04:07,608 - mmaction - INFO - Epoch(val) [25][4]\ttop1_acc: 0.5000, top5_acc: 0.9500, mean_class_accuracy: 0.3202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 20/20, 10.8 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-23 23:05:39,939 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-23 23:05:39,940 - mmaction - INFO - \n",
      "top1_acc\t0.2500\n",
      "top5_acc\t0.9500\n",
      "2021-08-23 23:05:39,941 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-23 23:05:39,942 - mmaction - INFO - \n",
      "mean_acc\t0.1476\n",
      "2021-08-23 23:05:39,943 - mmaction - INFO - Epoch(val) [30][4]\ttop1_acc: 0.2500, top5_acc: 0.9500, mean_class_accuracy: 0.1476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 20/20, 11.3 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-23 23:07:11,709 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-23 23:07:11,711 - mmaction - INFO - \n",
      "top1_acc\t0.3000\n",
      "top5_acc\t0.9500\n",
      "2021-08-23 23:07:11,712 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-23 23:07:11,713 - mmaction - INFO - \n",
      "mean_acc\t0.1964\n",
      "2021-08-23 23:07:11,714 - mmaction - INFO - Epoch(val) [35][4]\ttop1_acc: 0.3000, top5_acc: 0.9500, mean_class_accuracy: 0.1964\n",
      "2021-08-23 23:08:41,428 - mmaction - INFO - Saving checkpoint at 40 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 20/20, 10.7 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-23 23:08:43,704 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-23 23:08:43,705 - mmaction - INFO - \n",
      "top1_acc\t0.4000\n",
      "top5_acc\t1.0000\n",
      "2021-08-23 23:08:43,706 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-23 23:08:43,706 - mmaction - INFO - \n",
      "mean_acc\t0.2536\n",
      "2021-08-23 23:08:43,707 - mmaction - INFO - Epoch(val) [40][4]\ttop1_acc: 0.4000, top5_acc: 1.0000, mean_class_accuracy: 0.2536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 20/20, 11.0 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-23 23:10:15,633 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-23 23:10:15,636 - mmaction - INFO - \n",
      "top1_acc\t0.4500\n",
      "top5_acc\t0.9500\n",
      "2021-08-23 23:10:15,636 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-23 23:10:15,638 - mmaction - INFO - \n",
      "mean_acc\t0.2964\n",
      "2021-08-23 23:10:15,639 - mmaction - INFO - Epoch(val) [45][4]\ttop1_acc: 0.4500, top5_acc: 0.9500, mean_class_accuracy: 0.2964\n",
      "2021-08-23 23:11:45,494 - mmaction - INFO - Saving checkpoint at 50 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 20/20, 10.9 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-23 23:11:47,748 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-23 23:11:47,759 - mmaction - INFO - \n",
      "top1_acc\t0.3000\n",
      "top5_acc\t0.9500\n",
      "2021-08-23 23:11:47,759 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-23 23:11:47,762 - mmaction - INFO - \n",
      "mean_acc\t0.1881\n",
      "2021-08-23 23:11:47,763 - mmaction - INFO - Epoch(val) [50][4]\ttop1_acc: 0.3000, top5_acc: 0.9500, mean_class_accuracy: 0.1881\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "from mmaction.datasets import build_dataset\n",
    "from mmaction.models import build_model\n",
    "from mmaction.apis import train_model\n",
    "\n",
    "import mmcv\n",
    "\n",
    "# Build the dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# Build the recognizer\n",
    "model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.train_cfg, test_cfg=cfg.test_cfg)\n",
    "# CUDA_LAUNCH_BLOCKING=1\n",
    "# Create work_dir\n",
    "mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
    "train_model(model, datasets, cfg, distributed=False, validate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4715fb04-d960-41ee-b55f-186c810b3f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f\"{cfg.work_dir}/model50e\", 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "246e9bf5-0e9b-4f36-9528-fd3608b63d0c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 16:54:18,016 - mmaction - INFO - load model from: https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth\n",
      "2021-08-10 16:54:18,017 - mmaction - INFO - Use load_from_http loader\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "from mmaction.datasets import build_dataset\n",
    "from mmaction.models import build_model\n",
    "from mmaction.apis import train_model\n",
    "import pickle\n",
    "import mmcv\n",
    "# Build the dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# Build the recognizer\n",
    "model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "\n",
    "model = pickle.load(open(f\"{cfg.work_dir}/model50e\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fc91578-0bb3-4f1f-8c78-6dbafdfd5004",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eyY3hCMwyTct",
    "outputId": "200e37c7-0da4-421f-98da-41418c3110ca",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 11/11, 0.2 task/s, elapsed: 51s, ETA:     0s\n",
      "Evaluating top_k_accuracy ...\n",
      "\n",
      "top1_acc\t0.4545\n",
      "top5_acc\t0.9091\n",
      "\n",
      "Evaluating mean_class_accuracy ...\n",
      "\n",
      "mean_acc\t0.6389\n",
      "top1_acc: 0.4545\n",
      "top5_acc: 0.9091\n",
      "mean_class_accuracy: 0.6389\n"
     ]
    }
   ],
   "source": [
    "from mmaction.apis import single_gpu_test\n",
    "from mmaction.datasets import build_dataloader\n",
    "from mmcv.parallel import MMDataParallel\n",
    "# from mmaction.models import build_model\n",
    "# from mmaction.datasets import build_dataset\n",
    "\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# Build a test dataloader\n",
    "dataset = build_dataset(cfg.data.test, dict(test_mode=True))\n",
    "data_loader = build_dataloader(\n",
    "        dataset,\n",
    "        videos_per_gpu=1,\n",
    "        workers_per_gpu=1,\n",
    "        dist=False,\n",
    "        shuffle=False)\n",
    "model = MMDataParallel(model, device_ids=[0])\n",
    "outputs = single_gpu_test(model, data_loader)\n",
    "\n",
    "eval_config = cfg.evaluation\n",
    "eval_config.pop('interval')\n",
    "eval_res = dataset.evaluate(outputs, **eval_config)\n",
    "for name, val in eval_res.items():\n",
    "    print(f'{name}: {val:.04f}')\n",
    "    \n",
    "output_config = cfg.output_config\n",
    "dataset.dump_results(outputs, **output_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9d80380-c516-428b-8fee-f77bd0bfd6aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from mmaction.core import confusion_matrix \n",
    "\n",
    "gt_labels = [ann['label'] for ann in dataset.load_annotations()]\n",
    "pred = np.argmax(outputs, axis=1)\n",
    "cf_mat = confusion_matrix(pred, gt_labels).astype(float)\n",
    "cmap = sns.cubehelix_palette(50, hue=0.05, rot=0, light=0.9, dark=0, as_cmap=True)\n",
    "# /np.sum(cf_mat), fmt='.2%',\n",
    "sns.heatmap(cf_mat, cmap=cmap, annot=True, xticklabels = ['6yo', '7yo', '8yo'], yticklabels = ['6yo', '7yo', '8yo'])\n",
    "# sns.heatmap(cf_mat, cmap=cmap, annot=True, xticklabels = ['7yo', '8yo', '9yo', '10yo'], yticklabels = ['7yo', '8yo', '9yo', '10yo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d913e147-f21f-493b-a2b5-656b6d69ff53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8181818181818182\n"
     ]
    }
   ],
   "source": [
    "mean_error = (2+6+1)/11 #number of all tested videos\n",
    "print(mean_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d52f32d-5203-49b4-ae61-2b8ff2ce6bda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e05164d-f5e3-4b27-90d6-3c8ac8333984",
   "metadata": {},
   "source": [
    "# Gender recognition from actions with CSN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c387a898-e2c2-4f9a-89c5-e5880b99cdb3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bf8PpPXtVvmg",
    "outputId": "2c685a33-474b-4e71-8f98-c2533c66095e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2019 NVIDIA Corporation\n",
      "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
      "Cuda compilation tools, release 10.1, V10.1.243\n",
      "gcc (Ubuntu 8.4.0-3ubuntu2) 8.4.0\n",
      "Copyright (C) 2018 Free Software Foundation, Inc.\n",
      "This is free software; see the source for copying conditions.  There is NO\n",
      "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check nvcc version\n",
    "!nvcc -V\n",
    "# Check GCC version\n",
    "!gcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42fcd6f9-e94e-4059-9bbd-a3c34e879624",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5PAJ4ArzV5Ry",
    "outputId": "e48dbf61-fae0-431c-e964-04c7caaee4bc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install some optional requirements\n",
    "# !pip install -r requirements/optional.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88f32778-1d0a-4df2-839d-140081efb6e4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "No_zZAFpWC-a",
    "outputId": "1d425eea-d44e-434a-991c-01eb15abaab2",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0 True\n",
      "0.16.0\n",
      "10.2\n",
      "GCC 7.3\n"
     ]
    }
   ],
   "source": [
    "# Check Pytorch installation\n",
    "import torch, torchvision\n",
    "print(torch.__version__, torch.cuda.is_available())\n",
    "\n",
    "# Check MMAction2 installation\n",
    "import mmaction\n",
    "print(mmaction.__version__)\n",
    "\n",
    "# Check MMCV installation\n",
    "from mmcv.ops import get_compiling_cuda_version, get_compiler_version\n",
    "print(get_compiling_cuda_version())\n",
    "print(get_compiler_version())\n",
    "\n",
    "import sys\n",
    "# sys.path.append('/home/actrec/.local/lib/python3.6/site-packages/decord-0.5.3-py3.6-linux-x86_64.egg')\n",
    "# import decord\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db0a4fa5-0d5b-4420-8a9f-6309f5c90766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/robt427nv/childact\n"
     ]
    }
   ],
   "source": [
    "cd /home/robt427nv/childact/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71d25ae8-362a-446e-aaeb-f16769a52115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/robt427nv/childact'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1d82e38-156e-4947-be2e-8f7d448387b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \u001b[0m\u001b[01;34mage-gender-3split-rgb-frames\u001b[0m/\n",
      " \u001b[01;34mage-gender-3split-vids\u001b[0m/\n",
      " \u001b[01;34mcheckpoints\u001b[0m/\n",
      " ChildAct_age_gender.csv\n",
      " \u001b[34;42mchildact_videos_nosplit\u001b[0m/\n",
      " \u001b[01;34mmmaction2\u001b[0m/\n",
      " \u001b[01;32mmy-mmaction.ipynb\u001b[0m*\n",
      "\u001b[01;32m'Split by Folders and Annotation Files creation.ipynb'\u001b[0m*\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09af24c9-89bf-4e69-88e0-334d26735a45",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "# CSN gender "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "027528db-246f-417a-b543-9281b0ce7f16",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "64CW6d_AaT-Q",
    "outputId": "3b284fd8-4ee7-4a34-90d7-5023cd123a04",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-07-21 17:39:20--  https://download.openmmlab.com/mmaction/recognition/csn/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth\n",
      "Resolving download.openmmlab.com (download.openmmlab.com)... 47.75.20.25\n",
      "Connecting to download.openmmlab.com (download.openmmlab.com)|47.75.20.25|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 119580180 (114M) [application/octet-stream]\n",
      "Saving to: ‘checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth’\n",
      "\n",
      "checkpoints/ircsn_i 100%[===================>] 114,04M  6,78MB/s    in 17s     \n",
      "\n",
      "2021-07-21 17:39:39 (6,87 MB/s) - ‘checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth’ saved [119580180/119580180]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir checkpoints\n",
    "!wget -c https://download.openmmlab.com/mmaction/recognition/csn/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth \\\n",
    "      -O checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "91b025c7-4c84-456b-864a-6613a3cd53f1",
   "metadata": {
    "id": "LjCcmCKOjktc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mmcv import Config\n",
    "cfg = Config.fromfile('mmaction2/configs/recognition/csn/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "55c42484-9547-4010-8dd9-d848414e3ad5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "tlhu9byjjt-K",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "13d1bb01-f351-48c0-9efb-0bdf2c125d29",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "model = dict(\n",
      "    type='Recognizer3D',\n",
      "    backbone=dict(\n",
      "        type='ResNet3dCSN',\n",
      "        pretrained2d=False,\n",
      "        pretrained=\n",
      "        'https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth',\n",
      "        depth=152,\n",
      "        with_pool2=False,\n",
      "        bottleneck_mode='ir',\n",
      "        norm_eval=True,\n",
      "        zero_init_residual=False,\n",
      "        bn_frozen=True),\n",
      "    cls_head=dict(\n",
      "        type='I3DHead',\n",
      "        num_classes=2,\n",
      "        in_channels=2048,\n",
      "        spatial_type='avg',\n",
      "        dropout_ratio=0.5,\n",
      "        init_std=0.01),\n",
      "    train_cfg=None,\n",
      "    test_cfg=dict(average_clips='prob'))\n",
      "checkpoint_config = dict(interval=20)\n",
      "log_config = dict(interval=100, hooks=[dict(type='TextLoggerHook')])\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = 'checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth'\n",
      "resume_from = None\n",
      "workflow = [('train', 1)]\n",
      "dataset_type = 'RawframeDataset'\n",
      "data_root = 'age-gender-3split-rgb-frames/'\n",
      "data_root_val = 'age-gender-3split-rgb-frames/val/'\n",
      "ann_file_train = 'age-gender-3split-rgb-frames/childact_train_rgb320_gender.txt'\n",
      "ann_file_val = 'age-gender-3split-rgb-frames/childact_val_rgb320_gender.txt'\n",
      "ann_file_test = 'age-gender-3split-rgb-frames/childact_test_rgb320_gender.txt'\n",
      "img_norm_cfg = dict(\n",
      "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_bgr=False)\n",
      "train_pipeline = [\n",
      "    dict(type='SampleFrames', clip_len=32, frame_interval=2, num_clips=1),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='RandomResizedCrop'),\n",
      "    dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "    dict(type='Flip', flip_ratio=0.5),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "]\n",
      "val_pipeline = [\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=32,\n",
      "        frame_interval=2,\n",
      "        num_clips=1,\n",
      "        test_mode=True),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='CenterCrop', crop_size=224),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=32,\n",
      "        frame_interval=2,\n",
      "        num_clips=10,\n",
      "        test_mode=True),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='ThreeCrop', crop_size=256),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "data = dict(\n",
      "    videos_per_gpu=8,\n",
      "    workers_per_gpu=4,\n",
      "    train=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_train_rgb320_gender.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/train/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=1),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='RandomResizedCrop'),\n",
      "            dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "            dict(type='Flip', flip_ratio=0.5),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'),\n",
      "    val=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file='age-gender-3split-rgb-frames/childact_val_rgb320_gender.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/val/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=1,\n",
      "                test_mode=True),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='CenterCrop', crop_size=224),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'),\n",
      "    test=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file='age-gender-3split-rgb-frames/childact_test_rgb320_gender.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/test/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=10,\n",
      "                test_mode=True),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='ThreeCrop', crop_size=256),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'))\n",
      "evaluation = dict(\n",
      "    interval=5, metrics=['top_k_accuracy', 'mean_class_accuracy'])\n",
      "optimizer = dict(type='SGD', lr=0.000125, momentum=0.9, weight_decay=0.0001)\n",
      "optimizer_config = dict(grad_clip=dict(max_norm=40, norm_type=2))\n",
      "lr_config = dict(\n",
      "    policy='step',\n",
      "    step=[32, 48],\n",
      "    warmup='linear',\n",
      "    warmup_ratio=0.1,\n",
      "    warmup_by_epoch=True,\n",
      "    warmup_iters=16)\n",
      "total_epochs = 50\n",
      "work_dir = './childact-checkpoints/CSN-gender'\n",
      "find_unused_parameters = True\n",
      "omnisource = False\n",
      "seed = 42\n",
      "gpu_ids = range(0, 1)\n",
      "output_config = dict(out='./childact-checkpoints/CSN-gender/results.json')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mmcv.runner import set_random_seed\n",
    "\n",
    "# Modify dataset type and path\n",
    "# cfg.dataset_type = 'VideoDataset'\n",
    "cfg.data_root = 'age-gender-3split-rgb-frames/'\n",
    "cfg.data_root_val = 'age-gender-3split-rgb-frames/val/'\n",
    "cfg.ann_file_train = 'age-gender-3split-rgb-frames/childact_train_rgb320_gender.txt'\n",
    "cfg.ann_file_val = 'age-gender-3split-rgb-frames/childact_val_rgb320_gender.txt'\n",
    "cfg.ann_file_test = 'age-gender-3split-rgb-frames/childact_test_rgb320_gender.txt'\n",
    "\n",
    "# cfg.data.test.type = 'VideoDataset'\n",
    "cfg.data.test.ann_file = 'age-gender-3split-rgb-frames/childact_test_rgb320_gender.txt'\n",
    "cfg.data.test.data_prefix = 'age-gender-3split-rgb-frames/test/'\n",
    "\n",
    "# cfg.data.train.type = 'VideoDataset'\n",
    "cfg.data.train.ann_file = 'age-gender-3split-rgb-frames/childact_train_rgb320_gender.txt'\n",
    "cfg.data.train.data_prefix = 'age-gender-3split-rgb-frames/train/'\n",
    "\n",
    "# cfg.data.val.type = 'VideoDataset'\n",
    "cfg.data.val.ann_file = 'age-gender-3split-rgb-frames/childact_val_rgb320_gender.txt'\n",
    "cfg.data.val.data_prefix = 'age-gender-3split-rgb-frames/val/'\n",
    "\n",
    "# cfg.data.test.modality = 'Flow'\n",
    "# cfg.data.val.modality = 'Flow'\n",
    "# cfg.data.train.modality = 'Flow'\n",
    "\n",
    "# cfg.data.train.start_index = 0\n",
    "# cfg.data.test.start_index = 0\n",
    "# cfg.data.val.start_index = 0\n",
    "\n",
    "cfg.data.test.filename_tmpl = '{:03}.jpeg'\n",
    "cfg.data.train.filename_tmpl = '{:03}.jpeg'\n",
    "cfg.data.val.filename_tmpl = '{:03}.jpeg'\n",
    "# The flag is used to determine whether it is omnisource training\n",
    "cfg.setdefault('omnisource', False)\n",
    "# Modify num classes of the model in cls_head\n",
    "cfg.model.cls_head.num_classes = 2\n",
    "# We can use the pre-trained TSN model\n",
    "cfg.load_from = 'checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth'\n",
    "# cfg.resume_from = './childact-mm/latest.pth'\n",
    "\n",
    "# Set up working dir to save files and logs.\n",
    "cfg.work_dir = './childact-checkpoints/CSN-gender'\n",
    "\n",
    "# cfg.img_norm_cfg = dict(mean=[128, 128], std=[128, 128])\n",
    "\n",
    "\n",
    "# cfg.val_pipeline = [\n",
    "#     dict(\n",
    "#         type='SampleFrames',\n",
    "#         clip_len=32,\n",
    "#         frame_interval=2,\n",
    "#         num_clips=1,\n",
    "#         test_mode=True),\n",
    "#     dict(type='RawFrameDecode'),\n",
    "#     dict(type='Resize', scale=(-1, 256)),\n",
    "#     dict(type='CenterCrop', crop_size=224),\n",
    "# #     dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
    "# #     dict(type='Flip', flip_ratio=0.5)\n",
    "#     dict(type='Normalize', mean=[128, 128], std=[128, 128]),\n",
    "#     dict(type='FormatShape', input_format='NCTHW'),\n",
    "#     dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
    "#     dict(type='ToTensor', keys=['imgs'])\n",
    "# ]\n",
    "# cfg.test_pipeline = [\n",
    "#     dict(type='SampleFrames', clip_len=32, frame_interval=2, num_clips=10, test_mode=True),\n",
    "#     dict(type='RawFrameDecode'),\n",
    "#     dict(type='Resize', scale=(-1, 256)),\n",
    "#     dict(type='CenterCrop', crop_size=224),\n",
    "#     dict(type='Normalize', mean=[128, 128], std=[128, 128]),\n",
    "#     dict(type='FormatShape', input_format='NCTHW'),\n",
    "#     dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
    "#     dict(type='ToTensor', keys=['imgs'])\n",
    "# ]\n",
    "\n",
    "\n",
    "\n",
    "# cfg.train_pipeline = [\n",
    "#     dict(type='SampleFrames', clip_len=32, frame_interval=2, num_clips=1),\n",
    "#     dict(type='RawFrameDecode'),\n",
    "#     dict(type='Resize', scale=(-1, 256)),\n",
    "# #     dict(type='RandomCrop', size=224),\n",
    "#     dict(type='RandomResizedCrop'),\n",
    "#     dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
    "# #     dict(type='Flip', flip_ratio=0.5),\n",
    "#     dict(type='Normalize', mean=[128, 128], std=[128, 128]),\n",
    "#     dict(type='FormatShape', input_format='NCTHW'),\n",
    "#     dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
    "#     dict(type='ToTensor', keys=['imgs', 'label'])\n",
    "# ]\n",
    "\n",
    "# cfg.data.train.pipeline = cfg.train_pipeline\n",
    "# cfg.data.val.pipeline = cfg.val_pipeline\n",
    "# cfg.data.test.pipeline = cfg.test_pipeline\n",
    "\n",
    "\n",
    "# The original learning rate (LR) is set for 8-GPU training.\n",
    "# We divide it by 8 since we only use one GPU.\n",
    "# cfg.data.videos_per_gpu = 24\n",
    "# cfg.optimizer.type = 'Adam'\n",
    "# cfg.optimizer.weight_decay=0.0001\n",
    "\n",
    "# cfg.optimizer_config.grad_clip=None\n",
    "# cfg.optimizer.lr = 0.01\n",
    "\n",
    "# cfg.lr_config = dict(\n",
    "#     policy='cyclic',\n",
    "#     target_ratio=(10, 1e-5),\n",
    "#     cyclic_times=1,\n",
    "#     step_ratio_up=0.4,\n",
    "# )\n",
    "\n",
    "cfg.total_epochs = 50\n",
    "\n",
    "# cfg.momentum_config = dict(\n",
    "#     policy='cyclic',\n",
    "#     target_ratio=(0.85 / 0.95, 1),\n",
    "#     cyclic_times=1,\n",
    "#     step_ratio_up=0.4,\n",
    "# )\n",
    "\n",
    "# We can set the checkpoint saving interval to reduce the storage cost\n",
    "cfg.checkpoint_config.interval = 20\n",
    "# We can set the log print interval to reduce the the times of printing log\n",
    "cfg.log_config.interval = 100\n",
    "\n",
    "# Set seed thus the results are more reproducible\n",
    "cfg.seed = 42\n",
    "set_random_seed(42, deterministic=False)\n",
    "cfg.gpu_ids = range(1)\n",
    "\n",
    "cfg.data.videos_per_gpu=8\n",
    "\n",
    "# cfg.model.backbone.in_channels = 2\n",
    "\n",
    "cfg.output_config = dict(out=f'{cfg.work_dir}/results.json')\n",
    "# We can initialize the logger for training and have a look\n",
    "# at the final config used for training\n",
    "# del cfg.optimizer['momentum']\n",
    "print(f'Config:\\n{cfg.pretty_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "736e5d8f-b95c-4bf9-aa96-7067e715b27e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dDBWkdDRk6oz",
    "outputId": "85a52ef3-7b5c-4c52-8fef-00322a8c65e6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-21 19:01:01,494 - mmaction - INFO - load model from: https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth\n",
      "2021-07-21 19:01:01,495 - mmaction - INFO - Use load_from_http loader\n",
      "2021-07-21 19:01:01,711 - mmaction - INFO - load checkpoint from checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth\n",
      "2021-07-21 19:01:01,712 - mmaction - INFO - Use load_from_local loader\n",
      "2021-07-21 19:01:01,873 - mmaction - WARNING - The model and loaded state dict do not match exactly\n",
      "\n",
      "size mismatch for cls_head.fc_cls.weight: copying a param with shape torch.Size([400, 2048]) from checkpoint, the shape in current model is torch.Size([2, 2048]).\n",
      "size mismatch for cls_head.fc_cls.bias: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([2]).\n",
      "2021-07-21 19:01:01,875 - mmaction - INFO - Start running, host: robt427nv@robt427NV, work_dir: /home/robt427nv/childact/childact-checkpoints/CSN-gender\n",
      "2021-07-21 19:01:01,876 - mmaction - INFO - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(ABOVE_NORMAL) OptimizerHook                      \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "2021-07-21 19:01:01,876 - mmaction - INFO - workflow: [('train', 1)], max: 50 epochs\n",
      "2021-07-21 19:02:46,763 - mmaction - INFO - Epoch [1][100/119]\tlr: 1.835e-05, eta: 1:42:15, time: 1.049, data_time: 0.038, memory: 21985, top1_acc: 0.5863, top5_acc: 1.0000, loss_cls: 0.6680, loss: 0.6680, grad_norm: 28.0138\n",
      "2021-07-21 19:04:51,919 - mmaction - INFO - Epoch [2][100/119]\tlr: 2.538e-05, eta: 1:32:13, time: 1.066, data_time: 0.041, memory: 21985, top1_acc: 0.7400, top5_acc: 1.0000, loss_cls: 0.5123, loss: 0.5123, grad_norm: 24.5655\n",
      "2021-07-21 19:06:56,614 - mmaction - INFO - Epoch [3][100/119]\tlr: 3.241e-05, eta: 1:27:47, time: 1.058, data_time: 0.043, memory: 21985, top1_acc: 0.7688, top5_acc: 1.0000, loss_cls: 0.4602, loss: 0.4602, grad_norm: 22.9446\n",
      "2021-07-21 19:09:00,750 - mmaction - INFO - Epoch [4][100/119]\tlr: 3.944e-05, eta: 1:24:40, time: 1.054, data_time: 0.038, memory: 21985, top1_acc: 0.7638, top5_acc: 1.0000, loss_cls: 0.4817, loss: 0.4817, grad_norm: 21.7096\n",
      "2021-07-21 19:11:05,225 - mmaction - INFO - Epoch [5][100/119]\tlr: 4.647e-05, eta: 1:22:09, time: 1.057, data_time: 0.042, memory: 21985, top1_acc: 0.7750, top5_acc: 1.0000, loss_cls: 0.4437, loss: 0.4437, grad_norm: 19.9014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 118/118, 18.6 task/s, elapsed: 6s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-21 19:11:30,240 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-07-21 19:11:30,242 - mmaction - INFO - \n",
      "top1_acc\t0.8729\n",
      "top5_acc\t1.0000\n",
      "2021-07-21 19:11:30,242 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-07-21 19:11:30,243 - mmaction - INFO - \n",
      "mean_acc\t0.8617\n",
      "2021-07-21 19:11:30,570 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_5.pth.\n",
      "2021-07-21 19:11:30,571 - mmaction - INFO - Best top1_acc is 0.8729 at 5 epoch.\n",
      "2021-07-21 19:11:30,572 - mmaction - INFO - Epoch(val) [5][15]\ttop1_acc: 0.8729, top5_acc: 1.0000, mean_class_accuracy: 0.8617\n",
      "2021-07-21 19:13:15,414 - mmaction - INFO - Epoch [6][100/119]\tlr: 5.351e-05, eta: 1:19:47, time: 1.048, data_time: 0.035, memory: 21985, top1_acc: 0.8025, top5_acc: 1.0000, loss_cls: 0.4066, loss: 0.4066, grad_norm: 21.0505\n",
      "2021-07-21 19:15:19,363 - mmaction - INFO - Epoch [7][100/119]\tlr: 6.054e-05, eta: 1:17:39, time: 1.053, data_time: 0.039, memory: 21985, top1_acc: 0.8000, top5_acc: 1.0000, loss_cls: 0.4327, loss: 0.4327, grad_norm: 19.8975\n",
      "2021-07-21 19:17:23,023 - mmaction - INFO - Epoch [8][100/119]\tlr: 6.757e-05, eta: 1:15:35, time: 1.050, data_time: 0.037, memory: 21985, top1_acc: 0.8213, top5_acc: 1.0000, loss_cls: 0.3948, loss: 0.3948, grad_norm: 19.2134\n",
      "2021-07-21 19:19:27,162 - mmaction - INFO - Epoch [9][100/119]\tlr: 7.460e-05, eta: 1:13:38, time: 1.055, data_time: 0.041, memory: 21985, top1_acc: 0.8113, top5_acc: 1.0000, loss_cls: 0.3915, loss: 0.3915, grad_norm: 20.0616\n",
      "2021-07-21 19:21:31,140 - mmaction - INFO - Epoch [10][100/119]\tlr: 8.163e-05, eta: 1:11:41, time: 1.051, data_time: 0.038, memory: 21985, top1_acc: 0.8100, top5_acc: 1.0000, loss_cls: 0.3725, loss: 0.3725, grad_norm: 18.9876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 118/118, 16.1 task/s, elapsed: 7s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-21 19:21:57,274 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-07-21 19:21:57,300 - mmaction - INFO - \n",
      "top1_acc\t0.8814\n",
      "top5_acc\t1.0000\n",
      "2021-07-21 19:21:57,304 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-07-21 19:21:57,316 - mmaction - INFO - \n",
      "mean_acc\t0.8719\n",
      "2021-07-21 19:21:58,213 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_10.pth.\n",
      "2021-07-21 19:21:58,226 - mmaction - INFO - Best top1_acc is 0.8814 at 10 epoch.\n",
      "2021-07-21 19:21:58,251 - mmaction - INFO - Epoch(val) [10][15]\ttop1_acc: 0.8814, top5_acc: 1.0000, mean_class_accuracy: 0.8719\n",
      "2021-07-21 19:23:43,644 - mmaction - INFO - Epoch [11][100/119]\tlr: 8.866e-05, eta: 1:09:48, time: 1.053, data_time: 0.041, memory: 21985, top1_acc: 0.8337, top5_acc: 1.0000, loss_cls: 0.3496, loss: 0.3496, grad_norm: 18.7204\n",
      "2021-07-21 19:25:47,645 - mmaction - INFO - Epoch [12][100/119]\tlr: 9.569e-05, eta: 1:07:55, time: 1.053, data_time: 0.039, memory: 21985, top1_acc: 0.8150, top5_acc: 1.0000, loss_cls: 0.3653, loss: 0.3653, grad_norm: 17.8160\n",
      "2021-07-21 19:27:51,448 - mmaction - INFO - Epoch [13][100/119]\tlr: 1.027e-04, eta: 1:06:04, time: 1.051, data_time: 0.038, memory: 21985, top1_acc: 0.8287, top5_acc: 1.0000, loss_cls: 0.3581, loss: 0.3581, grad_norm: 19.7685\n",
      "2021-07-21 19:29:55,357 - mmaction - INFO - Epoch [14][100/119]\tlr: 1.098e-04, eta: 1:04:13, time: 1.052, data_time: 0.039, memory: 21985, top1_acc: 0.8450, top5_acc: 1.0000, loss_cls: 0.3395, loss: 0.3395, grad_norm: 18.6968\n",
      "2021-07-21 19:31:59,084 - mmaction - INFO - Epoch [15][100/119]\tlr: 1.168e-04, eta: 1:02:23, time: 1.051, data_time: 0.038, memory: 21985, top1_acc: 0.8363, top5_acc: 1.0000, loss_cls: 0.3341, loss: 0.3341, grad_norm: 18.0437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 118/118, 19.2 task/s, elapsed: 6s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-21 19:32:23,955 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-07-21 19:32:23,956 - mmaction - INFO - \n",
      "top1_acc\t0.8898\n",
      "top5_acc\t1.0000\n",
      "2021-07-21 19:32:23,957 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-07-21 19:32:23,958 - mmaction - INFO - \n",
      "mean_acc\t0.8910\n",
      "2021-07-21 19:32:24,315 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_15.pth.\n",
      "2021-07-21 19:32:24,316 - mmaction - INFO - Best top1_acc is 0.8898 at 15 epoch.\n",
      "2021-07-21 19:32:24,316 - mmaction - INFO - Epoch(val) [15][15]\ttop1_acc: 0.8898, top5_acc: 1.0000, mean_class_accuracy: 0.8910\n",
      "2021-07-21 19:34:09,228 - mmaction - INFO - Epoch [16][100/119]\tlr: 1.238e-04, eta: 1:00:33, time: 1.049, data_time: 0.036, memory: 21985, top1_acc: 0.8225, top5_acc: 1.0000, loss_cls: 0.3533, loss: 0.3533, grad_norm: 17.0690\n",
      "2021-07-21 19:36:13,326 - mmaction - INFO - Epoch [17][100/119]\tlr: 1.250e-04, eta: 0:58:45, time: 1.054, data_time: 0.039, memory: 21985, top1_acc: 0.8575, top5_acc: 1.0000, loss_cls: 0.3217, loss: 0.3217, grad_norm: 17.3102\n",
      "2021-07-21 19:38:16,986 - mmaction - INFO - Epoch [18][100/119]\tlr: 1.250e-04, eta: 0:56:56, time: 1.050, data_time: 0.037, memory: 21985, top1_acc: 0.8475, top5_acc: 1.0000, loss_cls: 0.3328, loss: 0.3328, grad_norm: 17.6770\n",
      "2021-07-21 19:40:20,859 - mmaction - INFO - Epoch [19][100/119]\tlr: 1.250e-04, eta: 0:55:08, time: 1.051, data_time: 0.039, memory: 21985, top1_acc: 0.8525, top5_acc: 1.0000, loss_cls: 0.3026, loss: 0.3026, grad_norm: 15.1387\n",
      "2021-07-21 19:42:24,631 - mmaction - INFO - Epoch [20][100/119]\tlr: 1.250e-04, eta: 0:53:20, time: 1.050, data_time: 0.037, memory: 21985, top1_acc: 0.8662, top5_acc: 1.0000, loss_cls: 0.2898, loss: 0.2898, grad_norm: 17.8795\n",
      "2021-07-21 19:42:43,206 - mmaction - INFO - Saving checkpoint at 20 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 118/118, 17.9 task/s, elapsed: 7s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-21 19:42:50,192 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-07-21 19:42:50,194 - mmaction - INFO - \n",
      "top1_acc\t0.8814\n",
      "top5_acc\t1.0000\n",
      "2021-07-21 19:42:50,194 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-07-21 19:42:50,196 - mmaction - INFO - \n",
      "mean_acc\t0.8690\n",
      "2021-07-21 19:42:50,197 - mmaction - INFO - Epoch(val) [20][15]\ttop1_acc: 0.8814, top5_acc: 1.0000, mean_class_accuracy: 0.8690\n",
      "2021-07-21 19:44:35,200 - mmaction - INFO - Epoch [21][100/119]\tlr: 1.250e-04, eta: 0:51:33, time: 1.050, data_time: 0.037, memory: 21985, top1_acc: 0.8625, top5_acc: 1.0000, loss_cls: 0.2920, loss: 0.2920, grad_norm: 16.2119\n",
      "2021-07-21 19:46:39,343 - mmaction - INFO - Epoch [22][100/119]\tlr: 1.250e-04, eta: 0:49:46, time: 1.054, data_time: 0.040, memory: 21985, top1_acc: 0.8625, top5_acc: 1.0000, loss_cls: 0.2806, loss: 0.2806, grad_norm: 15.7211\n",
      "2021-07-21 19:48:42,914 - mmaction - INFO - Epoch [23][100/119]\tlr: 1.250e-04, eta: 0:47:58, time: 1.049, data_time: 0.038, memory: 21985, top1_acc: 0.8675, top5_acc: 1.0000, loss_cls: 0.2730, loss: 0.2730, grad_norm: 14.5350\n",
      "2021-07-21 19:50:46,582 - mmaction - INFO - Epoch [24][100/119]\tlr: 1.250e-04, eta: 0:46:11, time: 1.050, data_time: 0.038, memory: 21985, top1_acc: 0.8400, top5_acc: 1.0000, loss_cls: 0.3022, loss: 0.3022, grad_norm: 18.3650\n",
      "2021-07-21 19:52:49,990 - mmaction - INFO - Epoch [25][100/119]\tlr: 1.250e-04, eta: 0:44:24, time: 1.047, data_time: 0.035, memory: 21985, top1_acc: 0.8675, top5_acc: 1.0000, loss_cls: 0.2573, loss: 0.2573, grad_norm: 15.9068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 118/118, 17.7 task/s, elapsed: 7s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-21 19:53:15,530 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-07-21 19:53:15,533 - mmaction - INFO - \n",
      "top1_acc\t0.8390\n",
      "top5_acc\t1.0000\n",
      "2021-07-21 19:53:15,534 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-07-21 19:53:15,536 - mmaction - INFO - \n",
      "mean_acc\t0.8209\n",
      "2021-07-21 19:53:15,537 - mmaction - INFO - Epoch(val) [25][15]\ttop1_acc: 0.8390, top5_acc: 1.0000, mean_class_accuracy: 0.8209\n",
      "2021-07-21 19:55:00,636 - mmaction - INFO - Epoch [26][100/119]\tlr: 1.250e-04, eta: 0:42:37, time: 1.051, data_time: 0.038, memory: 21985, top1_acc: 0.8888, top5_acc: 1.0000, loss_cls: 0.2464, loss: 0.2464, grad_norm: 15.2354\n",
      "2021-07-21 19:57:04,280 - mmaction - INFO - Epoch [27][100/119]\tlr: 1.250e-04, eta: 0:40:51, time: 1.049, data_time: 0.037, memory: 21985, top1_acc: 0.8625, top5_acc: 1.0000, loss_cls: 0.2844, loss: 0.2844, grad_norm: 18.1862\n",
      "2021-07-21 19:59:08,035 - mmaction - INFO - Epoch [28][100/119]\tlr: 1.250e-04, eta: 0:39:04, time: 1.050, data_time: 0.038, memory: 21985, top1_acc: 0.8975, top5_acc: 1.0000, loss_cls: 0.2165, loss: 0.2165, grad_norm: 14.4885\n",
      "2021-07-21 20:01:11,045 - mmaction - INFO - Epoch [29][100/119]\tlr: 1.250e-04, eta: 0:37:17, time: 1.044, data_time: 0.034, memory: 21985, top1_acc: 0.8662, top5_acc: 1.0000, loss_cls: 0.2587, loss: 0.2587, grad_norm: 17.7602\n",
      "2021-07-21 20:03:14,928 - mmaction - INFO - Epoch [30][100/119]\tlr: 1.250e-04, eta: 0:35:31, time: 1.052, data_time: 0.039, memory: 21985, top1_acc: 0.8738, top5_acc: 1.0000, loss_cls: 0.2623, loss: 0.2623, grad_norm: 14.8761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 118/118, 17.8 task/s, elapsed: 7s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-21 20:03:40,405 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-07-21 20:03:40,406 - mmaction - INFO - \n",
      "top1_acc\t0.8898\n",
      "top5_acc\t1.0000\n",
      "2021-07-21 20:03:40,407 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-07-21 20:03:40,407 - mmaction - INFO - \n",
      "mean_acc\t0.8762\n",
      "2021-07-21 20:03:40,408 - mmaction - INFO - Epoch(val) [30][15]\ttop1_acc: 0.8898, top5_acc: 1.0000, mean_class_accuracy: 0.8762\n",
      "2021-07-21 20:05:25,332 - mmaction - INFO - Epoch [31][100/119]\tlr: 1.250e-04, eta: 0:33:45, time: 1.049, data_time: 0.036, memory: 21985, top1_acc: 0.8675, top5_acc: 1.0000, loss_cls: 0.2397, loss: 0.2397, grad_norm: 11.6113\n",
      "2021-07-21 20:07:28,761 - mmaction - INFO - Epoch [32][100/119]\tlr: 1.250e-04, eta: 0:31:59, time: 1.048, data_time: 0.034, memory: 21985, top1_acc: 0.8738, top5_acc: 1.0000, loss_cls: 0.2408, loss: 0.2408, grad_norm: 14.7725\n",
      "2021-07-21 20:09:32,256 - mmaction - INFO - Epoch [33][100/119]\tlr: 1.250e-05, eta: 0:30:12, time: 1.047, data_time: 0.035, memory: 21985, top1_acc: 0.8812, top5_acc: 1.0000, loss_cls: 0.2221, loss: 0.2221, grad_norm: 13.7444\n",
      "2021-07-21 20:11:35,964 - mmaction - INFO - Epoch [34][100/119]\tlr: 1.250e-05, eta: 0:28:26, time: 1.049, data_time: 0.036, memory: 21985, top1_acc: 0.9012, top5_acc: 1.0000, loss_cls: 0.1947, loss: 0.1947, grad_norm: 13.1558\n",
      "2021-07-21 20:13:39,374 - mmaction - INFO - Epoch [35][100/119]\tlr: 1.250e-05, eta: 0:26:40, time: 1.048, data_time: 0.035, memory: 21985, top1_acc: 0.9000, top5_acc: 1.0000, loss_cls: 0.2056, loss: 0.2056, grad_norm: 13.9399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 118/118, 18.8 task/s, elapsed: 6s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-21 20:14:04,437 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-07-21 20:14:04,438 - mmaction - INFO - \n",
      "top1_acc\t0.8814\n",
      "top5_acc\t1.0000\n",
      "2021-07-21 20:14:04,439 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-07-21 20:14:04,440 - mmaction - INFO - \n",
      "mean_acc\t0.8749\n",
      "2021-07-21 20:14:04,441 - mmaction - INFO - Epoch(val) [35][15]\ttop1_acc: 0.8814, top5_acc: 1.0000, mean_class_accuracy: 0.8749\n",
      "2021-07-21 20:15:49,050 - mmaction - INFO - Epoch [36][100/119]\tlr: 1.250e-05, eta: 0:24:54, time: 1.046, data_time: 0.033, memory: 21985, top1_acc: 0.8912, top5_acc: 1.0000, loss_cls: 0.2052, loss: 0.2052, grad_norm: 14.0746\n",
      "2021-07-21 20:17:52,772 - mmaction - INFO - Epoch [37][100/119]\tlr: 1.250e-05, eta: 0:23:09, time: 1.051, data_time: 0.039, memory: 21985, top1_acc: 0.8762, top5_acc: 1.0000, loss_cls: 0.2203, loss: 0.2203, grad_norm: 13.1916\n",
      "2021-07-21 20:19:56,322 - mmaction - INFO - Epoch [38][100/119]\tlr: 1.250e-05, eta: 0:21:23, time: 1.049, data_time: 0.037, memory: 21985, top1_acc: 0.8838, top5_acc: 1.0000, loss_cls: 0.2083, loss: 0.2083, grad_norm: 13.5195\n",
      "2021-07-21 20:21:59,749 - mmaction - INFO - Epoch [39][100/119]\tlr: 1.250e-05, eta: 0:19:37, time: 1.048, data_time: 0.037, memory: 21985, top1_acc: 0.8975, top5_acc: 1.0000, loss_cls: 0.1917, loss: 0.1917, grad_norm: 12.8005\n",
      "2021-07-21 20:24:03,386 - mmaction - INFO - Epoch [40][100/119]\tlr: 1.250e-05, eta: 0:17:52, time: 1.049, data_time: 0.038, memory: 21985, top1_acc: 0.8762, top5_acc: 1.0000, loss_cls: 0.2278, loss: 0.2278, grad_norm: 15.0458\n",
      "2021-07-21 20:24:21,987 - mmaction - INFO - Saving checkpoint at 40 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 118/118, 14.5 task/s, elapsed: 8s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-21 20:24:30,622 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-07-21 20:24:30,699 - mmaction - INFO - \n",
      "top1_acc\t0.8814\n",
      "top5_acc\t1.0000\n",
      "2021-07-21 20:24:30,722 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-07-21 20:24:30,780 - mmaction - INFO - \n",
      "mean_acc\t0.8749\n",
      "2021-07-21 20:24:30,818 - mmaction - INFO - Epoch(val) [40][15]\ttop1_acc: 0.8814, top5_acc: 1.0000, mean_class_accuracy: 0.8749\n",
      "2021-07-21 20:26:16,018 - mmaction - INFO - Epoch [41][100/119]\tlr: 1.250e-05, eta: 0:16:06, time: 1.050, data_time: 0.038, memory: 21985, top1_acc: 0.9050, top5_acc: 1.0000, loss_cls: 0.1927, loss: 0.1927, grad_norm: 13.8017\n",
      "2021-07-21 20:28:19,638 - mmaction - INFO - Epoch [42][100/119]\tlr: 1.250e-05, eta: 0:14:20, time: 1.049, data_time: 0.035, memory: 21985, top1_acc: 0.8850, top5_acc: 1.0000, loss_cls: 0.2039, loss: 0.2039, grad_norm: 13.4039\n",
      "2021-07-21 20:30:23,117 - mmaction - INFO - Epoch [43][100/119]\tlr: 1.250e-05, eta: 0:12:35, time: 1.048, data_time: 0.035, memory: 21985, top1_acc: 0.8962, top5_acc: 1.0000, loss_cls: 0.1919, loss: 0.1919, grad_norm: 12.2780\n",
      "2021-07-21 20:32:26,998 - mmaction - INFO - Epoch [44][100/119]\tlr: 1.250e-05, eta: 0:10:49, time: 1.051, data_time: 0.038, memory: 21985, top1_acc: 0.9075, top5_acc: 1.0000, loss_cls: 0.1921, loss: 0.1921, grad_norm: 13.7058\n",
      "2021-07-21 20:34:30,740 - mmaction - INFO - Epoch [45][100/119]\tlr: 1.250e-05, eta: 0:09:04, time: 1.051, data_time: 0.039, memory: 21985, top1_acc: 0.8875, top5_acc: 1.0000, loss_cls: 0.2058, loss: 0.2058, grad_norm: 12.2147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 118/118, 18.1 task/s, elapsed: 7s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-21 20:34:56,102 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-07-21 20:34:56,103 - mmaction - INFO - \n",
      "top1_acc\t0.8898\n",
      "top5_acc\t1.0000\n",
      "2021-07-21 20:34:56,104 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-07-21 20:34:56,106 - mmaction - INFO - \n",
      "mean_acc\t0.8821\n",
      "2021-07-21 20:34:56,106 - mmaction - INFO - Epoch(val) [45][15]\ttop1_acc: 0.8898, top5_acc: 1.0000, mean_class_accuracy: 0.8821\n",
      "2021-07-21 20:36:41,192 - mmaction - INFO - Epoch [46][100/119]\tlr: 1.250e-05, eta: 0:07:18, time: 1.051, data_time: 0.037, memory: 21985, top1_acc: 0.8912, top5_acc: 1.0000, loss_cls: 0.2117, loss: 0.2117, grad_norm: 12.4332\n",
      "2021-07-21 20:38:44,905 - mmaction - INFO - Epoch [47][100/119]\tlr: 1.250e-05, eta: 0:05:33, time: 1.050, data_time: 0.037, memory: 21985, top1_acc: 0.9038, top5_acc: 1.0000, loss_cls: 0.1846, loss: 0.1846, grad_norm: 12.7917\n",
      "2021-07-21 20:40:48,716 - mmaction - INFO - Epoch [48][100/119]\tlr: 1.250e-05, eta: 0:03:47, time: 1.050, data_time: 0.036, memory: 21985, top1_acc: 0.9075, top5_acc: 1.0000, loss_cls: 0.1847, loss: 0.1847, grad_norm: 10.5106\n",
      "2021-07-21 20:42:52,289 - mmaction - INFO - Epoch [49][100/119]\tlr: 1.250e-06, eta: 0:02:02, time: 1.050, data_time: 0.036, memory: 21985, top1_acc: 0.9025, top5_acc: 1.0000, loss_cls: 0.1752, loss: 0.1752, grad_norm: 10.9181\n",
      "2021-07-21 20:44:56,107 - mmaction - INFO - Epoch [50][100/119]\tlr: 1.250e-06, eta: 0:00:16, time: 1.052, data_time: 0.037, memory: 21985, top1_acc: 0.8962, top5_acc: 1.0000, loss_cls: 0.2123, loss: 0.2123, grad_norm: 15.1517\n",
      "2021-07-21 20:45:14,751 - mmaction - INFO - Saving checkpoint at 50 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 118/118, 18.6 task/s, elapsed: 6s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-21 20:45:21,508 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-07-21 20:45:21,510 - mmaction - INFO - \n",
      "top1_acc\t0.8814\n",
      "top5_acc\t1.0000\n",
      "2021-07-21 20:45:21,511 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-07-21 20:45:21,513 - mmaction - INFO - \n",
      "mean_acc\t0.8749\n",
      "2021-07-21 20:45:21,513 - mmaction - INFO - Epoch(val) [50][15]\ttop1_acc: 0.8814, top5_acc: 1.0000, mean_class_accuracy: 0.8749\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "from mmaction.datasets import build_dataset\n",
    "from mmaction.models import build_model\n",
    "from mmaction.apis import train_model\n",
    "\n",
    "import mmcv\n",
    "\n",
    "# Build the dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# Build the recognizer\n",
    "model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.train_cfg, test_cfg=cfg.test_cfg)\n",
    "\n",
    "# Create work_dir\n",
    "mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
    "train_model(model, datasets, cfg, distributed=False, validate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d6e6a38-2cc6-4eb7-b030-4ff874c7b000",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f\"{cfg.work_dir}/model50e_ig65m\", 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979e77b2-4aef-4fe7-a842-7a26fe9ec792",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os.path as osp\n",
    "\n",
    "# from mmaction.datasets import build_dataset\n",
    "# from mmaction.models import build_model\n",
    "# from mmaction.apis import train_model\n",
    "# import pickle\n",
    "# import mmcv\n",
    "# # Build the dataset\n",
    "# datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# # Build the recognizer\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "\n",
    "# model = pickle.load(open(f\"{cfg.work_dir}/model50e\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "176ca9dc-6dea-4e53-b418-603248e4d725",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eyY3hCMwyTct",
    "outputId": "200e37c7-0da4-421f-98da-41418c3110ca",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 119/119, 0.2 task/s, elapsed: 591s, ETA:     0s\n",
      "Evaluating top_k_accuracy ...\n",
      "\n",
      "top1_acc\t0.9580\n",
      "top5_acc\t1.0000\n",
      "\n",
      "Evaluating mean_class_accuracy ...\n",
      "\n",
      "mean_acc\t0.9608\n",
      "top1_acc: 0.9580\n",
      "top5_acc: 1.0000\n",
      "mean_class_accuracy: 0.9608\n"
     ]
    }
   ],
   "source": [
    "from mmaction.apis import single_gpu_test\n",
    "from mmaction.datasets import build_dataloader\n",
    "from mmcv.parallel import MMDataParallel\n",
    "# from mmaction.models import build_model\n",
    "# from mmaction.datasets import build_dataset\n",
    "\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# Build a test dataloader\n",
    "dataset = build_dataset(cfg.data.test, dict(test_mode=True))\n",
    "data_loader = build_dataloader(\n",
    "        dataset,\n",
    "        videos_per_gpu=1,\n",
    "        workers_per_gpu=1,\n",
    "        dist=False,\n",
    "        shuffle=False)\n",
    "model = MMDataParallel(model, device_ids=[0])\n",
    "outputs = single_gpu_test(model, data_loader)\n",
    "\n",
    "eval_config = cfg.evaluation\n",
    "eval_config.pop('interval')\n",
    "eval_res = dataset.evaluate(outputs, **eval_config)\n",
    "for name, val in eval_res.items():\n",
    "    print(f'{name}: {val:.04f}')\n",
    "    \n",
    "output_config = cfg.output_config\n",
    "dataset.dump_results(outputs, **output_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "80ce555a-19a6-46cc-ba4b-d51ba1c6bb02",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from mmaction.core import confusion_matrix \n",
    "\n",
    "gt_labels = [ann['label'] for ann in dataset.load_annotations()]\n",
    "pred = np.argmax(outputs, axis=1)\n",
    "cf_mat = confusion_matrix(pred, gt_labels).astype(float)\n",
    "\n",
    "sns.heatmap(cf_mat, annot=True, xticklabels = ['male', 'female'], yticklabels = ['male', 'female'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad81aee-59ff-4d1c-a357-d27f94aaa538",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

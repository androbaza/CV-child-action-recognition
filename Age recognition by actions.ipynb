{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44ef7d31-79f4-4b8d-84b7-03a87ce489d9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bf8PpPXtVvmg",
    "outputId": "2c685a33-474b-4e71-8f98-c2533c66095e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2019 NVIDIA Corporation\n",
      "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
      "Cuda compilation tools, release 10.1, V10.1.243\n",
      "gcc (Ubuntu 8.4.0-3ubuntu2) 8.4.0\n",
      "Copyright (C) 2018 Free Software Foundation, Inc.\n",
      "This is free software; see the source for copying conditions.  There is NO\n",
      "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check nvcc version\n",
    "!nvcc -V\n",
    "# Check GCC version\n",
    "!gcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bb1a682-5796-45c9-be5f-570b0f780a26",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "No_zZAFpWC-a",
    "outputId": "1d425eea-d44e-434a-991c-01eb15abaab2",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0 True\n",
      "0.16.0\n",
      "10.2\n",
      "GCC 7.3\n"
     ]
    }
   ],
   "source": [
    "# Check Pytorch installation\n",
    "import torch, torchvision\n",
    "print(torch.__version__, torch.cuda.is_available())\n",
    "\n",
    "# Check MMAction2 installation\n",
    "import mmaction\n",
    "print(mmaction.__version__)\n",
    "\n",
    "# Check MMCV installation\n",
    "from mmcv.ops import get_compiling_cuda_version, get_compiler_version\n",
    "print(get_compiling_cuda_version())\n",
    "print(get_compiler_version())\n",
    "\n",
    "import sys\n",
    "# sys.path.append('/home/actrec/.local/lib/python3.6/site-packages/decord-0.5.3-py3.6-linux-x86_64.egg')\n",
    "# import decord\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "916b8b4d-5611-4ec9-9411-451c41a2f4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/robt427nv/childact\n"
     ]
    }
   ],
   "source": [
    "cd /home/robt427nv/childact/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981efb1d-74e1-464e-aaef-6ce83a034aea",
   "metadata": {},
   "source": [
    "# CSN age BOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec97f7fb-9aae-4f6b-9238-706fd00a86ec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "64CW6d_AaT-Q",
    "outputId": "3b284fd8-4ee7-4a34-90d7-5023cd123a04",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-07-21 17:39:20--  https://download.openmmlab.com/mmaction/recognition/csn/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth\n",
      "Resolving download.openmmlab.com (download.openmmlab.com)... 47.75.20.25\n",
      "Connecting to download.openmmlab.com (download.openmmlab.com)|47.75.20.25|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 119580180 (114M) [application/octet-stream]\n",
      "Saving to: ‘checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth’\n",
      "\n",
      "checkpoints/ircsn_i 100%[===================>] 114,04M  6,78MB/s    in 17s     \n",
      "\n",
      "2021-07-21 17:39:39 (6,87 MB/s) - ‘checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth’ saved [119580180/119580180]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir checkpoints\n",
    "!wget -c https://download.openmmlab.com/mmaction/recognition/csn/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth \\\n",
    "      -O checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "775b5624-5baf-41d7-b8e6-e9a274bb317b",
   "metadata": {
    "id": "LjCcmCKOjktc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mmcv import Config\n",
    "cfg = Config.fromfile('mmaction2/configs/recognition/csn/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1cdd7df-c256-4d1a-b47b-1a027c33d315",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "tlhu9byjjt-K",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "13d1bb01-f351-48c0-9efb-0bdf2c125d29",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "model = dict(\n",
      "    type='Recognizer3D',\n",
      "    backbone=dict(\n",
      "        type='ResNet3dCSN',\n",
      "        pretrained2d=False,\n",
      "        pretrained=\n",
      "        'https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth',\n",
      "        depth=152,\n",
      "        with_pool2=False,\n",
      "        bottleneck_mode='ir',\n",
      "        norm_eval=True,\n",
      "        zero_init_residual=False,\n",
      "        bn_frozen=True),\n",
      "    cls_head=dict(\n",
      "        type='I3DHead',\n",
      "        num_classes=6,\n",
      "        in_channels=2048,\n",
      "        spatial_type='avg',\n",
      "        dropout_ratio=0.5,\n",
      "        init_std=0.01),\n",
      "    train_cfg=None,\n",
      "    test_cfg=dict(average_clips='prob'))\n",
      "checkpoint_config = dict(interval=20)\n",
      "log_config = dict(interval=100, hooks=[dict(type='TextLoggerHook')])\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = 'checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth'\n",
      "resume_from = None\n",
      "workflow = [('train', 1)]\n",
      "dataset_type = 'RawframeDataset'\n",
      "data_root = 'age-gender-3split-rgb-frames/'\n",
      "data_root_val = 'age-gender-3split-rgb-frames/val/'\n",
      "ann_file_train = 'age-gender-3split-rgb-frames/childact_train_rgb320_age_box.txt'\n",
      "ann_file_val = 'age-gender-3split-rgb-frames/childact_val_rgb320_age_box.txt'\n",
      "ann_file_test = 'age-gender-3split-rgb-frames/childact_test_rgb320_age_box.txt'\n",
      "img_norm_cfg = dict(\n",
      "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_bgr=False)\n",
      "train_pipeline = [\n",
      "    dict(type='SampleFrames', clip_len=32, frame_interval=2, num_clips=1),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='RandomResizedCrop'),\n",
      "    dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "    dict(type='Flip', flip_ratio=0.5),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "]\n",
      "val_pipeline = [\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=32,\n",
      "        frame_interval=2,\n",
      "        num_clips=1,\n",
      "        test_mode=True),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='CenterCrop', crop_size=224),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=32,\n",
      "        frame_interval=2,\n",
      "        num_clips=10,\n",
      "        test_mode=True),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='ThreeCrop', crop_size=256),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "data = dict(\n",
      "    videos_per_gpu=6,\n",
      "    workers_per_gpu=4,\n",
      "    train=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_train_rgb320_age_box.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/train/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=1),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='RandomResizedCrop'),\n",
      "            dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "            dict(type='Flip', flip_ratio=0.5),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'),\n",
      "    val=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file='age-gender-3split-rgb-frames/childact_val_rgb320_age_box.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/val/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=1,\n",
      "                test_mode=True),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='CenterCrop', crop_size=224),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'),\n",
      "    test=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_test_rgb320_age_box.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/test/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=10,\n",
      "                test_mode=True),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='ThreeCrop', crop_size=256),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'))\n",
      "evaluation = dict(\n",
      "    interval=5, metrics=['top_k_accuracy', 'mean_class_accuracy'])\n",
      "optimizer = dict(type='SGD', lr=0.000125, momentum=0.9, weight_decay=0.0001)\n",
      "optimizer_config = dict(grad_clip=dict(max_norm=40, norm_type=2))\n",
      "lr_config = dict(\n",
      "    policy='step',\n",
      "    step=[32, 48],\n",
      "    warmup='linear',\n",
      "    warmup_ratio=0.1,\n",
      "    warmup_by_epoch=True,\n",
      "    warmup_iters=16)\n",
      "total_epochs = 50\n",
      "work_dir = './childact-checkpoints/CSN-age-box'\n",
      "find_unused_parameters = True\n",
      "omnisource = False\n",
      "seed = 42\n",
      "gpu_ids = range(0, 1)\n",
      "output_config = dict(out='./childact-checkpoints/CSN-age-box/results.json')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mmcv.runner import set_random_seed\n",
    "\n",
    "# Modify dataset type and path\n",
    "# cfg.dataset_type = 'VideoDataset'\n",
    "cfg.data_root = 'age-gender-3split-rgb-frames/'\n",
    "cfg.data_root_val = 'age-gender-3split-rgb-frames/val/'\n",
    "cfg.ann_file_train = 'age-gender-3split-rgb-frames/childact_train_rgb320_age_box.txt'\n",
    "cfg.ann_file_val = 'age-gender-3split-rgb-frames/childact_val_rgb320_age_box.txt'\n",
    "cfg.ann_file_test = 'age-gender-3split-rgb-frames/childact_test_rgb320_age_box.txt'\n",
    "\n",
    "# cfg.data.test.type = 'VideoDataset'\n",
    "cfg.data.test.ann_file = 'age-gender-3split-rgb-frames/childact_test_rgb320_age_box.txt'\n",
    "cfg.data.test.data_prefix = 'age-gender-3split-rgb-frames/test/'\n",
    "\n",
    "# cfg.data.train.type = 'VideoDataset'\n",
    "cfg.data.train.ann_file = 'age-gender-3split-rgb-frames/childact_train_rgb320_age_box.txt'\n",
    "cfg.data.train.data_prefix = 'age-gender-3split-rgb-frames/train/'\n",
    "\n",
    "# cfg.data.val.type = 'VideoDataset'\n",
    "cfg.data.val.ann_file = 'age-gender-3split-rgb-frames/childact_val_rgb320_age_box.txt'\n",
    "cfg.data.val.data_prefix = 'age-gender-3split-rgb-frames/val/'\n",
    "\n",
    "# cfg.data.test.modality = 'Flow'\n",
    "# cfg.data.val.modality = 'Flow'\n",
    "# cfg.data.train.modality = 'Flow'\n",
    "\n",
    "# cfg.data.train.start_index = 0\n",
    "# cfg.data.test.start_index = 0\n",
    "# cfg.data.val.start_index = 0\n",
    "\n",
    "cfg.data.test.filename_tmpl = '{:03}.jpeg'\n",
    "cfg.data.train.filename_tmpl = '{:03}.jpeg'\n",
    "cfg.data.val.filename_tmpl = '{:03}.jpeg'\n",
    "# The flag is used to determine whether it is omnisource training\n",
    "cfg.setdefault('omnisource', False)\n",
    "# Modify num classes of the model in cls_head\n",
    "cfg.model.cls_head.num_classes = 6\n",
    "# We can use the pre-trained TSN model\n",
    "cfg.load_from = 'checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth'\n",
    "# cfg.resume_from = './childact-mm/latest.pth'\n",
    "\n",
    "# Set up working dir to save files and logs.\n",
    "cfg.work_dir = './childact-checkpoints/CSN-age-box'\n",
    "\n",
    "cfg.total_epochs = 50\n",
    "\n",
    "# cfg.momentum_config = dict(\n",
    "#     policy='cyclic',\n",
    "#     target_ratio=(0.85 / 0.95, 1),\n",
    "#     cyclic_times=1,\n",
    "#     step_ratio_up=0.4,\n",
    "# )\n",
    "\n",
    "# We can set the checkpoint saving interval to reduce the storage cost\n",
    "cfg.checkpoint_config.interval = 20\n",
    "# We can set the log print interval to reduce the the times of printing log\n",
    "cfg.log_config.interval = 100\n",
    "\n",
    "# Set seed thus the results are more reproducible\n",
    "cfg.seed = 42\n",
    "set_random_seed(42, deterministic=False)\n",
    "cfg.gpu_ids = range(1)\n",
    "\n",
    "cfg.data.videos_per_gpu=6\n",
    "\n",
    "# cfg.model.backbone.in_channels = 2\n",
    "\n",
    "cfg.output_config = dict(out=f'{cfg.work_dir}/results.json')\n",
    "# We can initialize the logger for training and have a look\n",
    "# at the final config used for training\n",
    "# del cfg.optimizer['momentum']\n",
    "print(f'Config:\\n{cfg.pretty_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b2dfc99-ac49-4b23-be8d-379dc9fb4d2b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dDBWkdDRk6oz",
    "outputId": "85a52ef3-7b5c-4c52-8fef-00322a8c65e6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-24 15:07:39,747 - mmaction - INFO - load model from: https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth\n",
      "2021-08-24 15:07:39,748 - mmaction - INFO - Use load_from_http loader\n",
      "2021-08-24 15:07:41,789 - mmaction - INFO - load checkpoint from checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth\n",
      "2021-08-24 15:07:41,790 - mmaction - INFO - Use load_from_local loader\n",
      "2021-08-24 15:07:41,960 - mmaction - WARNING - The model and loaded state dict do not match exactly\n",
      "\n",
      "size mismatch for cls_head.fc_cls.weight: copying a param with shape torch.Size([400, 2048]) from checkpoint, the shape in current model is torch.Size([6, 2048]).\n",
      "size mismatch for cls_head.fc_cls.bias: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([6]).\n",
      "2021-08-24 15:07:41,966 - mmaction - INFO - Start running, host: robt427nv@robt427NV, work_dir: /home/robt427nv/childact/childact-checkpoints/CSN-age-box\n",
      "2021-08-24 15:07:41,967 - mmaction - INFO - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(ABOVE_NORMAL) OptimizerHook                      \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "2021-08-24 15:07:41,968 - mmaction - INFO - workflow: [('train', 1)], max: 50 epochs\n",
      "/home/robt427nv/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/mmcv/runner/hooks/evaluation.py:190: UserWarning: runner.meta is None. Creating an empty one.\n",
      "  warnings.warn('runner.meta is None. Creating an empty one.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 11/11, 8.7 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-24 15:09:31,419 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-24 15:09:31,421 - mmaction - INFO - \n",
      "top1_acc\t0.2727\n",
      "top5_acc\t1.0000\n",
      "2021-08-24 15:09:31,422 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-24 15:09:31,424 - mmaction - INFO - \n",
      "mean_acc\t0.2500\n",
      "2021-08-24 15:09:31,748 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_5.pth.\n",
      "2021-08-24 15:09:31,749 - mmaction - INFO - Best top1_acc is 0.2727 at 5 epoch.\n",
      "2021-08-24 15:09:31,750 - mmaction - INFO - Epoch(val) [5][2]\ttop1_acc: 0.2727, top5_acc: 1.0000, mean_class_accuracy: 0.2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 11/11, 8.8 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-24 15:11:20,086 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-24 15:11:20,088 - mmaction - INFO - \n",
      "top1_acc\t0.2727\n",
      "top5_acc\t1.0000\n",
      "2021-08-24 15:11:20,089 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-24 15:11:20,090 - mmaction - INFO - \n",
      "mean_acc\t0.2500\n",
      "2021-08-24 15:11:20,091 - mmaction - INFO - Epoch(val) [10][2]\ttop1_acc: 0.2727, top5_acc: 1.0000, mean_class_accuracy: 0.2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 11/11, 8.7 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-24 15:13:09,115 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-24 15:13:09,117 - mmaction - INFO - \n",
      "top1_acc\t0.2727\n",
      "top5_acc\t1.0000\n",
      "2021-08-24 15:13:09,117 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-24 15:13:09,118 - mmaction - INFO - \n",
      "mean_acc\t0.2292\n",
      "2021-08-24 15:13:09,118 - mmaction - INFO - Epoch(val) [15][2]\ttop1_acc: 0.2727, top5_acc: 1.0000, mean_class_accuracy: 0.2292\n",
      "2021-08-24 15:14:56,722 - mmaction - INFO - Saving checkpoint at 20 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 11/11, 8.5 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-24 15:14:58,471 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-24 15:14:58,473 - mmaction - INFO - \n",
      "top1_acc\t0.4545\n",
      "top5_acc\t1.0000\n",
      "2021-08-24 15:14:58,473 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-24 15:14:58,476 - mmaction - INFO - \n",
      "mean_acc\t0.3750\n",
      "2021-08-24 15:14:58,867 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_20.pth.\n",
      "2021-08-24 15:14:58,868 - mmaction - INFO - Best top1_acc is 0.4545 at 20 epoch.\n",
      "2021-08-24 15:14:58,869 - mmaction - INFO - Epoch(val) [20][2]\ttop1_acc: 0.4545, top5_acc: 1.0000, mean_class_accuracy: 0.3750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 11/11, 8.8 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-24 15:16:48,312 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-24 15:16:48,313 - mmaction - INFO - \n",
      "top1_acc\t0.4545\n",
      "top5_acc\t1.0000\n",
      "2021-08-24 15:16:48,313 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-24 15:16:48,314 - mmaction - INFO - \n",
      "mean_acc\t0.3750\n",
      "2021-08-24 15:16:48,314 - mmaction - INFO - Epoch(val) [25][2]\ttop1_acc: 0.4545, top5_acc: 1.0000, mean_class_accuracy: 0.3750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 11/11, 8.4 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-24 15:18:37,482 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-24 15:18:37,483 - mmaction - INFO - \n",
      "top1_acc\t0.3636\n",
      "top5_acc\t1.0000\n",
      "2021-08-24 15:18:37,484 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-24 15:18:37,484 - mmaction - INFO - \n",
      "mean_acc\t0.2917\n",
      "2021-08-24 15:18:37,485 - mmaction - INFO - Epoch(val) [30][2]\ttop1_acc: 0.3636, top5_acc: 1.0000, mean_class_accuracy: 0.2917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 11/11, 8.2 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-24 15:20:26,739 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-24 15:20:26,740 - mmaction - INFO - \n",
      "top1_acc\t0.5455\n",
      "top5_acc\t1.0000\n",
      "2021-08-24 15:20:26,741 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-24 15:20:26,742 - mmaction - INFO - \n",
      "mean_acc\t0.4583\n",
      "2021-08-24 15:20:27,069 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_35.pth.\n",
      "2021-08-24 15:20:27,070 - mmaction - INFO - Best top1_acc is 0.5455 at 35 epoch.\n",
      "2021-08-24 15:20:27,070 - mmaction - INFO - Epoch(val) [35][2]\ttop1_acc: 0.5455, top5_acc: 1.0000, mean_class_accuracy: 0.4583\n",
      "2021-08-24 15:22:14,466 - mmaction - INFO - Saving checkpoint at 40 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 11/11, 8.4 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-24 15:22:16,218 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-24 15:22:16,220 - mmaction - INFO - \n",
      "top1_acc\t0.3636\n",
      "top5_acc\t1.0000\n",
      "2021-08-24 15:22:16,221 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-24 15:22:16,223 - mmaction - INFO - \n",
      "mean_acc\t0.3125\n",
      "2021-08-24 15:22:16,224 - mmaction - INFO - Epoch(val) [40][2]\ttop1_acc: 0.3636, top5_acc: 1.0000, mean_class_accuracy: 0.3125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 11/11, 8.5 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-24 15:24:05,326 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-24 15:24:05,328 - mmaction - INFO - \n",
      "top1_acc\t0.3636\n",
      "top5_acc\t1.0000\n",
      "2021-08-24 15:24:05,329 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-24 15:24:05,331 - mmaction - INFO - \n",
      "mean_acc\t0.3125\n",
      "2021-08-24 15:24:05,332 - mmaction - INFO - Epoch(val) [45][2]\ttop1_acc: 0.3636, top5_acc: 1.0000, mean_class_accuracy: 0.3125\n",
      "2021-08-24 15:25:53,022 - mmaction - INFO - Saving checkpoint at 50 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 11/11, 8.7 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-24 15:25:54,733 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-24 15:25:54,735 - mmaction - INFO - \n",
      "top1_acc\t0.3636\n",
      "top5_acc\t1.0000\n",
      "2021-08-24 15:25:54,735 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-24 15:25:54,736 - mmaction - INFO - \n",
      "mean_acc\t0.3125\n",
      "2021-08-24 15:25:54,736 - mmaction - INFO - Epoch(val) [50][2]\ttop1_acc: 0.3636, top5_acc: 1.0000, mean_class_accuracy: 0.3125\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "from mmaction.datasets import build_dataset\n",
    "from mmaction.models import build_model\n",
    "from mmaction.apis import train_model\n",
    "\n",
    "import mmcv\n",
    "\n",
    "# Build the dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# Build the recognizer\n",
    "model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.train_cfg, test_cfg=cfg.test_cfg)\n",
    "# CUDA_LAUNCH_BLOCKING=1\n",
    "# Create work_dir\n",
    "mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
    "train_model(model, datasets, cfg, distributed=False, validate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f5b739c-1b26-40ec-b6d6-74ee4d0b8a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f\"{cfg.work_dir}/model50e2\", 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530c68db-25a9-441e-be02-7914c721bb53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "711b574d-0218-4064-beab-3043fa372096",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 10:10:19,285 - mmaction - INFO - load model from: https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth\n",
      "2021-08-10 10:10:19,286 - mmaction - INFO - Use load_from_http loader\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "from mmaction.datasets import build_dataset\n",
    "from mmaction.models import build_model\n",
    "from mmaction.apis import train_model\n",
    "import pickle\n",
    "import mmcv\n",
    "# Build the dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# Build the recognizer\n",
    "model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "\n",
    "model = pickle.load(open(f\"{cfg.work_dir}/model50e\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03c849b7-ddd1-4743-97f2-f98f7186016c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eyY3hCMwyTct",
    "outputId": "200e37c7-0da4-421f-98da-41418c3110ca",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 8/8, 0.5 task/s, elapsed: 15s, ETA:     0s\n",
      "Evaluating top_k_accuracy ...\n",
      "\n",
      "top1_acc\t0.2500\n",
      "top5_acc\t0.8750\n",
      "\n",
      "Evaluating mean_class_accuracy ...\n",
      "\n",
      "mean_acc\t0.3000\n",
      "top1_acc: 0.2500\n",
      "top5_acc: 0.8750\n",
      "mean_class_accuracy: 0.3000\n"
     ]
    }
   ],
   "source": [
    "from mmaction.apis import single_gpu_test\n",
    "from mmaction.datasets import build_dataloader\n",
    "from mmcv.parallel import MMDataParallel\n",
    "# from mmaction.models import build_model\n",
    "# from mmaction.datasets import build_dataset\n",
    "\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# Build a test dataloader\n",
    "dataset = build_dataset(cfg.data.test, dict(test_mode=True))\n",
    "data_loader = build_dataloader(\n",
    "        dataset,\n",
    "        videos_per_gpu=1,\n",
    "        workers_per_gpu=1,\n",
    "        dist=False,\n",
    "        shuffle=False)\n",
    "model = MMDataParallel(model, device_ids=[0])\n",
    "outputs = single_gpu_test(model, data_loader)\n",
    "\n",
    "eval_config = cfg.evaluation\n",
    "eval_config.pop('interval')\n",
    "eval_res = dataset.evaluate(outputs, **eval_config)\n",
    "for name, val in eval_res.items():\n",
    "    print(f'{name}: {val:.04f}')\n",
    "    \n",
    "output_config = cfg.output_config\n",
    "dataset.dump_results(outputs, **output_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35c36b81-8229-4d07-80c1-521fc7dae107",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from mmaction.core import confusion_matrix \n",
    "\n",
    "gt_labels = [ann['label'] for ann in dataset.load_annotations()]\n",
    "pred = np.argmax(outputs, axis=1)\n",
    "cf_mat = confusion_matrix(pred, gt_labels).astype(float)\n",
    "cmap = sns.cubehelix_palette(50, hue=0.05, rot=0, light=0.9, dark=0, as_cmap=True)\n",
    "# /np.sum(cf_mat), fmt='.2%',\n",
    "sns.heatmap(cf_mat, cmap=cmap, annot=True, xticklabels = ['6yo', '7yo', '8yo', '9yo'], yticklabels = ['6yo', '7yo', '8yo', '9yo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6944e37-59d8-437c-a112-a147ce045619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n"
     ]
    }
   ],
   "source": [
    "mean_error = (2+4)/8 #number of all tested videos\n",
    "print(mean_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c47bde-6314-4b20-a9f5-2bd279e7868a",
   "metadata": {},
   "source": [
    "# CSN age clap "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "261aa6f1-84a1-457d-95ee-d807eee5ddc4",
   "metadata": {
    "id": "LjCcmCKOjktc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mmcv import Config\n",
    "cfg = Config.fromfile('mmaction2/configs/recognition/csn/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27880464-a44d-4f37-9223-27f0dc2fc8bf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "tlhu9byjjt-K",
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "outputId": "13d1bb01-f351-48c0-9efb-0bdf2c125d29",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "model = dict(\n",
      "    type='Recognizer3D',\n",
      "    backbone=dict(\n",
      "        type='ResNet3dCSN',\n",
      "        pretrained2d=False,\n",
      "        pretrained=\n",
      "        'https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth',\n",
      "        depth=152,\n",
      "        with_pool2=False,\n",
      "        bottleneck_mode='ir',\n",
      "        norm_eval=True,\n",
      "        zero_init_residual=False,\n",
      "        bn_frozen=True),\n",
      "    cls_head=dict(\n",
      "        type='I3DHead',\n",
      "        num_classes=6,\n",
      "        in_channels=2048,\n",
      "        spatial_type='avg',\n",
      "        dropout_ratio=0.5,\n",
      "        init_std=0.01),\n",
      "    train_cfg=None,\n",
      "    test_cfg=dict(average_clips='prob'))\n",
      "checkpoint_config = dict(interval=20)\n",
      "log_config = dict(interval=100, hooks=[dict(type='TextLoggerHook')])\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = 'checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth'\n",
      "resume_from = None\n",
      "workflow = [('train', 1)]\n",
      "dataset_type = 'RawframeDataset'\n",
      "data_root = 'age-gender-3split-rgb-frames/'\n",
      "data_root_val = 'age-gender-3split-rgb-frames/val/'\n",
      "ann_file_train = 'age-gender-3split-rgb-frames/childact_train_rgb320_age_clap.txt'\n",
      "ann_file_val = 'age-gender-3split-rgb-frames/childact_val_rgb320_age_clap.txt'\n",
      "ann_file_test = 'age-gender-3split-rgb-frames/childact_test_rgb320_age_clap.txt'\n",
      "img_norm_cfg = dict(\n",
      "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_bgr=False)\n",
      "train_pipeline = [\n",
      "    dict(type='SampleFrames', clip_len=32, frame_interval=2, num_clips=1),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='RandomResizedCrop'),\n",
      "    dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "    dict(type='Flip', flip_ratio=0.5),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "]\n",
      "val_pipeline = [\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=32,\n",
      "        frame_interval=2,\n",
      "        num_clips=1,\n",
      "        test_mode=True),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='CenterCrop', crop_size=224),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=32,\n",
      "        frame_interval=2,\n",
      "        num_clips=10,\n",
      "        test_mode=True),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='ThreeCrop', crop_size=256),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "data = dict(\n",
      "    videos_per_gpu=4,\n",
      "    workers_per_gpu=4,\n",
      "    train=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_train_rgb320_age_clap.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/train/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=1),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='RandomResizedCrop'),\n",
      "            dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "            dict(type='Flip', flip_ratio=0.5),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'),\n",
      "    val=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_val_rgb320_age_clap.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/val/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=1,\n",
      "                test_mode=True),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='CenterCrop', crop_size=224),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'),\n",
      "    test=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_test_rgb320_age_clap.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/test/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=10,\n",
      "                test_mode=True),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='ThreeCrop', crop_size=256),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'))\n",
      "evaluation = dict(\n",
      "    interval=5, metrics=['top_k_accuracy', 'mean_class_accuracy'])\n",
      "optimizer = dict(type='SGD', lr=0.000125, momentum=0.9, weight_decay=0.0001)\n",
      "optimizer_config = dict(grad_clip=dict(max_norm=40, norm_type=2))\n",
      "lr_config = dict(\n",
      "    policy='step',\n",
      "    step=[32, 48],\n",
      "    warmup='linear',\n",
      "    warmup_ratio=0.1,\n",
      "    warmup_by_epoch=True,\n",
      "    warmup_iters=16)\n",
      "total_epochs = 50\n",
      "work_dir = './childact-checkpoints/CSN-age-box'\n",
      "find_unused_parameters = True\n",
      "omnisource = False\n",
      "seed = 42\n",
      "gpu_ids = range(0, 1)\n",
      "output_config = dict(out='./childact-checkpoints/CSN-age-box/results.json')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mmcv.runner import set_random_seed\n",
    "\n",
    "# Modify dataset type and path\n",
    "# cfg.dataset_type = 'VideoDataset'\n",
    "cfg.data_root = 'age-gender-3split-rgb-frames/'\n",
    "cfg.data_root_val = 'age-gender-3split-rgb-frames/val/'\n",
    "cfg.ann_file_train = 'age-gender-3split-rgb-frames/childact_train_rgb320_age_clap.txt'\n",
    "cfg.ann_file_val = 'age-gender-3split-rgb-frames/childact_val_rgb320_age_clap.txt'\n",
    "cfg.ann_file_test = 'age-gender-3split-rgb-frames/childact_test_rgb320_age_clap.txt'\n",
    "\n",
    "# cfg.data.test.type = 'VideoDataset'\n",
    "cfg.data.test.ann_file = 'age-gender-3split-rgb-frames/childact_test_rgb320_age_clap.txt'\n",
    "cfg.data.test.data_prefix = 'age-gender-3split-rgb-frames/test/'\n",
    "\n",
    "# cfg.data.train.type = 'VideoDataset'\n",
    "cfg.data.train.ann_file = 'age-gender-3split-rgb-frames/childact_train_rgb320_age_clap.txt'\n",
    "cfg.data.train.data_prefix = 'age-gender-3split-rgb-frames/train/'\n",
    "\n",
    "# cfg.data.val.type = 'VideoDataset'\n",
    "cfg.data.val.ann_file = 'age-gender-3split-rgb-frames/childact_val_rgb320_age_clap.txt'\n",
    "cfg.data.val.data_prefix = 'age-gender-3split-rgb-frames/val/'\n",
    "\n",
    "# cfg.data.test.modality = 'Flow'\n",
    "# cfg.data.val.modality = 'Flow'\n",
    "# cfg.data.train.modality = 'Flow'\n",
    "\n",
    "# cfg.data.train.start_index = 0\n",
    "# cfg.data.test.start_index = 0\n",
    "# cfg.data.val.start_index = 0\n",
    "\n",
    "cfg.data.test.filename_tmpl = '{:03}.jpeg'\n",
    "cfg.data.train.filename_tmpl = '{:03}.jpeg'\n",
    "cfg.data.val.filename_tmpl = '{:03}.jpeg'\n",
    "# The flag is used to determine whether it is omnisource training\n",
    "cfg.setdefault('omnisource', False)\n",
    "# Modify num classes of the model in cls_head\n",
    "cfg.model.cls_head.num_classes = 6\n",
    "# We can use the pre-trained TSN model\n",
    "cfg.load_from = 'checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth'\n",
    "# cfg.resume_from = './childact-mm/latest.pth'\n",
    "\n",
    "# Set up working dir to save files and logs.\n",
    "cfg.work_dir = './childact-checkpoints/CSN-age-box'\n",
    "\n",
    "cfg.total_epochs = 50\n",
    "\n",
    "# cfg.momentum_config = dict(\n",
    "#     policy='cyclic',\n",
    "#     target_ratio=(0.85 / 0.95, 1),\n",
    "#     cyclic_times=1,\n",
    "#     step_ratio_up=0.4,\n",
    "# )\n",
    "\n",
    "# We can set the checkpoint saving interval to reduce the storage cost\n",
    "cfg.checkpoint_config.interval = 20\n",
    "# We can set the log print interval to reduce the the times of printing log\n",
    "cfg.log_config.interval = 100\n",
    "\n",
    "# Set seed thus the results are more reproducible\n",
    "cfg.seed = 42\n",
    "set_random_seed(42, deterministic=False)\n",
    "cfg.gpu_ids = range(1)\n",
    "\n",
    "cfg.data.videos_per_gpu=4\n",
    "\n",
    "# cfg.model.backbone.in_channels = 2\n",
    "\n",
    "cfg.output_config = dict(out=f'{cfg.work_dir}/results.json')\n",
    "# We can initialize the logger for training and have a look\n",
    "# at the final config used for training\n",
    "# del cfg.optimizer['momentum']\n",
    "print(f'Config:\\n{cfg.pretty_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd758729-843c-4476-b478-97d77772b037",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "dDBWkdDRk6oz",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "85a52ef3-7b5c-4c52-8fef-00322a8c65e6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 08:52:09,598 - mmaction - INFO - load model from: https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth\n",
      "2021-08-10 08:52:09,599 - mmaction - INFO - Use load_from_http loader\n",
      "2021-08-10 08:52:11,641 - mmaction - INFO - load checkpoint from checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth\n",
      "2021-08-10 08:52:11,642 - mmaction - INFO - Use load_from_local loader\n",
      "2021-08-10 08:52:11,809 - mmaction - WARNING - The model and loaded state dict do not match exactly\n",
      "\n",
      "size mismatch for cls_head.fc_cls.weight: copying a param with shape torch.Size([400, 2048]) from checkpoint, the shape in current model is torch.Size([6, 2048]).\n",
      "size mismatch for cls_head.fc_cls.bias: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([6]).\n",
      "2021-08-10 08:52:11,815 - mmaction - INFO - Start running, host: robt427nv@robt427NV, work_dir: /home/robt427nv/childact/childact-checkpoints/CSN-age-box\n",
      "2021-08-10 08:52:11,816 - mmaction - INFO - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(ABOVE_NORMAL) OptimizerHook                      \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "2021-08-10 08:52:11,816 - mmaction - INFO - workflow: [('train', 1)], max: 50 epochs\n",
      "/home/robt427nv/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/mmcv/runner/hooks/evaluation.py:190: UserWarning: runner.meta is None. Creating an empty one.\n",
      "  warnings.warn('runner.meta is None. Creating an empty one.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 14/14, 9.8 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 08:54:38,069 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 08:54:38,070 - mmaction - INFO - \n",
      "top1_acc\t0.2143\n",
      "top5_acc\t0.9286\n",
      "2021-08-10 08:54:38,071 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 08:54:38,073 - mmaction - INFO - \n",
      "mean_acc\t0.1905\n",
      "2021-08-10 08:54:38,394 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_5.pth.\n",
      "2021-08-10 08:54:38,396 - mmaction - INFO - Best top1_acc is 0.2143 at 5 epoch.\n",
      "2021-08-10 08:54:38,396 - mmaction - INFO - Epoch(val) [5][4]\ttop1_acc: 0.2143, top5_acc: 0.9286, mean_class_accuracy: 0.1905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 14/14, 9.8 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 08:57:05,509 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 08:57:05,511 - mmaction - INFO - \n",
      "top1_acc\t0.5000\n",
      "top5_acc\t0.9286\n",
      "2021-08-10 08:57:05,511 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 08:57:05,512 - mmaction - INFO - \n",
      "mean_acc\t0.1667\n",
      "2021-08-10 08:57:05,849 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_10.pth.\n",
      "2021-08-10 08:57:05,850 - mmaction - INFO - Best top1_acc is 0.5000 at 10 epoch.\n",
      "2021-08-10 08:57:05,851 - mmaction - INFO - Epoch(val) [10][4]\ttop1_acc: 0.5000, top5_acc: 0.9286, mean_class_accuracy: 0.1667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 14/14, 10.0 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 08:59:33,436 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 08:59:33,438 - mmaction - INFO - \n",
      "top1_acc\t0.4286\n",
      "top5_acc\t0.9286\n",
      "2021-08-10 08:59:33,439 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 08:59:33,441 - mmaction - INFO - \n",
      "mean_acc\t0.2857\n",
      "2021-08-10 08:59:33,442 - mmaction - INFO - Epoch(val) [15][4]\ttop1_acc: 0.4286, top5_acc: 0.9286, mean_class_accuracy: 0.2857\n",
      "2021-08-10 09:01:59,151 - mmaction - INFO - Saving checkpoint at 20 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 14/14, 10.5 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 09:02:00,960 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 09:02:00,962 - mmaction - INFO - \n",
      "top1_acc\t0.5000\n",
      "top5_acc\t0.8571\n",
      "2021-08-10 09:02:00,963 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 09:02:00,964 - mmaction - INFO - \n",
      "mean_acc\t0.4286\n",
      "2021-08-10 09:02:00,965 - mmaction - INFO - Epoch(val) [20][4]\ttop1_acc: 0.5000, top5_acc: 0.8571, mean_class_accuracy: 0.4286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 14/14, 10.6 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 09:04:28,396 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 09:04:28,398 - mmaction - INFO - \n",
      "top1_acc\t0.3571\n",
      "top5_acc\t0.9286\n",
      "2021-08-10 09:04:28,398 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 09:04:28,399 - mmaction - INFO - \n",
      "mean_acc\t0.3214\n",
      "2021-08-10 09:04:28,399 - mmaction - INFO - Epoch(val) [25][4]\ttop1_acc: 0.3571, top5_acc: 0.9286, mean_class_accuracy: 0.3214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 14/14, 10.2 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 09:06:55,330 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 09:06:55,331 - mmaction - INFO - \n",
      "top1_acc\t0.3571\n",
      "top5_acc\t0.9286\n",
      "2021-08-10 09:06:55,332 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 09:06:55,333 - mmaction - INFO - \n",
      "mean_acc\t0.3810\n",
      "2021-08-10 09:06:55,334 - mmaction - INFO - Epoch(val) [30][4]\ttop1_acc: 0.3571, top5_acc: 0.9286, mean_class_accuracy: 0.3810\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 14/14, 10.1 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 09:09:21,996 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 09:09:21,998 - mmaction - INFO - \n",
      "top1_acc\t0.3571\n",
      "top5_acc\t0.9286\n",
      "2021-08-10 09:09:21,998 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 09:09:22,000 - mmaction - INFO - \n",
      "mean_acc\t0.3214\n",
      "2021-08-10 09:09:22,001 - mmaction - INFO - Epoch(val) [35][4]\ttop1_acc: 0.3571, top5_acc: 0.9286, mean_class_accuracy: 0.3214\n",
      "2021-08-10 09:11:48,431 - mmaction - INFO - Saving checkpoint at 40 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 14/14, 10.0 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 09:11:50,333 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 09:11:50,334 - mmaction - INFO - \n",
      "top1_acc\t0.2857\n",
      "top5_acc\t0.9286\n",
      "2021-08-10 09:11:50,335 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 09:11:50,336 - mmaction - INFO - \n",
      "mean_acc\t0.2976\n",
      "2021-08-10 09:11:50,336 - mmaction - INFO - Epoch(val) [40][4]\ttop1_acc: 0.2857, top5_acc: 0.9286, mean_class_accuracy: 0.2976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 14/14, 10.1 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 09:14:17,482 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 09:14:17,483 - mmaction - INFO - \n",
      "top1_acc\t0.3571\n",
      "top5_acc\t0.9286\n",
      "2021-08-10 09:14:17,483 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 09:14:17,484 - mmaction - INFO - \n",
      "mean_acc\t0.3810\n",
      "2021-08-10 09:14:17,484 - mmaction - INFO - Epoch(val) [45][4]\ttop1_acc: 0.3571, top5_acc: 0.9286, mean_class_accuracy: 0.3810\n",
      "2021-08-10 09:16:43,161 - mmaction - INFO - Saving checkpoint at 50 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 14/14, 10.0 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 09:16:45,035 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 09:16:45,037 - mmaction - INFO - \n",
      "top1_acc\t0.3571\n",
      "top5_acc\t0.9286\n",
      "2021-08-10 09:16:45,037 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 09:16:45,038 - mmaction - INFO - \n",
      "mean_acc\t0.3810\n",
      "2021-08-10 09:16:45,039 - mmaction - INFO - Epoch(val) [50][4]\ttop1_acc: 0.3571, top5_acc: 0.9286, mean_class_accuracy: 0.3810\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "from mmaction.datasets import build_dataset\n",
    "from mmaction.models import build_model\n",
    "from mmaction.apis import train_model\n",
    "\n",
    "import mmcv\n",
    "\n",
    "# Build the dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# Build the recognizer\n",
    "model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.train_cfg, test_cfg=cfg.test_cfg)\n",
    "# CUDA_LAUNCH_BLOCKING=1\n",
    "# Create work_dir\n",
    "mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
    "train_model(model, datasets, cfg, distributed=False, validate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e16f52ee-f73c-439a-b579-637a777dcf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f\"{cfg.work_dir}/model50e\", 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "293f078b-0dd0-4912-82a8-7a3bfdd1ea91",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eyY3hCMwyTct",
    "outputId": "200e37c7-0da4-421f-98da-41418c3110ca",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 18/18, 0.6 task/s, elapsed: 29s, ETA:     0s\n",
      "Evaluating top_k_accuracy ...\n",
      "\n",
      "top1_acc\t0.5000\n",
      "top5_acc\t0.9444\n",
      "\n",
      "Evaluating mean_class_accuracy ...\n",
      "\n",
      "mean_acc\t0.2569\n",
      "top1_acc: 0.5000\n",
      "top5_acc: 0.9444\n",
      "mean_class_accuracy: 0.2569\n"
     ]
    }
   ],
   "source": [
    "from mmaction.apis import single_gpu_test\n",
    "from mmaction.datasets import build_dataloader\n",
    "from mmcv.parallel import MMDataParallel\n",
    "# from mmaction.models import build_model\n",
    "# from mmaction.datasets import build_dataset\n",
    "\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# Build a test dataloader\n",
    "dataset = build_dataset(cfg.data.test, dict(test_mode=True))\n",
    "data_loader = build_dataloader(\n",
    "        dataset,\n",
    "        videos_per_gpu=1,\n",
    "        workers_per_gpu=2,\n",
    "        dist=False,\n",
    "        shuffle=False)\n",
    "model = MMDataParallel(model, device_ids=[0])\n",
    "outputs = single_gpu_test(model, data_loader)\n",
    "\n",
    "eval_config = cfg.evaluation\n",
    "eval_config.pop('interval')\n",
    "eval_res = dataset.evaluate(outputs, **eval_config)\n",
    "for name, val in eval_res.items():\n",
    "    print(f'{name}: {val:.04f}')\n",
    "    \n",
    "output_config = cfg.output_config\n",
    "dataset.dump_results(outputs, **output_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f5f6358-f472-459d-8b27-7b51bb8e18ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from mmaction.core import confusion_matrix \n",
    "\n",
    "gt_labels = [ann['label'] for ann in dataset.load_annotations()]\n",
    "pred = np.argmax(outputs, axis=1)\n",
    "cf_mat = confusion_matrix(pred, gt_labels).astype(float)\n",
    "cmap = sns.cubehelix_palette(50, hue=0.05, rot=0, light=0.9, dark=0, as_cmap=True)\n",
    "# /np.sum(cf_mat), fmt='.2%',\n",
    "sns.heatmap(cf_mat, cmap=cmap, annot=True, xticklabels = ['6yo', '7yo', '8yo', '9yo', '10yo', '11yo'], yticklabels = ['6yo', '7yo', '8yo', '9yo', '10yo', '11yo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65a7f8bc-5787-4ea2-88c0-f8f35ae396bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "mean_error = (1+1+1+1+3+1+1)/18 #number of all tested videos\n",
    "print(mean_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0d3c5c-7c01-473b-8c2e-1fe84570c1ef",
   "metadata": {},
   "source": [
    "# CSN age go "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9532f2b7-9f58-44ce-9119-6f4ec27a7457",
   "metadata": {
    "id": "LjCcmCKOjktc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mmcv import Config\n",
    "cfg = Config.fromfile('mmaction2/configs/recognition/csn/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a49c91d-d2a5-44dd-96ed-e38f1616e463",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "tlhu9byjjt-K",
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "outputId": "13d1bb01-f351-48c0-9efb-0bdf2c125d29",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "model = dict(\n",
      "    type='Recognizer3D',\n",
      "    backbone=dict(\n",
      "        type='ResNet3dCSN',\n",
      "        pretrained2d=False,\n",
      "        pretrained=\n",
      "        'https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth',\n",
      "        depth=152,\n",
      "        with_pool2=False,\n",
      "        bottleneck_mode='ir',\n",
      "        norm_eval=True,\n",
      "        zero_init_residual=False,\n",
      "        bn_frozen=True),\n",
      "    cls_head=dict(\n",
      "        type='I3DHead',\n",
      "        num_classes=6,\n",
      "        in_channels=2048,\n",
      "        spatial_type='avg',\n",
      "        dropout_ratio=0.5,\n",
      "        init_std=0.01),\n",
      "    train_cfg=None,\n",
      "    test_cfg=dict(average_clips='prob'))\n",
      "checkpoint_config = dict(interval=20)\n",
      "log_config = dict(interval=100, hooks=[dict(type='TextLoggerHook')])\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = 'checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth'\n",
      "resume_from = None\n",
      "workflow = [('train', 1)]\n",
      "dataset_type = 'RawframeDataset'\n",
      "data_root = 'age-gender-3split-rgb-frames/'\n",
      "data_root_val = 'age-gender-3split-rgb-frames/val/'\n",
      "ann_file_train = 'age-gender-3split-rgb-frames/childact_train_rgb320_age_go.txt'\n",
      "ann_file_val = 'age-gender-3split-rgb-frames/childact_val_rgb320_age_go.txt'\n",
      "ann_file_test = 'age-gender-3split-rgb-frames/childact_test_rgb320_age_go.txt'\n",
      "img_norm_cfg = dict(\n",
      "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_bgr=False)\n",
      "train_pipeline = [\n",
      "    dict(type='SampleFrames', clip_len=32, frame_interval=2, num_clips=1),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='RandomResizedCrop'),\n",
      "    dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "    dict(type='Flip', flip_ratio=0.5),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "]\n",
      "val_pipeline = [\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=32,\n",
      "        frame_interval=2,\n",
      "        num_clips=1,\n",
      "        test_mode=True),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='CenterCrop', crop_size=224),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=32,\n",
      "        frame_interval=2,\n",
      "        num_clips=10,\n",
      "        test_mode=True),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='ThreeCrop', crop_size=256),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "data = dict(\n",
      "    videos_per_gpu=4,\n",
      "    workers_per_gpu=4,\n",
      "    train=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_train_rgb320_age_go.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/train/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=1),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='RandomResizedCrop'),\n",
      "            dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "            dict(type='Flip', flip_ratio=0.5),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'),\n",
      "    val=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file='age-gender-3split-rgb-frames/childact_val_rgb320_age_go.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/val/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=1,\n",
      "                test_mode=True),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='CenterCrop', crop_size=224),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'),\n",
      "    test=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file='age-gender-3split-rgb-frames/childact_test_rgb320_age_go.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/test/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=10,\n",
      "                test_mode=True),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='ThreeCrop', crop_size=256),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'))\n",
      "evaluation = dict(\n",
      "    interval=5, metrics=['top_k_accuracy', 'mean_class_accuracy'])\n",
      "optimizer = dict(type='SGD', lr=0.000125, momentum=0.9, weight_decay=0.0001)\n",
      "optimizer_config = dict(grad_clip=dict(max_norm=40, norm_type=2))\n",
      "lr_config = dict(\n",
      "    policy='step',\n",
      "    step=[32, 48],\n",
      "    warmup='linear',\n",
      "    warmup_ratio=0.1,\n",
      "    warmup_by_epoch=True,\n",
      "    warmup_iters=16)\n",
      "total_epochs = 50\n",
      "work_dir = './childact-checkpoints/CSN-age-go'\n",
      "find_unused_parameters = True\n",
      "omnisource = False\n",
      "seed = 42\n",
      "gpu_ids = range(0, 1)\n",
      "output_config = dict(out='./childact-checkpoints/CSN-age-go/results.json')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mmcv.runner import set_random_seed\n",
    "\n",
    "# Modify dataset type and path\n",
    "# cfg.dataset_type = 'VideoDataset'\n",
    "cfg.data_root = 'age-gender-3split-rgb-frames/'\n",
    "cfg.data_root_val = 'age-gender-3split-rgb-frames/val/'\n",
    "cfg.ann_file_train = 'age-gender-3split-rgb-frames/childact_train_rgb320_age_go.txt'\n",
    "cfg.ann_file_val = 'age-gender-3split-rgb-frames/childact_val_rgb320_age_go.txt'\n",
    "cfg.ann_file_test = 'age-gender-3split-rgb-frames/childact_test_rgb320_age_go.txt'\n",
    "\n",
    "# cfg.data.test.type = 'VideoDataset'\n",
    "cfg.data.test.ann_file = 'age-gender-3split-rgb-frames/childact_test_rgb320_age_go.txt'\n",
    "cfg.data.test.data_prefix = 'age-gender-3split-rgb-frames/test/'\n",
    "\n",
    "# cfg.data.train.type = 'VideoDataset'\n",
    "cfg.data.train.ann_file = 'age-gender-3split-rgb-frames/childact_train_rgb320_age_go.txt'\n",
    "cfg.data.train.data_prefix = 'age-gender-3split-rgb-frames/train/'\n",
    "\n",
    "# cfg.data.val.type = 'VideoDataset'\n",
    "cfg.data.val.ann_file = 'age-gender-3split-rgb-frames/childact_val_rgb320_age_go.txt'\n",
    "cfg.data.val.data_prefix = 'age-gender-3split-rgb-frames/val/'\n",
    "\n",
    "# cfg.data.test.modality = 'Flow'\n",
    "# cfg.data.val.modality = 'Flow'\n",
    "# cfg.data.train.modality = 'Flow'\n",
    "\n",
    "# cfg.data.train.start_index = 0\n",
    "# cfg.data.test.start_index = 0\n",
    "# cfg.data.val.start_index = 0\n",
    "\n",
    "cfg.data.test.filename_tmpl = '{:03}.jpeg'\n",
    "cfg.data.train.filename_tmpl = '{:03}.jpeg'\n",
    "cfg.data.val.filename_tmpl = '{:03}.jpeg'\n",
    "# The flag is used to determine whether it is omnisource training\n",
    "cfg.setdefault('omnisource', False)\n",
    "# Modify num classes of the model in cls_head\n",
    "cfg.model.cls_head.num_classes = 6\n",
    "# We can use the pre-trained TSN model\n",
    "cfg.load_from = 'checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth'\n",
    "# cfg.resume_from = './childact-mm/latest.pth'\n",
    "\n",
    "# Set up working dir to save files and logs.\n",
    "cfg.work_dir = './childact-checkpoints/CSN-age-go'\n",
    "\n",
    "cfg.total_epochs = 50\n",
    "\n",
    "# cfg.momentum_config = dict(\n",
    "#     policy='cyclic',\n",
    "#     target_ratio=(0.85 / 0.95, 1),\n",
    "#     cyclic_times=1,\n",
    "#     step_ratio_up=0.4,\n",
    "# )\n",
    "\n",
    "# We can set the checkpoint saving interval to reduce the storage cost\n",
    "cfg.checkpoint_config.interval = 20\n",
    "# We can set the log print interval to reduce the the times of printing log\n",
    "cfg.log_config.interval = 100\n",
    "\n",
    "# Set seed thus the results are more reproducible\n",
    "cfg.seed = 42\n",
    "set_random_seed(42, deterministic=False)\n",
    "cfg.gpu_ids = range(1)\n",
    "\n",
    "cfg.data.videos_per_gpu=4\n",
    "\n",
    "# cfg.model.backbone.in_channels = 2\n",
    "\n",
    "cfg.output_config = dict(out=f'{cfg.work_dir}/results.json')\n",
    "# We can initialize the logger for training and have a look\n",
    "# at the final config used for training\n",
    "# del cfg.optimizer['momentum']\n",
    "print(f'Config:\\n{cfg.pretty_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15612c13-8dea-4515-b084-43d185f78fe7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "dDBWkdDRk6oz",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "85a52ef3-7b5c-4c52-8fef-00322a8c65e6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 09:40:44,136 - mmaction - INFO - load model from: https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth\n",
      "2021-08-10 09:40:44,137 - mmaction - INFO - Use load_from_http loader\n",
      "2021-08-10 09:40:44,381 - mmaction - INFO - load checkpoint from checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth\n",
      "2021-08-10 09:40:44,382 - mmaction - INFO - Use load_from_local loader\n",
      "2021-08-10 09:40:44,546 - mmaction - WARNING - The model and loaded state dict do not match exactly\n",
      "\n",
      "size mismatch for cls_head.fc_cls.weight: copying a param with shape torch.Size([400, 2048]) from checkpoint, the shape in current model is torch.Size([6, 2048]).\n",
      "size mismatch for cls_head.fc_cls.bias: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([6]).\n",
      "2021-08-10 09:40:44,549 - mmaction - INFO - Start running, host: robt427nv@robt427NV, work_dir: /home/robt427nv/childact/childact-checkpoints/CSN-age-go\n",
      "2021-08-10 09:40:44,551 - mmaction - INFO - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(ABOVE_NORMAL) OptimizerHook                      \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "2021-08-10 09:40:44,552 - mmaction - INFO - workflow: [('train', 1)], max: 50 epochs\n",
      "/home/robt427nv/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/mmcv/runner/hooks/evaluation.py:190: UserWarning: runner.meta is None. Creating an empty one.\n",
      "  warnings.warn('runner.meta is None. Creating an empty one.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 18/18, 10.1 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 09:42:50,544 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 09:42:50,545 - mmaction - INFO - \n",
      "top1_acc\t0.3889\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 09:42:50,546 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 09:42:50,547 - mmaction - INFO - \n",
      "mean_acc\t0.2500\n",
      "2021-08-10 09:42:50,846 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_5.pth.\n",
      "2021-08-10 09:42:50,847 - mmaction - INFO - Best top1_acc is 0.3889 at 5 epoch.\n",
      "2021-08-10 09:42:50,848 - mmaction - INFO - Epoch(val) [5][5]\ttop1_acc: 0.3889, top5_acc: 1.0000, mean_class_accuracy: 0.2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 18/18, 10.3 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 09:44:57,985 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 09:44:57,988 - mmaction - INFO - \n",
      "top1_acc\t0.5000\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 09:44:57,988 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 09:44:57,990 - mmaction - INFO - \n",
      "mean_acc\t0.3000\n",
      "2021-08-10 09:44:58,325 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_10.pth.\n",
      "2021-08-10 09:44:58,326 - mmaction - INFO - Best top1_acc is 0.5000 at 10 epoch.\n",
      "2021-08-10 09:44:58,327 - mmaction - INFO - Epoch(val) [10][5]\ttop1_acc: 0.5000, top5_acc: 1.0000, mean_class_accuracy: 0.3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 18/18, 10.7 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 09:47:05,944 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 09:47:05,946 - mmaction - INFO - \n",
      "top1_acc\t0.6111\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 09:47:05,946 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 09:47:05,948 - mmaction - INFO - \n",
      "mean_acc\t0.4550\n",
      "2021-08-10 09:47:06,291 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_15.pth.\n",
      "2021-08-10 09:47:06,292 - mmaction - INFO - Best top1_acc is 0.6111 at 15 epoch.\n",
      "2021-08-10 09:47:06,293 - mmaction - INFO - Epoch(val) [15][5]\ttop1_acc: 0.6111, top5_acc: 1.0000, mean_class_accuracy: 0.4550\n",
      "2021-08-10 09:49:12,101 - mmaction - INFO - Saving checkpoint at 20 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 18/18, 10.9 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 09:49:14,144 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 09:49:14,146 - mmaction - INFO - \n",
      "top1_acc\t0.4444\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 09:49:14,146 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 09:49:14,147 - mmaction - INFO - \n",
      "mean_acc\t0.2300\n",
      "2021-08-10 09:49:14,147 - mmaction - INFO - Epoch(val) [20][5]\ttop1_acc: 0.4444, top5_acc: 1.0000, mean_class_accuracy: 0.2300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 18/18, 10.3 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 09:51:21,538 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 09:51:21,539 - mmaction - INFO - \n",
      "top1_acc\t0.6667\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 09:51:21,539 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 09:51:21,540 - mmaction - INFO - \n",
      "mean_acc\t0.5100\n",
      "2021-08-10 09:51:21,858 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_25.pth.\n",
      "2021-08-10 09:51:21,859 - mmaction - INFO - Best top1_acc is 0.6667 at 25 epoch.\n",
      "2021-08-10 09:51:21,860 - mmaction - INFO - Epoch(val) [25][5]\ttop1_acc: 0.6667, top5_acc: 1.0000, mean_class_accuracy: 0.5100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 18/18, 10.7 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 09:53:29,780 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 09:53:29,782 - mmaction - INFO - \n",
      "top1_acc\t0.3333\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 09:53:29,783 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 09:53:29,784 - mmaction - INFO - \n",
      "mean_acc\t0.2250\n",
      "2021-08-10 09:53:29,785 - mmaction - INFO - Epoch(val) [30][5]\ttop1_acc: 0.3333, top5_acc: 1.0000, mean_class_accuracy: 0.2250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 18/18, 10.5 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 09:55:37,088 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 09:55:37,090 - mmaction - INFO - \n",
      "top1_acc\t0.6667\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 09:55:37,090 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 09:55:37,091 - mmaction - INFO - \n",
      "mean_acc\t0.6450\n",
      "2021-08-10 09:55:37,092 - mmaction - INFO - Epoch(val) [35][5]\ttop1_acc: 0.6667, top5_acc: 1.0000, mean_class_accuracy: 0.6450\n",
      "2021-08-10 09:57:42,632 - mmaction - INFO - Saving checkpoint at 40 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 18/18, 10.6 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 09:57:44,741 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 09:57:44,742 - mmaction - INFO - \n",
      "top1_acc\t0.6667\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 09:57:44,743 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 09:57:44,744 - mmaction - INFO - \n",
      "mean_acc\t0.6450\n",
      "2021-08-10 09:57:44,745 - mmaction - INFO - Epoch(val) [40][5]\ttop1_acc: 0.6667, top5_acc: 1.0000, mean_class_accuracy: 0.6450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 18/18, 10.6 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 09:59:51,790 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 09:59:51,792 - mmaction - INFO - \n",
      "top1_acc\t0.6667\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 09:59:51,793 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 09:59:51,794 - mmaction - INFO - \n",
      "mean_acc\t0.6450\n",
      "2021-08-10 09:59:51,795 - mmaction - INFO - Epoch(val) [45][5]\ttop1_acc: 0.6667, top5_acc: 1.0000, mean_class_accuracy: 0.6450\n",
      "2021-08-10 10:01:57,274 - mmaction - INFO - Saving checkpoint at 50 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 18/18, 11.0 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 10:01:59,299 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 10:01:59,301 - mmaction - INFO - \n",
      "top1_acc\t0.6667\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 10:01:59,302 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 10:01:59,303 - mmaction - INFO - \n",
      "mean_acc\t0.5850\n",
      "2021-08-10 10:01:59,304 - mmaction - INFO - Epoch(val) [50][5]\ttop1_acc: 0.6667, top5_acc: 1.0000, mean_class_accuracy: 0.5850\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "from mmaction.datasets import build_dataset\n",
    "from mmaction.models import build_model\n",
    "from mmaction.apis import train_model\n",
    "\n",
    "import mmcv\n",
    "\n",
    "# Build the dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# Build the recognizer\n",
    "model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.train_cfg, test_cfg=cfg.test_cfg)\n",
    "# CUDA_LAUNCH_BLOCKING=1\n",
    "# Create work_dir\n",
    "mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
    "train_model(model, datasets, cfg, distributed=False, validate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3fa717db-1fa7-4a43-abe7-3da42cd8d3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f\"{cfg.work_dir}/model50e\", 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22098c4d-b3a1-4c46-abad-e4ef9f36332c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 10:10:19,285 - mmaction - INFO - load model from: https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth\n",
      "2021-08-10 10:10:19,286 - mmaction - INFO - Use load_from_http loader\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "from mmaction.datasets import build_dataset\n",
    "from mmaction.models import build_model\n",
    "from mmaction.apis import train_model\n",
    "import pickle\n",
    "import mmcv\n",
    "# Build the dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# Build the recognizer\n",
    "model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "\n",
    "model = pickle.load(open(f\"{cfg.work_dir}/model50e\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d475d2b-9e9b-4016-ac3a-511eaf41e2c4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eyY3hCMwyTct",
    "outputId": "200e37c7-0da4-421f-98da-41418c3110ca",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 22/22, 0.7 task/s, elapsed: 33s, ETA:     0s\n",
      "Evaluating top_k_accuracy ...\n",
      "\n",
      "top1_acc\t0.5455\n",
      "top5_acc\t0.9091\n",
      "\n",
      "Evaluating mean_class_accuracy ...\n",
      "\n",
      "mean_acc\t0.3847\n",
      "top1_acc: 0.5455\n",
      "top5_acc: 0.9091\n",
      "mean_class_accuracy: 0.3847\n"
     ]
    }
   ],
   "source": [
    "from mmaction.apis import single_gpu_test\n",
    "from mmaction.datasets import build_dataloader\n",
    "from mmcv.parallel import MMDataParallel\n",
    "# from mmaction.models import build_model\n",
    "# from mmaction.datasets import build_dataset\n",
    "\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# Build a test dataloader\n",
    "dataset = build_dataset(cfg.data.test, dict(test_mode=True))\n",
    "data_loader = build_dataloader(\n",
    "        dataset,\n",
    "        videos_per_gpu=1,\n",
    "        workers_per_gpu=2,\n",
    "        dist=False,\n",
    "        shuffle=False)\n",
    "model = MMDataParallel(model, device_ids=[0])\n",
    "outputs = single_gpu_test(model, data_loader)\n",
    "\n",
    "eval_config = cfg.evaluation\n",
    "eval_config.pop('interval')\n",
    "eval_res = dataset.evaluate(outputs, **eval_config)\n",
    "for name, val in eval_res.items():\n",
    "    print(f'{name}: {val:.04f}')\n",
    "    \n",
    "output_config = cfg.output_config\n",
    "dataset.dump_results(outputs, **output_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0bd7db4-ae6b-4f08-8164-bfef79f6e59c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from mmaction.core import confusion_matrix \n",
    "\n",
    "gt_labels = [ann['label'] for ann in dataset.load_annotations()]\n",
    "pred = np.argmax(outputs, axis=1)\n",
    "cf_mat = confusion_matrix(pred, gt_labels).astype(float)\n",
    "cmap = sns.cubehelix_palette(50, hue=0.05, rot=0, light=0.9, dark=0, as_cmap=True)\n",
    "# /np.sum(cf_mat), fmt='.2%',\n",
    "sns.heatmap(cf_mat, cmap=cmap, annot=True, xticklabels = ['6yo', '7yo', '8yo', '9yo', '10yo', '11yo'], yticklabels = ['6yo', '7yo', '8yo', '9yo', '10yo', '11yo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5a001d4-df1e-46d5-9f5e-805438ba858d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5909090909090909\n"
     ]
    }
   ],
   "source": [
    "mean_error = (4+2+1+2+2+1+1)/22 #number of all tested videos\n",
    "print(mean_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650c7218-bd9f-4399-996b-98206ce1b259",
   "metadata": {},
   "source": [
    "# CSN age jog "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "177ce2db-e34c-4ebf-b704-60eb60dd370f",
   "metadata": {
    "id": "LjCcmCKOjktc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mmcv import Config\n",
    "cfg = Config.fromfile('mmaction2/configs/recognition/csn/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21f37bad-181b-4687-aeb7-c7b4cb202f52",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "tlhu9byjjt-K",
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "outputId": "13d1bb01-f351-48c0-9efb-0bdf2c125d29",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "model = dict(\n",
      "    type='Recognizer3D',\n",
      "    backbone=dict(\n",
      "        type='ResNet3dCSN',\n",
      "        pretrained2d=False,\n",
      "        pretrained=\n",
      "        'https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth',\n",
      "        depth=152,\n",
      "        with_pool2=False,\n",
      "        bottleneck_mode='ir',\n",
      "        norm_eval=True,\n",
      "        zero_init_residual=False,\n",
      "        bn_frozen=True),\n",
      "    cls_head=dict(\n",
      "        type='I3DHead',\n",
      "        num_classes=6,\n",
      "        in_channels=2048,\n",
      "        spatial_type='avg',\n",
      "        dropout_ratio=0.5,\n",
      "        init_std=0.01),\n",
      "    train_cfg=None,\n",
      "    test_cfg=dict(average_clips='prob'))\n",
      "checkpoint_config = dict(interval=20)\n",
      "log_config = dict(interval=100, hooks=[dict(type='TextLoggerHook')])\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = 'checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth'\n",
      "resume_from = None\n",
      "workflow = [('train', 1)]\n",
      "dataset_type = 'RawframeDataset'\n",
      "data_root = 'age-gender-3split-rgb-frames/'\n",
      "data_root_val = 'age-gender-3split-rgb-frames/val/'\n",
      "ann_file_train = 'age-gender-3split-rgb-frames/childact_train_rgb320_age_jog.txt'\n",
      "ann_file_val = 'age-gender-3split-rgb-frames/childact_val_rgb320_age_jog.txt'\n",
      "ann_file_test = 'age-gender-3split-rgb-frames/childact_test_rgb320_age_jog.txt'\n",
      "img_norm_cfg = dict(\n",
      "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_bgr=False)\n",
      "train_pipeline = [\n",
      "    dict(type='SampleFrames', clip_len=32, frame_interval=2, num_clips=1),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='RandomResizedCrop'),\n",
      "    dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "    dict(type='Flip', flip_ratio=0.5),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "]\n",
      "val_pipeline = [\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=32,\n",
      "        frame_interval=2,\n",
      "        num_clips=1,\n",
      "        test_mode=True),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='CenterCrop', crop_size=224),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=32,\n",
      "        frame_interval=2,\n",
      "        num_clips=10,\n",
      "        test_mode=True),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='ThreeCrop', crop_size=256),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "data = dict(\n",
      "    videos_per_gpu=4,\n",
      "    workers_per_gpu=4,\n",
      "    train=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_train_rgb320_age_jog.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/train/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=1),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='RandomResizedCrop'),\n",
      "            dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "            dict(type='Flip', flip_ratio=0.5),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'),\n",
      "    val=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file='age-gender-3split-rgb-frames/childact_val_rgb320_age_jog.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/val/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=1,\n",
      "                test_mode=True),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='CenterCrop', crop_size=224),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'),\n",
      "    test=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_test_rgb320_age_jog.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/test/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=10,\n",
      "                test_mode=True),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='ThreeCrop', crop_size=256),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'))\n",
      "evaluation = dict(\n",
      "    interval=5, metrics=['top_k_accuracy', 'mean_class_accuracy'])\n",
      "optimizer = dict(type='SGD', lr=0.000125, momentum=0.9, weight_decay=0.0001)\n",
      "optimizer_config = dict(grad_clip=dict(max_norm=40, norm_type=2))\n",
      "lr_config = dict(\n",
      "    policy='step',\n",
      "    step=[32, 48],\n",
      "    warmup='linear',\n",
      "    warmup_ratio=0.1,\n",
      "    warmup_by_epoch=True,\n",
      "    warmup_iters=16)\n",
      "total_epochs = 50\n",
      "work_dir = './childact-checkpoints/CSN-age-jog'\n",
      "find_unused_parameters = True\n",
      "omnisource = False\n",
      "seed = 42\n",
      "gpu_ids = range(0, 1)\n",
      "output_config = dict(out='./childact-checkpoints/CSN-age-jog/results.json')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mmcv.runner import set_random_seed\n",
    "\n",
    "# Modify dataset type and path\n",
    "# cfg.dataset_type = 'VideoDataset'\n",
    "cfg.data_root = 'age-gender-3split-rgb-frames/'\n",
    "cfg.data_root_val = 'age-gender-3split-rgb-frames/val/'\n",
    "cfg.ann_file_train = 'age-gender-3split-rgb-frames/childact_train_rgb320_age_jog.txt'\n",
    "cfg.ann_file_val = 'age-gender-3split-rgb-frames/childact_val_rgb320_age_jog.txt'\n",
    "cfg.ann_file_test = 'age-gender-3split-rgb-frames/childact_test_rgb320_age_jog.txt'\n",
    "\n",
    "# cfg.data.test.type = 'VideoDataset'\n",
    "cfg.data.test.ann_file = 'age-gender-3split-rgb-frames/childact_test_rgb320_age_jog.txt'\n",
    "cfg.data.test.data_prefix = 'age-gender-3split-rgb-frames/test/'\n",
    "\n",
    "# cfg.data.train.type = 'VideoDataset'\n",
    "cfg.data.train.ann_file = 'age-gender-3split-rgb-frames/childact_train_rgb320_age_jog.txt'\n",
    "cfg.data.train.data_prefix = 'age-gender-3split-rgb-frames/train/'\n",
    "\n",
    "# cfg.data.val.type = 'VideoDataset'\n",
    "cfg.data.val.ann_file = 'age-gender-3split-rgb-frames/childact_val_rgb320_age_jog.txt'\n",
    "cfg.data.val.data_prefix = 'age-gender-3split-rgb-frames/val/'\n",
    "\n",
    "# cfg.data.test.modality = 'Flow'\n",
    "# cfg.data.val.modality = 'Flow'\n",
    "# cfg.data.train.modality = 'Flow'\n",
    "\n",
    "# cfg.data.train.start_index = 0\n",
    "# cfg.data.test.start_index = 0\n",
    "# cfg.data.val.start_index = 0\n",
    "\n",
    "cfg.data.test.filename_tmpl = '{:03}.jpeg'\n",
    "cfg.data.train.filename_tmpl = '{:03}.jpeg'\n",
    "cfg.data.val.filename_tmpl = '{:03}.jpeg'\n",
    "# The flag is used to determine whether it is omnisource training\n",
    "cfg.setdefault('omnisource', False)\n",
    "# Modify num classes of the model in cls_head\n",
    "cfg.model.cls_head.num_classes = 6\n",
    "# We can use the pre-trained TSN model\n",
    "cfg.load_from = 'checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth'\n",
    "# cfg.resume_from = './childact-mm/latest.pth'\n",
    "\n",
    "# Set up working dir to save files and logs.\n",
    "cfg.work_dir = './childact-checkpoints/CSN-age-jog'\n",
    "\n",
    "cfg.total_epochs = 50\n",
    "\n",
    "# cfg.momentum_config = dict(\n",
    "#     policy='cyclic',\n",
    "#     target_ratio=(0.85 / 0.95, 1),\n",
    "#     cyclic_times=1,\n",
    "#     step_ratio_up=0.4,\n",
    "# )\n",
    "\n",
    "# We can set the checkpoint saving interval to reduce the storage cost\n",
    "cfg.checkpoint_config.interval = 20\n",
    "# We can set the log print interval to reduce the the times of printing log\n",
    "cfg.log_config.interval = 100\n",
    "\n",
    "# Set seed thus the results are more reproducible\n",
    "cfg.seed = 42\n",
    "set_random_seed(42, deterministic=False)\n",
    "cfg.gpu_ids = range(1)\n",
    "\n",
    "cfg.data.videos_per_gpu=4\n",
    "\n",
    "# cfg.model.backbone.in_channels = 2\n",
    "\n",
    "cfg.output_config = dict(out=f'{cfg.work_dir}/results.json')\n",
    "# We can initialize the logger for training and have a look\n",
    "# at the final config used for training\n",
    "# del cfg.optimizer['momentum']\n",
    "print(f'Config:\\n{cfg.pretty_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ba29a54-96ed-4dee-a29e-8fc6715f5dbd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "dDBWkdDRk6oz",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "85a52ef3-7b5c-4c52-8fef-00322a8c65e6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 16:16:38,766 - mmaction - INFO - load model from: https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth\n",
      "2021-08-10 16:16:38,767 - mmaction - INFO - Use load_from_http loader\n",
      "2021-08-10 16:16:39,003 - mmaction - INFO - load checkpoint from checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth\n",
      "2021-08-10 16:16:39,003 - mmaction - INFO - Use load_from_local loader\n",
      "2021-08-10 16:16:39,149 - mmaction - WARNING - The model and loaded state dict do not match exactly\n",
      "\n",
      "size mismatch for cls_head.fc_cls.weight: copying a param with shape torch.Size([400, 2048]) from checkpoint, the shape in current model is torch.Size([6, 2048]).\n",
      "size mismatch for cls_head.fc_cls.bias: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([6]).\n",
      "2021-08-10 16:16:39,150 - mmaction - INFO - Start running, host: robt427nv@robt427NV, work_dir: /home/robt427nv/childact/childact-checkpoints/CSN-age-jog\n",
      "2021-08-10 16:16:39,151 - mmaction - INFO - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(ABOVE_NORMAL) OptimizerHook                      \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "2021-08-10 16:16:39,151 - mmaction - INFO - workflow: [('train', 1)], max: 50 epochs\n",
      "/home/robt427nv/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/mmcv/runner/hooks/evaluation.py:190: UserWarning: runner.meta is None. Creating an empty one.\n",
      "  warnings.warn('runner.meta is None. Creating an empty one.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 15/15, 9.2 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 16:19:18,896 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 16:19:18,898 - mmaction - INFO - \n",
      "top1_acc\t0.7333\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 16:19:18,899 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 16:19:18,900 - mmaction - INFO - \n",
      "mean_acc\t0.3750\n",
      "2021-08-10 16:19:19,209 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_5.pth.\n",
      "2021-08-10 16:19:19,210 - mmaction - INFO - Best top1_acc is 0.7333 at 5 epoch.\n",
      "2021-08-10 16:19:19,210 - mmaction - INFO - Epoch(val) [5][4]\ttop1_acc: 0.7333, top5_acc: 1.0000, mean_class_accuracy: 0.3750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 15/15, 9.7 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 16:22:00,722 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 16:22:00,724 - mmaction - INFO - \n",
      "top1_acc\t0.6000\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 16:22:00,724 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 16:22:00,726 - mmaction - INFO - \n",
      "mean_acc\t0.2250\n",
      "2021-08-10 16:22:00,726 - mmaction - INFO - Epoch(val) [10][4]\ttop1_acc: 0.6000, top5_acc: 1.0000, mean_class_accuracy: 0.2250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 15/15, 9.5 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 16:24:39,911 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 16:24:39,914 - mmaction - INFO - \n",
      "top1_acc\t0.4000\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 16:24:39,914 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 16:24:39,917 - mmaction - INFO - \n",
      "mean_acc\t0.2500\n",
      "2021-08-10 16:24:39,918 - mmaction - INFO - Epoch(val) [15][4]\ttop1_acc: 0.4000, top5_acc: 1.0000, mean_class_accuracy: 0.2500\n",
      "2021-08-10 16:27:12,426 - mmaction - INFO - Saving checkpoint at 20 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 15/15, 9.5 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 16:27:14,392 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 16:27:14,394 - mmaction - INFO - \n",
      "top1_acc\t0.5333\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 16:27:14,394 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 16:27:14,396 - mmaction - INFO - \n",
      "mean_acc\t0.3000\n",
      "2021-08-10 16:27:14,396 - mmaction - INFO - Epoch(val) [20][4]\ttop1_acc: 0.5333, top5_acc: 1.0000, mean_class_accuracy: 0.3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 15/15, 9.8 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 16:29:49,037 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 16:29:49,039 - mmaction - INFO - \n",
      "top1_acc\t0.6000\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 16:29:49,039 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 16:29:49,040 - mmaction - INFO - \n",
      "mean_acc\t0.3250\n",
      "2021-08-10 16:29:49,041 - mmaction - INFO - Epoch(val) [25][4]\ttop1_acc: 0.6000, top5_acc: 1.0000, mean_class_accuracy: 0.3250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 15/15, 9.0 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 16:32:24,040 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 16:32:24,085 - mmaction - INFO - \n",
      "top1_acc\t0.2667\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 16:32:24,103 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 16:32:24,196 - mmaction - INFO - \n",
      "mean_acc\t0.4250\n",
      "2021-08-10 16:32:24,237 - mmaction - INFO - Epoch(val) [30][4]\ttop1_acc: 0.2667, top5_acc: 1.0000, mean_class_accuracy: 0.4250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 15/15, 9.6 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 16:34:59,790 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 16:34:59,792 - mmaction - INFO - \n",
      "top1_acc\t0.4667\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 16:34:59,793 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 16:34:59,795 - mmaction - INFO - \n",
      "mean_acc\t0.2750\n",
      "2021-08-10 16:34:59,796 - mmaction - INFO - Epoch(val) [35][4]\ttop1_acc: 0.4667, top5_acc: 1.0000, mean_class_accuracy: 0.2750\n",
      "2021-08-10 16:37:32,558 - mmaction - INFO - Saving checkpoint at 40 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 15/15, 9.2 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 16:37:34,605 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 16:37:34,606 - mmaction - INFO - \n",
      "top1_acc\t0.4667\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 16:37:34,607 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 16:37:34,608 - mmaction - INFO - \n",
      "mean_acc\t0.2750\n",
      "2021-08-10 16:37:34,609 - mmaction - INFO - Epoch(val) [40][4]\ttop1_acc: 0.4667, top5_acc: 1.0000, mean_class_accuracy: 0.2750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 15/15, 9.5 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 16:40:09,047 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 16:40:09,049 - mmaction - INFO - \n",
      "top1_acc\t0.4667\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 16:40:09,049 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 16:40:09,050 - mmaction - INFO - \n",
      "mean_acc\t0.2750\n",
      "2021-08-10 16:40:09,051 - mmaction - INFO - Epoch(val) [45][4]\ttop1_acc: 0.4667, top5_acc: 1.0000, mean_class_accuracy: 0.2750\n",
      "2021-08-10 16:42:41,388 - mmaction - INFO - Saving checkpoint at 50 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 15/15, 9.9 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 16:42:43,314 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-10 16:42:43,316 - mmaction - INFO - \n",
      "top1_acc\t0.5333\n",
      "top5_acc\t1.0000\n",
      "2021-08-10 16:42:43,317 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-10 16:42:43,319 - mmaction - INFO - \n",
      "mean_acc\t0.5250\n",
      "2021-08-10 16:42:43,320 - mmaction - INFO - Epoch(val) [50][4]\ttop1_acc: 0.5333, top5_acc: 1.0000, mean_class_accuracy: 0.5250\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "from mmaction.datasets import build_dataset\n",
    "from mmaction.models import build_model\n",
    "from mmaction.apis import train_model\n",
    "\n",
    "import mmcv\n",
    "\n",
    "# Build the dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# Build the recognizer\n",
    "model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.train_cfg, test_cfg=cfg.test_cfg)\n",
    "# CUDA_LAUNCH_BLOCKING=1\n",
    "# Create work_dir\n",
    "mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
    "train_model(model, datasets, cfg, distributed=False, validate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a41036b7-3d48-4055-b643-c3660bf5763e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f\"{cfg.work_dir}/model50e\", 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc25eda3-6a08-41d9-908c-8b016de5c947",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 16:54:18,016 - mmaction - INFO - load model from: https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth\n",
      "2021-08-10 16:54:18,017 - mmaction - INFO - Use load_from_http loader\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "from mmaction.datasets import build_dataset\n",
    "from mmaction.models import build_model\n",
    "from mmaction.apis import train_model\n",
    "import pickle\n",
    "import mmcv\n",
    "# Build the dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# Build the recognizer\n",
    "model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "\n",
    "model = pickle.load(open(f\"{cfg.work_dir}/model50e\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d24a99fd-5a76-492b-98b9-8ee6427e00b9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eyY3hCMwyTct",
    "outputId": "200e37c7-0da4-421f-98da-41418c3110ca",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 10/10, 0.6 task/s, elapsed: 17s, ETA:     0s\n",
      "Evaluating top_k_accuracy ...\n",
      "\n",
      "top1_acc\t0.4000\n",
      "top5_acc\t1.0000\n",
      "\n",
      "Evaluating mean_class_accuracy ...\n",
      "\n",
      "mean_acc\t0.5000\n",
      "top1_acc: 0.4000\n",
      "top5_acc: 1.0000\n",
      "mean_class_accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "from mmaction.apis import single_gpu_test\n",
    "from mmaction.datasets import build_dataloader\n",
    "from mmcv.parallel import MMDataParallel\n",
    "# from mmaction.models import build_model\n",
    "# from mmaction.datasets import build_dataset\n",
    "\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# Build a test dataloader\n",
    "dataset = build_dataset(cfg.data.test, dict(test_mode=True))\n",
    "data_loader = build_dataloader(\n",
    "        dataset,\n",
    "        videos_per_gpu=1,\n",
    "        workers_per_gpu=1,\n",
    "        dist=False,\n",
    "        shuffle=False)\n",
    "model = MMDataParallel(model, device_ids=[0])\n",
    "outputs = single_gpu_test(model, data_loader)\n",
    "\n",
    "eval_config = cfg.evaluation\n",
    "eval_config.pop('interval')\n",
    "eval_res = dataset.evaluate(outputs, **eval_config)\n",
    "for name, val in eval_res.items():\n",
    "    print(f'{name}: {val:.04f}')\n",
    "    \n",
    "output_config = cfg.output_config\n",
    "dataset.dump_results(outputs, **output_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd2bb12b-8dba-4586-a0f3-b0b61985ba00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAD8CAYAAADUv3dIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAcF0lEQVR4nO3de5QdZZ3u8e/TSRMMjEHJ9pA0kTSCUcbBGALGZMZhJTCSxIFZXhaJAZMsPRldKIkIHi9znAVnzdEog2QMwulJojFcghLF4EJHtPEIeGgN4U6CE0KQhDbpcBVQTLp/549d0Hva7n2hd1ftrn4+a9ViV+3aVb9+qX76zbvroojAzMzS0ZR1AWZmI4lD18wsRQ5dM7MUOXTNzFLk0DUzS5FD18wsRQ5dM7MyJI2SdLekH/bz3hhJ10vaIalD0uRK23PompmVtxzYNsB7HwGejojjgK8BKyttzKFrZjYASUcD84E1A6xyFrA+eX0DMEeSym1zdP3K69+mTTf6kjf7MzNnzsi6BGtAEyYcVTawqiGplsz5R2BZyXxbRLSVzF8OfAb4iwE+3wI8DhARByU9CxwJ7B9oh0MeumZmjSoJ2Lb+3pP0XmBfRNwl6dR67dPDC2aWK5KqniqYBZwpaRewEZgt6eo+6+wBJiX7HQ2MA54st1GHrpnlSr1CNyI+FxFHR8RkYAHQHhHn9FltM7A4ef2BZJ2ywxseXjCzXKmiBzvY7V8CbImIzcBaYIOkHcBTFMO5LIeumeVKU1P9/wEfET8Hfp68/mLJ8j8CH6xlWw5dM8uVoe7pDpZD18xyxaFrZpYih66ZWYoaPXR9ypiZWYrc0zWzXGlqGpV1CWU5dM0sVxp9eMGha2a54tA1M0uRQ9fMLEUOXTOzFA3FZcD15NA1s1xp9J5uY/9JMDPLGfd0zSxXGr2n69A1s1xx6JqZpciha2aWIp+9kLFNm77L9u3bOOyww1mx4oKsy8mU26JXR0cHq1d/ne7uHubPn8+iRYuyLikzeWuLRu/pNvafhDqYNu0kliz5SNZlNAS3RVF3dzerVl3OypVfYf369bS3/4xdu3ZlXVYm8tgW9XowpaRDJf1K0r2SHpR0cT/rLJHUJemeZPpopfpyH7qtrccyduxrsi6jIbgtirZv30ZLSwsTJ06kubmZ2bNnc8cdt2ddViby2BZ1fAT7S8DsiHg7MBU4Q9KMfta7PiKmJtOaShvNfeia9dXVtZ9C4Q2vzBcKBbq69mdYUXby2BZ1fAR7RMTzyWxzMpV9vHo1qgpdSc2Szpd0QzJ9UlLzYHduZlZvdezpImmUpHuAfcAtEdHRz2rvl3Rfko2TKm2z2p7ulcBJwDeSaVqybKBCl0naImnLLbf8pMpdmKWjUBhPV9e+V+a7urooFMZnWFF28tgWtYRuaVYl07LSbUVEd0RMBY4GTpH0tj67uwmYHBEnArcA6yvVV23onhwRiyOiPZmWAicPtHJEtEXE9IiYfvrpf1flLszSMWXKW9i9ezednZ0cOHCA9vZ2Zs6clXVZmchjW9QSuqVZlUxt/W0zIp4BbgXO6LP8yYh4KZldQ7FzWla1p4x1S3pTRDyS/FDHAt1VfjZTGzdey6OP7uSFF17gy1/+F0477XSmTz8l67Iy4bYoGj16NMuXr+Ciiy6kp6eHuXPn0dramnVZmchjW9TrlDFJBeBARDwj6TXA6cDKPutMiIjOZPZMYFvF7UZUHheWNAf4JrATEHAMsDQibq302U2bbhz0wLPlz8yZ/X0JbCPdhAlHDToxW1uPrTpzHn1054D7k3QixeGCURRHBb4TEZdIugTYEhGbJX2JYtgeBJ4CPh4R28vts6qebkT8TNLxwJRk0cMlXWozs4ZRr55uRNwHvKOf5V8sef054HO1bLeq0JV0F7AWuC4inq5lB2ZmaWr0y4Crre5soAX4taSNkt6jRr/WzsxGJKmp6ikLVe01InZExBeANwPXAuuAxyRdLOn1Q1mgmVkt6nme7lCoOuqTQeXLgK8Cm4APAs8B7UNTmplZ/tQypvsMxfPQPhMRf0re6pA0vE/qM7NcafSRz7I9XUnvlPRair3avwfeCmyStFLSOICIeN/Ql2lmVp3hPrywDngxInYClwOvpXhy8IsUz9s1M2soTU1NVU9ZqDS80BQRB5PX0yNiWvL69uQmEGZmDWVYDy8AD0hamry+V9J0AElvBg4MaWVmZq/CcB9e+Cjwt5IeAU4A/p+kncC/J++ZmTWURg/dssMLEfEssCT5Mq01WX93ROxNozgzs1o1+vBCtfdeeA64d4hrMTMbtFyErpnZcOHQNTNLkUPXzCxFDl0zsxQ5dM3MUuTQNTNLUaPfxNyha2a50ug93cb+k2BmVqN6XZEm6VBJv5J0r6QHJV3czzpjJF0vaYekDkmTK9Xn0DWzXKnjZcAvAbMj4u3AVOAMSX0fY/0R4OmIOA74Gn0e0d4fh66ZWT+i6PlktjmZ+j7e/SyKj2kHuAGYU+n5kUM+pjtzZt8/DCPXpZdelnUJDcPHhQ2VWsZ0JS0DlpUsaouItpL3RwF3AccBV0RER59NtACPA0TEQUnPAkcC+wfap79IM7NcqeXshSRg28q83w1MlXQE8H1Jb4uIBwZV32A+bGbWaIbi1o4R8QxwK3BGn7f2AJOS/Y4GxgFPltuWQ9fMcqWOZy8Ukh4ukl4DnA5s77PaZmBx8voDQHtE9B33/S88vGBmuVLH83QnAOuTcd0m4DsR8UNJlwBbImIzsBbYIGkH8BSwoNJGHbpmliv1Ct2IuA94Rz/Lv1jy+o8Un5ZeNYeumeVKo1+R5tA1s1xx6JqZpciha2aWIoeumVmKHLpmZily6JqZpaipaVTWJZTl0DWzXHFP18wsRQ5dM7MUNXro+oY3ZmYpck/XzHKl0Xu6Dl0zyxU/gt3MLEXu6ZqZpciha2aWIoeumVmKHLoZ6+joYPXqr9Pd3cP8+fNZtGhR1iVl4ogjxrFw4QIOP/xwILjzzg5uu+2OrMvKjI+LXnlrC4duhrq7u1m16nIuvfRfKRQKfOxj/8isWbOYPHly1qWlrru7h82bf8iePXsYM2YMn/rU+fzmN//J3r37si4tdT4ueuWxLep19oKkScC3gf8GBNAWEav6rHMq8APg0WTR9yLikrL11aW6BrV9+zZaWlqYOHEizc3NzJ49mzvuuD3rsjLx+9//nj179gDw0ksvsXfvPsaNG5dxVdnwcdHLbVHWQeDTEXECMAM4T9IJ/ax3W0RMTaaygQtVhq6kZknnS7ohmT4pqbm2+tPX1bWfQuENr8wXCgW6uvZnWFFjeN3rXkdLy0Qee+y3WZeSCR8XvfLYFvV6BHtEdEbE1uT174FtQMtg66u2p3slcBLwjWSalizrl6RlkrZI2nL11RsGW6PV0SGHHMLixefygx/cxEsvvZR1OWZ1V0volmZVMi0bYJuTKT4ZuKOft98l6V5JP5L0l5Xqq3ZM9+SIeHvJfLukewdaOSLagDaAzs7fRZX7qLtCYTxdXb1jll1dXRQK47MqJ3NNTU0sWXIuW7fezf33P5B1OZnxcdErj21RyxdppVlVZnuHA5uAFRHxXJ+3twLHRMTzkuYBNwLHl9tetT3dbklvKiniWKC7ys9mZsqUt7B79246Ozs5cOAA7e3tzJw5K+uyMnP22R9k7959/OIXt2VdSqZ8XPTKY1s0NTVVPVWSDKNuAq6JiO/1fT8inouI55PXNwPNksr+1aq2p3sRcKuknYCAY4ClVX42M6NHj2b58hVcdNGF9PT0MHfuPFpbW7MuKxOtrZOZPv0knniikwsuWAHAzTf/mO3bt2dbWAZ8XPTKY1vU65QxFTe0FtgWEZcNsM5RwN6ICEmnUOzIPll2uxHV/etf0hhgSjL7cERUNSCY5fBCo7n00n7/v41IF154QdYlWAOaMOGoQSfmOecsrjpzrr56/YD7k/TXwG3A/UBPsvjzwBsBIuIqSZ8APk7xTIc/ABdExC/L7bOqnq6kuygm/nUR8XQ1nzEzy0K9eroRcTvFf9mXW2c1sLqW7VY7pns2xVMlfi1po6T3qNEv+zCzEalep4wNlapCNyJ2RMQXgDcD1wLrgMckXSzp9UNZoJlZnlR9GbCkEyl+eTaP5Ns84K+BdmDqUBRnZlarXNzEPBnTfYbiuO5nS75E65A0vM8vMbNcafSRz7KhK+l84PvAByNiZ3/rRMT7hqIwM7NXo9FDt1I//H9RvOxtvaSPSyqkUJOZ2as23L9I2wkcTTF8pwMPSfqxpMWS/mLIqzMzq1Gjh26lMd2IiB7gJ8BPkkvi5gILgUsB93zNrKEM9y/S/sufgog4AGwGNksaO2RVmZm9So0+plspdM8e6I2IeLHOtZiZDVqjh27ZfnhE/CatQszMRoJcPyPNzEaeRu/pOnTNLFccumZmKRruZy+YmQ0r7umamaXIoWtmliKHrplZiho9dBt7xNnMrEb1uveCpEmSbpX0kKQHJS3vZx1J+jdJOyTdJ2lapfrc0zWzXKljT/cg8OmI2Jrc4OsuSbdExEMl68wFjk+mdwJXJv8dkHu6ZpYr9erpRkRnRGxNXv8e2EbxWZGlzgK+HUV3AkdImlBuu+7ppuiyy76adQkNw49gt6FSS09X0jJgWcmitoho62e9ycA7KN5fvFQL8HjJ/O5kWedA+3Tomlmu1BK6ScD+Wcj22d7hFJ8LuSIinhtcdQ5dM8uZep69kNxDfBNwTUR8r59V9gCTSuaPTpYNyGO6ZpYrTU1NVU/lqJjea4FtEXHZAKttBj6cnMUwA3g2IgYcWgD3dM0sZ+rY050FnAvcL+meZNnngTcCRMRVwM3APGAH8CKwtNJGHbpmliv1Ct2IuJ0+T8/pZ50Azqtlux5eMDNLkXu6ZpYrjX4ZsEPXzHLFoWtmliLfxNzMLEXu6ZqZpciha2aWIoeumVmKHLpmZily6JqZpciha2aWIoeumVmKHLpmZily6JqZpciha2aWIl8GbGaWIvd0zcxS1Oih29j98Dro6Ojg3HPP4UMf+hDXXHNN1uVkrqmpia1bt3LTTTdlXUqmfFz0yltbSKp6ykKuQ7e7u5tVqy5n5cqvsH79etrbf8auXbuyLitTy5cvZ9u2bVmXkSkfF73cFuVJWidpn6QHBnj/VEnPSronmb5YaZu5Dt3t27fR0tLCxIkTaW5uZvbs2dxxx+1Zl5WZlpYW5s+fz5o1a7IuJVM+LnrlsS3q3NP9FnBGhXVui4ipyXRJpQ1WHbqSzpR0aTL9fbWfy1JX134KhTe8Ml8oFOjq2p9hRdm6/PLL+cxnPkNPT0/WpWTKx0WvPLZFvR7BDhARvwCeqmt91awk6UvAcuChZDpf0v8us/4ySVskbbn66g31qdQGZf78+ezbt4+tW7dmXYrZkKqlp1uaVcm07FXs8l2S7pX0I0l/WWnlas9emA9MjYie5IdaD9xN8RnwfyYi2oA2gM7O30WV+6i7QmE8XV37Xpnv6uqiUBifVTmZmjVrFmeeeSbz5s3j0EMP5bWvfS0bNmzg3HPPzbq01Pm46JXHtqjlC7LSrHqVtgLHRMTzkuYBNwLHl/tALWO6R5S8HldzaRmYMuUt7N69m87OTg4cOEB7ezszZ87KuqxMfP7zn2fSpEm0trayYMEC2tvbR2Tggo+LUnlsizTPXoiI5yLi+eT1zUCzpLJ/tart6X4JuFvSrYCAdwOfHUyxaRg9ejTLl6/goosupKenh7lz59Ha2pp1WZYxHxe98tgWaZ4KJukoYG9EhKRTKHZknyz7mYjq/vUvaQJwcjL7q4j4XTWfy3J4odFMnDgh6xIaxhNPdGZdgjWgCROOGnRiXnHFVVVnznnnfazs/iRdB5wKjAf2Av8MNANExFWSPgF8HDgI/AG4ICJ+WW6bVfV0JW0C1gI/fHlc18ysEdWzpxsRCyu8vxpYXcs2qx3TvRJYBPynpC9LmlLLTszM0pKLK9Ii4qcRsQiYBuwCfirpl5KWSmoeygLNzGqRi9AFkHQksAT4KMXTxVZRDOFbhqQyM7NXodFDt9ox3e8DU4ANwHtLvkS7XtKWoSrOzKxWw/ouY5IOkfRh4IqIOAH4LfBPks57eVghIqanUKeZWVXqeRnwUKjU0/1mss5YSYuBw4DvA3OAU4DFQ1uemVltGr2nWyl0/yoiTpQ0GtgDTIyIbklXA/cOfXlmZrUZ7qHbJOkQij3csRQv/30KGENygrCZWSMZ7qG7FtgOjAK+AHxX0k5gBrBxiGszM6vZsA7diPiapOuT109I+jZwGvDvEfGrNAo0M8uTiqeMRcQTJa+fAW4YyoLMzAbDj2A3M0vRsB5eMDMbbhy6ZmYpcuiamaXIoWtmliJ/kWZmlqJG7+k29p8EM7OcceiaWa7U8366ktZJ2ifpgQHel6R/k7RD0n2SplXapkPXzHKlzjcx/xZwRpn35wLHJ9Myio82K8uha2a5Us/QjYhfULzJ10DOAr4dRXcCRyRPTh/QkH+R9stf3jnUuxg2/Nhx649/R3q9//3/MOht1HL2gqRlFHuoL2uLiLYadtcCPF4yvztZNuAvu89eMLNcqeXshSRgawnZQXPomlmupHzK2B5gUsn80cmyAXlM18xyJeWnAW8GPpycxTADeDYiyo4juqdrZrlSz56upOuAU4HxknYD/0zy1JyIuAq4GZgH7ABeBJZW2qZD18xsABGxsML7AZxXyzYdumaWK773gplZihr93gsOXTPLFYeumVmKHLpmZily6JqZpchfpJmZpajBO7oOXTPLl0YfXmjsfriZWc64p2tmudLoPV2HrpnlikPXzCxFPnvBzCxF7umamaXIoWtmliKHrplZihy6ZmYpcuhmbNOm77J9+zYOO+xwVqy4IOtyMtXR0cHq1V+nu7uH+fPns2jRoqxLyozboiiPvx+NHrqNfW5FHUybdhJLlnwk6zIy193dzapVl7Ny5VdYv3497e0/Y9euXVmXlQm3Ra88/n7U88GUks6Q9LCkHZI+28/7SyR1SbonmT5aaZu5D93W1mMZO/Y1WZeRue3bt9HS0sLEiRNpbm5m9uzZ3HHH7VmXlQm3Ra88/n7UK3QljQKuAOYCJwALJZ3Qz6rXR8TUZFpTqb7ch64VdXXtp1B4wyvzhUKBrq79GVaUHbdFvtWxp3sKsCMidkbEn4CNwFmDra+q0JXULOl8STck0yclNQ9252Zm9VbH0G0BHi+Z350s6+v9ku5LsnFSpY1W29O9EjgJ+EYyTUuW9UvSMklbJG255ZafVLkLG0qFwni6uva9Mt/V1UWhMD7DirLjtsi3pqamqqfSrEqmZTXu7iZgckScCNwCrK9YX5UbPjkiFkdEezItBU4eaOWIaIuI6REx/fTT/67KXdhQmjLlLezevZvOzk4OHDhAe3s7M2fOyrqsTLgt8q2Wnm5pViVTW8mm9gClPdejk2WviIgnI+KlZHYNxc5pWdWeMtYt6U0R8UjyQx0LdFf52Uxt3Hgtjz66kxdeeIEvf/lfOO2005k+/ZSsy0rd6NGjWb58BRdddCE9PT3MnTuP1tbWrMvKhNuiVx5/P+p4ytivgeMltVIM2wXAh/rsa0JEdCazZwLbKm202tC9CLhV0k5AwDHA0io/m6kFCz5UeaURYsaMGcyYMSPrMhqC26Ioj78f9QrdiDgo6RPAfwCjgHUR8aCkS4AtEbEZOF/SmcBB4ClgSaXtVhW6EfEzSccDU5JFD5d0qc3McikibgZu7rPsiyWvPwd8rpZtVhW6ku4C1gLXRcTTtezAzCxNebki7WyKp0r8WtJGSe9Ro/9kZjYi1XL2Qib1VbNSROyIiC8AbwauBdYBj0m6WNLrh7JAM7Na1PMy4KFQddRLOhG4DPgqsAn4IPAc0D40pZmZ1a7RQ7eWMd1nKJ6H9j9KvkTrkOQTHM2sYTT6yGfF0E3Oyb2e4onBJwPjJF0bEc8BRMT7hrZEM7PqNXrolh1ekHQ+cBVwCDAdGEPxCo07JZ061MWZmdVquA8v/HdgakR0S7oMuDkiTpX0f4AfAO8Y8grNzGrQ6D3dasZ0R1O85HcMcDhARPzWdxkzs0Y03EN3DcVzczuAvwFWAkgqULzkzcysoQzr0I2IVZJ+CrwV+NeI2J4s7wLenUJ9ZmY1GdahCxARDwIPplCLmdmgDfvQNTMbTpqaHLpmZqlxT9fMLEUOXTOzFDV66PoR7GZmKXJP18xypdF7ug5dM8uVrG5OXq3Grs7MrEb1vOGNpDMkPSxph6TP9vP+GEnXJ+93SJpcaZsOXTPLlXqFrqRRwBXAXOAEYKGkE/qs9hHg6Yg4Dvgaya0SynHomlmu1LGnewqwIyJ2RsSfgI3AWX3WOQtYn7y+AZhT6fmRiohX8WMNP5KWRURb1nU0ArdFL7dFr5HYFpKWActKFrW93AaSPgCcEREfTebPBd4ZEZ8o+fwDyTq7k/lHknX2D7TPkdTTXVZ5lRHDbdHLbdFrxLVFRLRFxPSSacj/6Iyk0DUzq8Ueik/KednRybJ+15E0GhgHPFluow5dM7P+/Ro4XlKrpEOABcDmPutsBhYnrz8AtEeFMduRdJ7uiBqrqsBt0ctt0cttUSIiDkr6BPAfwChgXUQ8KOkSYEtEbAbWAhsk7aD4YIcFlbY7Yr5IMzNrBB5eMDNLkUPXzCxFuQldSUdIukHSdknbJL0r65qyIGmKpHtKpuckrci6rqxI+pSkByU9IOk6SYdmXVNWJC1P2uHBkXxMZC03Y7qS1gO3RcSa5JvGsRHxTMZlZSq5jHEPxZO1H8u6nrRJagFuB06IiD9I+g5wc0R8K9vK0ifpbRSvqDoF+BPwY+BjEbEj08JGoFz0dCWNo/h04rUAySV7R0raWrLO8S/PS5oj6W5J90taJ2lMJoUPvTnAI8DoEdwWo4HXJOdQjgWekHTjy29KOl3S95PXC5N2eEBSxWvoh5m3Ah0R8WJEHAT+L/D+EXxcZCYXoQu0Al3AN5MDZQ3wO+BZSVOTdZYm7x8KfAs4OyL+iuIv5cfTLzkVC4DrIuIRRmBbRMQe4FLgt0An8CxwC/AWSYVktaXAOkkTKd6sZDYwFThZ0j+kXfMQegD4G0lHShoLzKN4sv+IOy6ylpfQHQ1MA66MiHcALwCfBdYAS5N/Zp8NXAtMAR6NiN8kn11PsZecK8kQy5nAd5NFI64tJL2O4g1JWoGJwGHAImADcI6kI4B3AT8CTgZ+HhFdSU/wGnLUFhGxjeIflZ9QHFq4B+hmBB4XWctL6O4GdkdERzJ/A8UQ3kTxtmzvBe6KiLKX5+XMXGBrROxN5kdiW5xGMTy6IuIA8D1gJvBN4BxgIfDdJGRzLyLWRsRJEfFu4GngN4zM4yJTuQjdiPgd8LikKcmiOcBDEfFHileTXEnxFw3gYWCypOOS+XMpjm/lzULgupdnRmhb/BaYIWlscru9OcC2iHgCeAL4J3rb4lfA30oan/T6FpKvtkDSG5L/vhF4H3DtCD0ushURuZgojsNtAe4DbgRelyyfQbEnPKpk3TnA3cD9wDpgTNb117ktDqN4041xfZaPxLa4GNhOcUxzw8s/H8Xx7jv7rLswaYcHgJVZ1z4EbXEb8BBwLzBnJB8XWU65OWVsIJIupBg+/zPrWrLmtuglaTVwd0SszbqWrPm4SFeub3iTnAr0JorfSI9oboteku6i+GXrp7OuJWs+LtKX+56umVkjycUXaWZmw4VD18wsRQ5dM7MUOXTNzFLk0DUzS9H/B++BOrHS/A9DAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from mmaction.core import confusion_matrix \n",
    "\n",
    "gt_labels = [ann['label'] for ann in dataset.load_annotations()]\n",
    "pred = np.argmax(outputs, axis=1)\n",
    "cf_mat = confusion_matrix(pred, gt_labels).astype(float)\n",
    "cmap = sns.cubehelix_palette(50, hue=0.05, rot=0, light=0.9, dark=0, as_cmap=True)\n",
    "# /np.sum(cf_mat), fmt='.2%',\n",
    "sns.heatmap(cf_mat, cmap=cmap, annot=True, xticklabels = ['6yo', '7yo', '8yo', '9yo'], yticklabels = ['6yo', '7yo', '8yo', '9yo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7230d4f-ba5e-4c7f-ac0a-cbf11a679a8f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n"
     ]
    }
   ],
   "source": [
    "mean_error = (1+4+3)/10 #number of all tested videos\n",
    "print(mean_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef536a9f-1d69-40dc-aa45-66373942e6aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed0ef921-f91a-4eb4-8434-23c61aa04646",
   "metadata": {},
   "source": [
    "# CSN age wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1df6aaf-c0ef-4d6d-8db0-6ff71344c370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/robt427nv/childact\n"
     ]
    }
   ],
   "source": [
    "cd /home/robt427nv/childact/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d8ca5e9-46c8-4fe5-a41e-d22359ef9dd3",
   "metadata": {
    "id": "LjCcmCKOjktc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mmcv import Config\n",
    "cfg = Config.fromfile('mmaction2/configs/recognition/csn/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c82d6fcb-4f24-4912-b0b8-d1fe792a94cf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "tlhu9byjjt-K",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "13d1bb01-f351-48c0-9efb-0bdf2c125d29",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "model = dict(\n",
      "    type='Recognizer3D',\n",
      "    backbone=dict(\n",
      "        type='ResNet3dCSN',\n",
      "        pretrained2d=False,\n",
      "        pretrained=\n",
      "        'https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth',\n",
      "        depth=152,\n",
      "        with_pool2=False,\n",
      "        bottleneck_mode='ir',\n",
      "        norm_eval=True,\n",
      "        zero_init_residual=False,\n",
      "        bn_frozen=True),\n",
      "    cls_head=dict(\n",
      "        type='I3DHead',\n",
      "        num_classes=6,\n",
      "        in_channels=2048,\n",
      "        spatial_type='avg',\n",
      "        dropout_ratio=0.5,\n",
      "        init_std=0.01),\n",
      "    train_cfg=None,\n",
      "    test_cfg=dict(average_clips='prob'))\n",
      "checkpoint_config = dict(interval=20)\n",
      "log_config = dict(interval=100, hooks=[dict(type='TextLoggerHook')])\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = 'checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth'\n",
      "resume_from = None\n",
      "workflow = [('train', 1)]\n",
      "dataset_type = 'RawframeDataset'\n",
      "data_root = 'age-gender-3split-rgb-frames/'\n",
      "data_root_val = 'age-gender-3split-rgb-frames/val/'\n",
      "ann_file_train = 'age-gender-3split-rgb-frames/childact_train_rgb320_age_wave.txt'\n",
      "ann_file_val = 'age-gender-3split-rgb-frames/childact_val_rgb320_age_wave.txt'\n",
      "ann_file_test = 'age-gender-3split-rgb-frames/childact_test_rgb320_age_wave.txt'\n",
      "img_norm_cfg = dict(\n",
      "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_bgr=False)\n",
      "train_pipeline = [\n",
      "    dict(type='SampleFrames', clip_len=32, frame_interval=2, num_clips=1),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='RandomResizedCrop'),\n",
      "    dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "    dict(type='Flip', flip_ratio=0.5),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "]\n",
      "val_pipeline = [\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=32,\n",
      "        frame_interval=2,\n",
      "        num_clips=1,\n",
      "        test_mode=True),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='CenterCrop', crop_size=224),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=32,\n",
      "        frame_interval=2,\n",
      "        num_clips=10,\n",
      "        test_mode=True),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='ThreeCrop', crop_size=256),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "data = dict(\n",
      "    videos_per_gpu=6,\n",
      "    workers_per_gpu=4,\n",
      "    train=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_train_rgb320_age_wave.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/train/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=1),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='RandomResizedCrop'),\n",
      "            dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "            dict(type='Flip', flip_ratio=0.5),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'),\n",
      "    val=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_val_rgb320_age_wave.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/val/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=1,\n",
      "                test_mode=True),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='CenterCrop', crop_size=224),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'),\n",
      "    test=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_test_rgb320_age_wave.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/test/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=10,\n",
      "                test_mode=True),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='ThreeCrop', crop_size=256),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'))\n",
      "evaluation = dict(\n",
      "    interval=5, metrics=['top_k_accuracy', 'mean_class_accuracy'])\n",
      "optimizer = dict(type='SGD', lr=0.000125, momentum=0.9, weight_decay=0.0001)\n",
      "optimizer_config = dict(grad_clip=dict(max_norm=40, norm_type=2))\n",
      "lr_config = dict(\n",
      "    policy='step',\n",
      "    step=[32, 48],\n",
      "    warmup='linear',\n",
      "    warmup_ratio=0.1,\n",
      "    warmup_by_epoch=True,\n",
      "    warmup_iters=16)\n",
      "total_epochs = 50\n",
      "work_dir = './childact-checkpoints/CSN-age-wave'\n",
      "find_unused_parameters = True\n",
      "omnisource = False\n",
      "seed = 42\n",
      "gpu_ids = range(0, 1)\n",
      "output_config = dict(out='./childact-checkpoints/CSN-age-wave/results.json')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mmcv.runner import set_random_seed\n",
    "\n",
    "# Modify dataset type and path\n",
    "# cfg.dataset_type = 'VideoDataset'\n",
    "cfg.data_root = 'age-gender-3split-rgb-frames/'\n",
    "cfg.data_root_val = 'age-gender-3split-rgb-frames/val/'\n",
    "cfg.ann_file_train = 'age-gender-3split-rgb-frames/childact_train_rgb320_age_wave.txt'\n",
    "cfg.ann_file_val = 'age-gender-3split-rgb-frames/childact_val_rgb320_age_wave.txt'\n",
    "cfg.ann_file_test = 'age-gender-3split-rgb-frames/childact_test_rgb320_age_wave.txt'\n",
    "\n",
    "# cfg.data.test.type = 'VideoDataset'\n",
    "cfg.data.test.ann_file = 'age-gender-3split-rgb-frames/childact_test_rgb320_age_wave.txt'\n",
    "cfg.data.test.data_prefix = 'age-gender-3split-rgb-frames/test/'\n",
    "\n",
    "# cfg.data.train.type = 'VideoDataset'\n",
    "cfg.data.train.ann_file = 'age-gender-3split-rgb-frames/childact_train_rgb320_age_wave.txt'\n",
    "cfg.data.train.data_prefix = 'age-gender-3split-rgb-frames/train/'\n",
    "\n",
    "# cfg.data.val.type = 'VideoDataset'\n",
    "cfg.data.val.ann_file = 'age-gender-3split-rgb-frames/childact_val_rgb320_age_wave.txt'\n",
    "cfg.data.val.data_prefix = 'age-gender-3split-rgb-frames/val/'\n",
    "\n",
    "# cfg.data.test.modality = 'Flow'\n",
    "# cfg.data.val.modality = 'Flow'\n",
    "# cfg.data.train.modality = 'Flow'\n",
    "\n",
    "# cfg.data.train.start_index = 0\n",
    "# cfg.data.test.start_index = 0\n",
    "# cfg.data.val.start_index = 0\n",
    "\n",
    "cfg.data.test.filename_tmpl = '{:03}.jpeg'\n",
    "cfg.data.train.filename_tmpl = '{:03}.jpeg'\n",
    "cfg.data.val.filename_tmpl = '{:03}.jpeg'\n",
    "# The flag is used to determine whether it is omnisource training\n",
    "cfg.setdefault('omnisource', False)\n",
    "# Modify num classes of the model in cls_head\n",
    "cfg.model.cls_head.num_classes = 6\n",
    "# We can use the pre-trained TSN model\n",
    "cfg.load_from = 'checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth'\n",
    "# cfg.resume_from = './childact-mm/latest.pth'\n",
    "\n",
    "# Set up working dir to save files and logs.\n",
    "cfg.work_dir = './childact-checkpoints/CSN-age-wave'\n",
    "\n",
    "cfg.total_epochs = 50\n",
    "\n",
    "# cfg.momentum_config = dict(\n",
    "#     policy='cyclic',\n",
    "#     target_ratio=(0.85 / 0.95, 1),\n",
    "#     cyclic_times=1,\n",
    "#     step_ratio_up=0.4,\n",
    "# )\n",
    "\n",
    "# We can set the checkpoint saving interval to reduce the storage cost\n",
    "cfg.checkpoint_config.interval = 20\n",
    "# We can set the log print interval to reduce the the times of printing log\n",
    "cfg.log_config.interval = 100\n",
    "\n",
    "# Set seed thus the results are more reproducible\n",
    "cfg.seed = 42\n",
    "set_random_seed(42, deterministic=False)\n",
    "cfg.gpu_ids = range(1)\n",
    "\n",
    "cfg.data.videos_per_gpu=6\n",
    "\n",
    "# cfg.model.backbone.in_channels = 2\n",
    "\n",
    "cfg.output_config = dict(out=f'{cfg.work_dir}/results.json')\n",
    "# We can initialize the logger for training and have a look\n",
    "# at the final config used for training\n",
    "# del cfg.optimizer['momentum']\n",
    "print(f'Config:\\n{cfg.pretty_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "241f22d5-8cb5-40d8-a59d-739ab6886dd8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "dDBWkdDRk6oz",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "85a52ef3-7b5c-4c52-8fef-00322a8c65e6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-23 17:04:58,879 - mmaction - INFO - load model from: https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth\n",
      "2021-08-23 17:04:58,880 - mmaction - INFO - Use load_from_http loader\n",
      "2021-08-23 17:05:00,934 - mmaction - INFO - load checkpoint from checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth\n",
      "2021-08-23 17:05:00,935 - mmaction - INFO - Use load_from_local loader\n",
      "2021-08-23 17:05:01,112 - mmaction - WARNING - The model and loaded state dict do not match exactly\n",
      "\n",
      "size mismatch for cls_head.fc_cls.weight: copying a param with shape torch.Size([400, 2048]) from checkpoint, the shape in current model is torch.Size([6, 2048]).\n",
      "size mismatch for cls_head.fc_cls.bias: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([6]).\n",
      "2021-08-23 17:05:01,121 - mmaction - INFO - Start running, host: robt427nv@robt427NV, work_dir: /home/robt427nv/childact/childact-checkpoints/CSN-age-wave\n",
      "2021-08-23 17:05:01,122 - mmaction - INFO - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(ABOVE_NORMAL) OptimizerHook                      \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "2021-08-23 17:05:01,122 - mmaction - INFO - workflow: [('train', 1)], max: 50 epochs\n",
      "/home/robt427nv/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/mmcv/runner/hooks/evaluation.py:190: UserWarning: runner.meta is None. Creating an empty one.\n",
      "  warnings.warn('runner.meta is None. Creating an empty one.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 17/17, 10.2 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-23 17:07:02,674 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-23 17:07:02,675 - mmaction - INFO - \n",
      "top1_acc\t0.4118\n",
      "top5_acc\t0.9412\n",
      "2021-08-23 17:07:02,676 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-23 17:07:02,677 - mmaction - INFO - \n",
      "mean_acc\t0.2000\n",
      "2021-08-23 17:07:03,002 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_5.pth.\n",
      "2021-08-23 17:07:03,003 - mmaction - INFO - Best top1_acc is 0.4118 at 5 epoch.\n",
      "2021-08-23 17:07:03,004 - mmaction - INFO - Epoch(val) [5][3]\ttop1_acc: 0.4118, top5_acc: 0.9412, mean_class_accuracy: 0.2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 17/17, 9.9 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-23 17:09:04,382 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-23 17:09:04,384 - mmaction - INFO - \n",
      "top1_acc\t0.2941\n",
      "top5_acc\t0.9412\n",
      "2021-08-23 17:09:04,385 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-23 17:09:04,387 - mmaction - INFO - \n",
      "mean_acc\t0.1556\n",
      "2021-08-23 17:09:04,388 - mmaction - INFO - Epoch(val) [10][3]\ttop1_acc: 0.2941, top5_acc: 0.9412, mean_class_accuracy: 0.1556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 17/17, 10.0 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-23 17:11:06,176 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-23 17:11:06,178 - mmaction - INFO - \n",
      "top1_acc\t0.3529\n",
      "top5_acc\t1.0000\n",
      "2021-08-23 17:11:06,178 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-23 17:11:06,180 - mmaction - INFO - \n",
      "mean_acc\t0.3000\n",
      "2021-08-23 17:11:06,180 - mmaction - INFO - Epoch(val) [15][3]\ttop1_acc: 0.3529, top5_acc: 1.0000, mean_class_accuracy: 0.3000\n",
      "2021-08-23 17:13:06,673 - mmaction - INFO - Saving checkpoint at 20 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 17/17, 10.0 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-23 17:13:08,833 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-23 17:13:08,835 - mmaction - INFO - \n",
      "top1_acc\t0.2941\n",
      "top5_acc\t1.0000\n",
      "2021-08-23 17:13:08,835 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-23 17:13:08,838 - mmaction - INFO - \n",
      "mean_acc\t0.2111\n",
      "2021-08-23 17:13:08,839 - mmaction - INFO - Epoch(val) [20][3]\ttop1_acc: 0.2941, top5_acc: 1.0000, mean_class_accuracy: 0.2111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 17/17, 10.3 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-23 17:15:10,789 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-23 17:15:10,791 - mmaction - INFO - \n",
      "top1_acc\t0.4706\n",
      "top5_acc\t0.9412\n",
      "2021-08-23 17:15:10,791 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-23 17:15:10,793 - mmaction - INFO - \n",
      "mean_acc\t0.3000\n",
      "2021-08-23 17:15:11,107 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_25.pth.\n",
      "2021-08-23 17:15:11,108 - mmaction - INFO - Best top1_acc is 0.4706 at 25 epoch.\n",
      "2021-08-23 17:15:11,109 - mmaction - INFO - Epoch(val) [25][3]\ttop1_acc: 0.4706, top5_acc: 0.9412, mean_class_accuracy: 0.3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 17/17, 10.2 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-23 17:17:12,889 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-23 17:17:12,890 - mmaction - INFO - \n",
      "top1_acc\t0.4706\n",
      "top5_acc\t0.9412\n",
      "2021-08-23 17:17:12,891 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-23 17:17:12,893 - mmaction - INFO - \n",
      "mean_acc\t0.2833\n",
      "2021-08-23 17:17:12,894 - mmaction - INFO - Epoch(val) [30][3]\ttop1_acc: 0.4706, top5_acc: 0.9412, mean_class_accuracy: 0.2833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 17/17, 10.2 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-23 17:19:14,504 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-23 17:19:14,506 - mmaction - INFO - \n",
      "top1_acc\t0.3529\n",
      "top5_acc\t1.0000\n",
      "2021-08-23 17:19:14,507 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-23 17:19:14,508 - mmaction - INFO - \n",
      "mean_acc\t0.2333\n",
      "2021-08-23 17:19:14,509 - mmaction - INFO - Epoch(val) [35][3]\ttop1_acc: 0.3529, top5_acc: 1.0000, mean_class_accuracy: 0.2333\n",
      "2021-08-23 17:21:14,616 - mmaction - INFO - Saving checkpoint at 40 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 17/17, 10.6 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-23 17:21:16,604 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-23 17:21:16,605 - mmaction - INFO - \n",
      "top1_acc\t0.4118\n",
      "top5_acc\t1.0000\n",
      "2021-08-23 17:21:16,606 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-23 17:21:16,607 - mmaction - INFO - \n",
      "mean_acc\t0.2667\n",
      "2021-08-23 17:21:16,607 - mmaction - INFO - Epoch(val) [40][3]\ttop1_acc: 0.4118, top5_acc: 1.0000, mean_class_accuracy: 0.2667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 17/17, 10.2 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-23 17:23:18,403 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-23 17:23:18,405 - mmaction - INFO - \n",
      "top1_acc\t0.4706\n",
      "top5_acc\t1.0000\n",
      "2021-08-23 17:23:18,405 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-23 17:23:18,406 - mmaction - INFO - \n",
      "mean_acc\t0.2944\n",
      "2021-08-23 17:23:18,407 - mmaction - INFO - Epoch(val) [45][3]\ttop1_acc: 0.4706, top5_acc: 1.0000, mean_class_accuracy: 0.2944\n",
      "2021-08-23 17:25:18,120 - mmaction - INFO - Saving checkpoint at 50 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 17/17, 10.2 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-23 17:25:20,187 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-23 17:25:20,189 - mmaction - INFO - \n",
      "top1_acc\t0.4118\n",
      "top5_acc\t1.0000\n",
      "2021-08-23 17:25:20,190 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-23 17:25:20,191 - mmaction - INFO - \n",
      "mean_acc\t0.2667\n",
      "2021-08-23 17:25:20,192 - mmaction - INFO - Epoch(val) [50][3]\ttop1_acc: 0.4118, top5_acc: 1.0000, mean_class_accuracy: 0.2667\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "from mmaction.datasets import build_dataset\n",
    "from mmaction.models import build_model\n",
    "from mmaction.apis import train_model\n",
    "\n",
    "import mmcv\n",
    "\n",
    "# Build the dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# Build the recognizer\n",
    "model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.train_cfg, test_cfg=cfg.test_cfg)\n",
    "# CUDA_LAUNCH_BLOCKING=1\n",
    "# Create work_dir\n",
    "mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
    "train_model(model, datasets, cfg, distributed=False, validate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d6de14c-7f8d-4397-b11c-4bdcb42bb850",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f\"{cfg.work_dir}/model50e\", 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce03ef4d-608d-4108-b6da-5768cfc5b83c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 16:54:18,016 - mmaction - INFO - load model from: https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth\n",
      "2021-08-10 16:54:18,017 - mmaction - INFO - Use load_from_http loader\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "from mmaction.datasets import build_dataset\n",
    "from mmaction.models import build_model\n",
    "from mmaction.apis import train_model\n",
    "import pickle\n",
    "import mmcv\n",
    "# Build the dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# Build the recognizer\n",
    "model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "\n",
    "model = pickle.load(open(f\"{cfg.work_dir}/model50e\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39c23b5e-3e29-444f-beff-579b92ec9f1b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eyY3hCMwyTct",
    "outputId": "200e37c7-0da4-421f-98da-41418c3110ca",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 24/24, 0.6 task/s, elapsed: 39s, ETA:     0s\n",
      "Evaluating top_k_accuracy ...\n",
      "\n",
      "top1_acc\t0.4167\n",
      "top5_acc\t0.9583\n",
      "\n",
      "Evaluating mean_class_accuracy ...\n",
      "\n",
      "mean_acc\t0.2533\n",
      "top1_acc: 0.4167\n",
      "top5_acc: 0.9583\n",
      "mean_class_accuracy: 0.2533\n"
     ]
    }
   ],
   "source": [
    "from mmaction.apis import single_gpu_test\n",
    "from mmaction.datasets import build_dataloader\n",
    "from mmcv.parallel import MMDataParallel\n",
    "# from mmaction.models import build_model\n",
    "# from mmaction.datasets import build_dataset\n",
    "\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# Build a test dataloader\n",
    "dataset = build_dataset(cfg.data.test, dict(test_mode=True))\n",
    "data_loader = build_dataloader(\n",
    "        dataset,\n",
    "        videos_per_gpu=1,\n",
    "        workers_per_gpu=1,\n",
    "        dist=False,\n",
    "        shuffle=False)\n",
    "model = MMDataParallel(model, device_ids=[0])\n",
    "outputs = single_gpu_test(model, data_loader)\n",
    "\n",
    "eval_config = cfg.evaluation\n",
    "eval_config.pop('interval')\n",
    "eval_res = dataset.evaluate(outputs, **eval_config)\n",
    "for name, val in eval_res.items():\n",
    "    print(f'{name}: {val:.04f}')\n",
    "    \n",
    "output_config = cfg.output_config\n",
    "dataset.dump_results(outputs, **output_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f01ead32-8af1-42bb-b420-17b49f9eed24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from mmaction.core import confusion_matrix \n",
    "\n",
    "gt_labels = [ann['label'] for ann in dataset.load_annotations()]\n",
    "pred = np.argmax(outputs, axis=1)\n",
    "cf_mat = confusion_matrix(pred, gt_labels).astype(float)\n",
    "cmap = sns.cubehelix_palette(50, hue=0.05, rot=0, light=0.9, dark=0, as_cmap=True)\n",
    "# /np.sum(cf_mat), fmt='.2%',\n",
    "sns.heatmap(cf_mat, cmap=cmap, annot=True, xticklabels = ['6yo', '7yo', '8yo', '9yo', '10yo'], yticklabels = ['6yo', '7yo', '8yo', '9yo', '10yo'])\n",
    "# sns.heatmap(cf_mat, cmap=cmap, annot=True, xticklabels = ['7yo', '8yo', '9yo', '10yo'], yticklabels = ['7yo', '8yo', '9yo', '10yo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "857793bd-40c8-41c9-9ad8-01c3e9099da5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7083333333333334\n"
     ]
    }
   ],
   "source": [
    "mean_error = (2+3+4+4+2+1+1)/24 #number of all tested videos\n",
    "print(mean_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad56099-6810-496c-9b39-e7e3422a358d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75489caa-817b-4aad-9d27-bb7115885f7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4417965-41aa-4932-afa3-f53de8399825",
   "metadata": {},
   "source": [
    "# CSN age walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee0fe8f4-bf21-4251-b1b1-91b693d4f7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/robt427nv/childact\n"
     ]
    }
   ],
   "source": [
    "cd /home/robt427nv/childact/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc1ea81c-ac38-4c9f-b280-ce8960ae3216",
   "metadata": {
    "id": "LjCcmCKOjktc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mmcv import Config\n",
    "cfg = Config.fromfile('mmaction2/configs/recognition/csn/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ee47121-b6be-4ea8-b9ea-a55ddf1d520f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "tlhu9byjjt-K",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "13d1bb01-f351-48c0-9efb-0bdf2c125d29",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "model = dict(\n",
      "    type='Recognizer3D',\n",
      "    backbone=dict(\n",
      "        type='ResNet3dCSN',\n",
      "        pretrained2d=False,\n",
      "        pretrained=\n",
      "        'https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth',\n",
      "        depth=152,\n",
      "        with_pool2=False,\n",
      "        bottleneck_mode='ir',\n",
      "        norm_eval=True,\n",
      "        zero_init_residual=False,\n",
      "        bn_frozen=True),\n",
      "    cls_head=dict(\n",
      "        type='I3DHead',\n",
      "        num_classes=6,\n",
      "        in_channels=2048,\n",
      "        spatial_type='avg',\n",
      "        dropout_ratio=0.5,\n",
      "        init_std=0.01),\n",
      "    train_cfg=None,\n",
      "    test_cfg=dict(average_clips='prob'))\n",
      "checkpoint_config = dict(interval=20)\n",
      "log_config = dict(interval=100, hooks=[dict(type='TextLoggerHook')])\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = 'checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth'\n",
      "resume_from = None\n",
      "workflow = [('train', 1)]\n",
      "dataset_type = 'RawframeDataset'\n",
      "data_root = 'age-gender-3split-rgb-frames/'\n",
      "data_root_val = 'age-gender-3split-rgb-frames/val/'\n",
      "ann_file_train = 'age-gender-3split-rgb-frames/childact_train_rgb320_age_walk.txt'\n",
      "ann_file_val = 'age-gender-3split-rgb-frames/childact_val_rgb320_age_walk.txt'\n",
      "ann_file_test = 'age-gender-3split-rgb-frames/childact_test_rgb320_age_walk.txt'\n",
      "img_norm_cfg = dict(\n",
      "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_bgr=False)\n",
      "train_pipeline = [\n",
      "    dict(type='SampleFrames', clip_len=32, frame_interval=2, num_clips=1),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='RandomResizedCrop'),\n",
      "    dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "    dict(type='Flip', flip_ratio=0.5),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "]\n",
      "val_pipeline = [\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=32,\n",
      "        frame_interval=2,\n",
      "        num_clips=1,\n",
      "        test_mode=True),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='CenterCrop', crop_size=224),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=32,\n",
      "        frame_interval=2,\n",
      "        num_clips=10,\n",
      "        test_mode=True),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='ThreeCrop', crop_size=256),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "data = dict(\n",
      "    videos_per_gpu=6,\n",
      "    workers_per_gpu=4,\n",
      "    train=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_train_rgb320_age_walk.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/train/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=1),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='RandomResizedCrop'),\n",
      "            dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "            dict(type='Flip', flip_ratio=0.5),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'),\n",
      "    val=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_val_rgb320_age_walk.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/val/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=1,\n",
      "                test_mode=True),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='CenterCrop', crop_size=224),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'),\n",
      "    test=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_test_rgb320_age_walk.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/test/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=10,\n",
      "                test_mode=True),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='ThreeCrop', crop_size=256),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'))\n",
      "evaluation = dict(\n",
      "    interval=5, metrics=['top_k_accuracy', 'mean_class_accuracy'])\n",
      "optimizer = dict(type='SGD', lr=0.000125, momentum=0.9, weight_decay=0.0001)\n",
      "optimizer_config = dict(grad_clip=dict(max_norm=40, norm_type=2))\n",
      "lr_config = dict(\n",
      "    policy='step',\n",
      "    step=[32, 48],\n",
      "    warmup='linear',\n",
      "    warmup_ratio=0.1,\n",
      "    warmup_by_epoch=True,\n",
      "    warmup_iters=16)\n",
      "total_epochs = 50\n",
      "work_dir = './childact-checkpoints/CSN-age-walk'\n",
      "find_unused_parameters = True\n",
      "omnisource = False\n",
      "seed = 42\n",
      "gpu_ids = range(0, 1)\n",
      "output_config = dict(out='./childact-checkpoints/CSN-age-walk/results.json')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mmcv.runner import set_random_seed\n",
    "\n",
    "# Modify dataset type and path\n",
    "# cfg.dataset_type = 'VideoDataset'\n",
    "cfg.data_root = 'age-gender-3split-rgb-frames/'\n",
    "cfg.data_root_val = 'age-gender-3split-rgb-frames/val/'\n",
    "cfg.ann_file_train = 'age-gender-3split-rgb-frames/childact_train_rgb320_age_walk.txt'\n",
    "cfg.ann_file_val = 'age-gender-3split-rgb-frames/childact_val_rgb320_age_walk.txt'\n",
    "cfg.ann_file_test = 'age-gender-3split-rgb-frames/childact_test_rgb320_age_walk.txt'\n",
    "\n",
    "# cfg.data.test.type = 'VideoDataset'\n",
    "cfg.data.test.ann_file = 'age-gender-3split-rgb-frames/childact_test_rgb320_age_walk.txt'\n",
    "cfg.data.test.data_prefix = 'age-gender-3split-rgb-frames/test/'\n",
    "\n",
    "# cfg.data.train.type = 'VideoDataset'\n",
    "cfg.data.train.ann_file = 'age-gender-3split-rgb-frames/childact_train_rgb320_age_walk.txt'\n",
    "cfg.data.train.data_prefix = 'age-gender-3split-rgb-frames/train/'\n",
    "\n",
    "# cfg.data.val.type = 'VideoDataset'\n",
    "cfg.data.val.ann_file = 'age-gender-3split-rgb-frames/childact_val_rgb320_age_walk.txt'\n",
    "cfg.data.val.data_prefix = 'age-gender-3split-rgb-frames/val/'\n",
    "\n",
    "# cfg.data.test.modality = 'Flow'\n",
    "# cfg.data.val.modality = 'Flow'\n",
    "# cfg.data.train.modality = 'Flow'\n",
    "\n",
    "# cfg.data.train.start_index = 0\n",
    "# cfg.data.test.start_index = 0\n",
    "# cfg.data.val.start_index = 0\n",
    "\n",
    "cfg.data.test.filename_tmpl = '{:03}.jpeg'\n",
    "cfg.data.train.filename_tmpl = '{:03}.jpeg'\n",
    "cfg.data.val.filename_tmpl = '{:03}.jpeg'\n",
    "# The flag is used to determine whether it is omnisource training\n",
    "cfg.setdefault('omnisource', False)\n",
    "# Modify num classes of the model in cls_head\n",
    "cfg.model.cls_head.num_classes = 6\n",
    "# We can use the pre-trained TSN model\n",
    "cfg.load_from = 'checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth'\n",
    "# cfg.resume_from = './childact-mm/latest.pth'\n",
    "\n",
    "# Set up working dir to save files and logs.\n",
    "cfg.work_dir = './childact-checkpoints/CSN-age-walk'\n",
    "\n",
    "cfg.total_epochs = 50\n",
    "\n",
    "# cfg.momentum_config = dict(\n",
    "#     policy='cyclic',\n",
    "#     target_ratio=(0.85 / 0.95, 1),\n",
    "#     cyclic_times=1,\n",
    "#     step_ratio_up=0.4,\n",
    "# )\n",
    "\n",
    "# We can set the checkpoint saving interval to reduce the storage cost\n",
    "cfg.checkpoint_config.interval = 20\n",
    "# We can set the log print interval to reduce the the times of printing log\n",
    "cfg.log_config.interval = 100\n",
    "\n",
    "# Set seed thus the results are more reproducible\n",
    "cfg.seed = 42\n",
    "set_random_seed(42, deterministic=False)\n",
    "cfg.gpu_ids = range(1)\n",
    "\n",
    "cfg.data.videos_per_gpu=6\n",
    "\n",
    "# cfg.model.backbone.in_channels = 2\n",
    "\n",
    "cfg.output_config = dict(out=f'{cfg.work_dir}/results.json')\n",
    "# We can initialize the logger for training and have a look\n",
    "# at the final config used for training\n",
    "# del cfg.optimizer['momentum']\n",
    "print(f'Config:\\n{cfg.pretty_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5980b85-cadc-45c3-a583-87d2ea87bb6f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "dDBWkdDRk6oz",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "85a52ef3-7b5c-4c52-8fef-00322a8c65e6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-23 22:56:23,515 - mmaction - INFO - load model from: https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth\n",
      "2021-08-23 22:56:23,516 - mmaction - INFO - Use load_from_http loader\n",
      "2021-08-23 22:56:25,627 - mmaction - INFO - load checkpoint from checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth\n",
      "2021-08-23 22:56:25,628 - mmaction - INFO - Use load_from_local loader\n",
      "2021-08-23 22:56:25,821 - mmaction - WARNING - The model and loaded state dict do not match exactly\n",
      "\n",
      "size mismatch for cls_head.fc_cls.weight: copying a param with shape torch.Size([400, 2048]) from checkpoint, the shape in current model is torch.Size([6, 2048]).\n",
      "size mismatch for cls_head.fc_cls.bias: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([6]).\n",
      "2021-08-23 22:56:25,827 - mmaction - INFO - Start running, host: robt427nv@robt427NV, work_dir: /home/robt427nv/childact/childact-checkpoints/CSN-age-walk\n",
      "2021-08-23 22:56:25,828 - mmaction - INFO - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(ABOVE_NORMAL) OptimizerHook                      \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "2021-08-23 22:56:25,829 - mmaction - INFO - workflow: [('train', 1)], max: 50 epochs\n",
      "/home/robt427nv/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/mmcv/runner/hooks/evaluation.py:190: UserWarning: runner.meta is None. Creating an empty one.\n",
      "  warnings.warn('runner.meta is None. Creating an empty one.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 20/20, 10.7 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-23 22:57:58,971 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-23 22:57:58,973 - mmaction - INFO - \n",
      "top1_acc\t0.2500\n",
      "top5_acc\t0.9500\n",
      "2021-08-23 22:57:58,973 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-23 22:57:58,974 - mmaction - INFO - \n",
      "mean_acc\t0.1905\n",
      "2021-08-23 22:57:59,297 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_5.pth.\n",
      "2021-08-23 22:57:59,298 - mmaction - INFO - Best top1_acc is 0.2500 at 5 epoch.\n",
      "2021-08-23 22:57:59,299 - mmaction - INFO - Epoch(val) [5][4]\ttop1_acc: 0.2500, top5_acc: 0.9500, mean_class_accuracy: 0.1905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 20/20, 10.7 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-23 22:59:31,058 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-23 22:59:31,059 - mmaction - INFO - \n",
      "top1_acc\t0.2500\n",
      "top5_acc\t0.9500\n",
      "2021-08-23 22:59:31,060 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-23 22:59:31,061 - mmaction - INFO - \n",
      "mean_acc\t0.1726\n",
      "2021-08-23 22:59:31,062 - mmaction - INFO - Epoch(val) [10][4]\ttop1_acc: 0.2500, top5_acc: 0.9500, mean_class_accuracy: 0.1726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 20/20, 11.2 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-23 23:01:02,645 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-23 23:01:02,646 - mmaction - INFO - \n",
      "top1_acc\t0.5000\n",
      "top5_acc\t0.9500\n",
      "2021-08-23 23:01:02,647 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-23 23:01:02,648 - mmaction - INFO - \n",
      "mean_acc\t0.3202\n",
      "2021-08-23 23:01:03,080 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_15.pth.\n",
      "2021-08-23 23:01:03,081 - mmaction - INFO - Best top1_acc is 0.5000 at 15 epoch.\n",
      "2021-08-23 23:01:03,082 - mmaction - INFO - Epoch(val) [15][4]\ttop1_acc: 0.5000, top5_acc: 0.9500, mean_class_accuracy: 0.3202\n",
      "2021-08-23 23:02:33,152 - mmaction - INFO - Saving checkpoint at 20 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 20/20, 10.7 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-23 23:02:35,443 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-23 23:02:35,444 - mmaction - INFO - \n",
      "top1_acc\t0.5000\n",
      "top5_acc\t0.9500\n",
      "2021-08-23 23:02:35,444 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-23 23:02:35,444 - mmaction - INFO - \n",
      "mean_acc\t0.3024\n",
      "2021-08-23 23:02:35,445 - mmaction - INFO - Epoch(val) [20][4]\ttop1_acc: 0.5000, top5_acc: 0.9500, mean_class_accuracy: 0.3024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 20/20, 10.7 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-23 23:04:07,604 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-23 23:04:07,606 - mmaction - INFO - \n",
      "top1_acc\t0.5000\n",
      "top5_acc\t0.9500\n",
      "2021-08-23 23:04:07,606 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-23 23:04:07,607 - mmaction - INFO - \n",
      "mean_acc\t0.3202\n",
      "2021-08-23 23:04:07,608 - mmaction - INFO - Epoch(val) [25][4]\ttop1_acc: 0.5000, top5_acc: 0.9500, mean_class_accuracy: 0.3202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 20/20, 10.8 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-23 23:05:39,939 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-23 23:05:39,940 - mmaction - INFO - \n",
      "top1_acc\t0.2500\n",
      "top5_acc\t0.9500\n",
      "2021-08-23 23:05:39,941 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-23 23:05:39,942 - mmaction - INFO - \n",
      "mean_acc\t0.1476\n",
      "2021-08-23 23:05:39,943 - mmaction - INFO - Epoch(val) [30][4]\ttop1_acc: 0.2500, top5_acc: 0.9500, mean_class_accuracy: 0.1476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 20/20, 11.3 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-23 23:07:11,709 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-23 23:07:11,711 - mmaction - INFO - \n",
      "top1_acc\t0.3000\n",
      "top5_acc\t0.9500\n",
      "2021-08-23 23:07:11,712 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-23 23:07:11,713 - mmaction - INFO - \n",
      "mean_acc\t0.1964\n",
      "2021-08-23 23:07:11,714 - mmaction - INFO - Epoch(val) [35][4]\ttop1_acc: 0.3000, top5_acc: 0.9500, mean_class_accuracy: 0.1964\n",
      "2021-08-23 23:08:41,428 - mmaction - INFO - Saving checkpoint at 40 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 20/20, 10.7 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-23 23:08:43,704 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-23 23:08:43,705 - mmaction - INFO - \n",
      "top1_acc\t0.4000\n",
      "top5_acc\t1.0000\n",
      "2021-08-23 23:08:43,706 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-23 23:08:43,706 - mmaction - INFO - \n",
      "mean_acc\t0.2536\n",
      "2021-08-23 23:08:43,707 - mmaction - INFO - Epoch(val) [40][4]\ttop1_acc: 0.4000, top5_acc: 1.0000, mean_class_accuracy: 0.2536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 20/20, 11.0 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-23 23:10:15,633 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-23 23:10:15,636 - mmaction - INFO - \n",
      "top1_acc\t0.4500\n",
      "top5_acc\t0.9500\n",
      "2021-08-23 23:10:15,636 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-23 23:10:15,638 - mmaction - INFO - \n",
      "mean_acc\t0.2964\n",
      "2021-08-23 23:10:15,639 - mmaction - INFO - Epoch(val) [45][4]\ttop1_acc: 0.4500, top5_acc: 0.9500, mean_class_accuracy: 0.2964\n",
      "2021-08-23 23:11:45,494 - mmaction - INFO - Saving checkpoint at 50 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 20/20, 10.9 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-23 23:11:47,748 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-23 23:11:47,759 - mmaction - INFO - \n",
      "top1_acc\t0.3000\n",
      "top5_acc\t0.9500\n",
      "2021-08-23 23:11:47,759 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-23 23:11:47,762 - mmaction - INFO - \n",
      "mean_acc\t0.1881\n",
      "2021-08-23 23:11:47,763 - mmaction - INFO - Epoch(val) [50][4]\ttop1_acc: 0.3000, top5_acc: 0.9500, mean_class_accuracy: 0.1881\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "from mmaction.datasets import build_dataset\n",
    "from mmaction.models import build_model\n",
    "from mmaction.apis import train_model\n",
    "\n",
    "import mmcv\n",
    "\n",
    "# Build the dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# Build the recognizer\n",
    "model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.train_cfg, test_cfg=cfg.test_cfg)\n",
    "# CUDA_LAUNCH_BLOCKING=1\n",
    "# Create work_dir\n",
    "mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
    "train_model(model, datasets, cfg, distributed=False, validate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4715fb04-d960-41ee-b55f-186c810b3f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f\"{cfg.work_dir}/model50e\", 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "246e9bf5-0e9b-4f36-9528-fd3608b63d0c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 16:54:18,016 - mmaction - INFO - load model from: https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth\n",
      "2021-08-10 16:54:18,017 - mmaction - INFO - Use load_from_http loader\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "from mmaction.datasets import build_dataset\n",
    "from mmaction.models import build_model\n",
    "from mmaction.apis import train_model\n",
    "import pickle\n",
    "import mmcv\n",
    "# Build the dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# Build the recognizer\n",
    "model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "\n",
    "model = pickle.load(open(f\"{cfg.work_dir}/model50e\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fc91578-0bb3-4f1f-8c78-6dbafdfd5004",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eyY3hCMwyTct",
    "outputId": "200e37c7-0da4-421f-98da-41418c3110ca",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 11/11, 0.2 task/s, elapsed: 51s, ETA:     0s\n",
      "Evaluating top_k_accuracy ...\n",
      "\n",
      "top1_acc\t0.4545\n",
      "top5_acc\t0.9091\n",
      "\n",
      "Evaluating mean_class_accuracy ...\n",
      "\n",
      "mean_acc\t0.6389\n",
      "top1_acc: 0.4545\n",
      "top5_acc: 0.9091\n",
      "mean_class_accuracy: 0.6389\n"
     ]
    }
   ],
   "source": [
    "from mmaction.apis import single_gpu_test\n",
    "from mmaction.datasets import build_dataloader\n",
    "from mmcv.parallel import MMDataParallel\n",
    "# from mmaction.models import build_model\n",
    "# from mmaction.datasets import build_dataset\n",
    "\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# Build a test dataloader\n",
    "dataset = build_dataset(cfg.data.test, dict(test_mode=True))\n",
    "data_loader = build_dataloader(\n",
    "        dataset,\n",
    "        videos_per_gpu=1,\n",
    "        workers_per_gpu=1,\n",
    "        dist=False,\n",
    "        shuffle=False)\n",
    "model = MMDataParallel(model, device_ids=[0])\n",
    "outputs = single_gpu_test(model, data_loader)\n",
    "\n",
    "eval_config = cfg.evaluation\n",
    "eval_config.pop('interval')\n",
    "eval_res = dataset.evaluate(outputs, **eval_config)\n",
    "for name, val in eval_res.items():\n",
    "    print(f'{name}: {val:.04f}')\n",
    "    \n",
    "output_config = cfg.output_config\n",
    "dataset.dump_results(outputs, **output_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9d80380-c516-428b-8fee-f77bd0bfd6aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAD8CAYAAADUv3dIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAXvElEQVR4nO3de5BdZZ3u8e/T6TYt4iQzpDnmhkTEOOGMJjGECCXBZlKTgMjUjBaBqJjSimWpgKUcZZxxkFM1c5iLyBgHpiXB5iKCDUgCQc3YwRiPJuYCBnLBENEEEtNACGMIMen85o+9wrRtd++9k73XWr36+VStyl6X/e5fViVPv/2ud62tiMDMzNLRkHUBZmZDiUPXzCxFDl0zsxQ5dM3MUuTQNTNLkUPXzCxFDl0zsz5Iapa0RtJjkp6Q9KU+jhku6W5J2yStlnRquXYdumZmfTsItEbE24HJwGxJM3od8xFgb0S8GbgBuL5cow5dM7M+RMlvk9WmZOl9N9nFQHvyugM4X5IGarexplX2oa1tkW95q7OlS5dmXcKQ8OCDD2RdQuFFxICBVQlJ1WTOx4AFPdbbIqKtR1vDgHXAm4GvRcTqXu8fC+wAiIjDkvYBJwHP9feBdQ9dM7O8SgK2bYD93cBkSSOB+yX974h4/Hg+08MLZlYokipeKhURLwIrgNm9dj0DjE8+txEYATw/UFsOXTMrlFqFrqSWpIeLpNcCs4AtvQ5bAlyevH4f0BllniLm4QUzK5RqerBljAbak3HdBuCeiHhQ0nXA2ohYAiwCbpe0DXgBmFuuUYeumRVKQ0NtfoGPiJ8DU/rY/sUer18B3l9Nuw5dMyuUGvZ068Kha2aF4tA1M0uRQ9fMLEV5D11PGTMzS5F7umZWKA0Nw7IuYUAOXTMrlLwPLzh0zaxQHLpmZily6JqZpciha2aWolrdBlwvDl0zK5S893Tz/SPBzKxg3NM1s0LJe0/XoWtmheLQNTNLkUPXzCxFnr1gZpYi93TNzFLk0DUzS5FD18wsRQ5dM7MUOXTNzFLk0DUzS5FD18wsRQ5dM7MUOXQHkZkz38Upp4znwIFX6Oi4L+tyCmnUqFF8+tNXMnLkSCD47ne/z9KlD2ZdVqEMHz6clStXMnz4cBobG+no6ODaa6/NuqzUOHQHka1bf8Hjj2/i3e+emXUphdXd3c3ixbfy1FPbee1rm7nhhn/l0UcfZceOnVmXVhgHDx6ktbWV/fv309jYyKpVq3j44YdZvXp11qWlola3AUsaD9wG/C8ggLaIuLHXMecBDwC/TDbdFxHXDdSuQ7eH3bt3c+KJJ2ZdRqHt3buXvXv3AnDgwCvs2LGTk046yaFbY/v37wegqamJpqYmIiLjitIj1ezZC4eBz0TEekmvB9ZJWh4Rm3od96OIeE+ljeb7yRBWaCeffDKnnfYmtm59MutSCqehoYENGzawZ88eli9fzpo1a7IuKTWSKl4GEhG7ImJ98vq/gM3A2OOtr6LQldQk6QpJHcnyKUlNx/vhNnQ1NzdzzTWf4+tfX8SBAweyLqdwjhw5wpQpUxg3bhzTp0/njDPOyLqkQU3SqcAUoK8xmndKekzSw5LKnuhKe7o3Ae8A/j1Zpibb+itwgaS1ktauXPnDCj/Chophw4ZxzTWf45FHfshPfvLTrMsptH379rFixQpmz56ddSmpqaan2zOrkmVBH+2dCNwLXBURL/XavR54Y0S8Hfgq8J1y9VUaumdGxOUR0Zks84Ez+zs4ItoiYlpETDv3XF+Ust93xRWfZMeOnTzwwJKsSymkUaNGMWLECKD0G8WsWbPYsmVLxlWlp5rQ7ZlVydLWq60mSoF7Z0T8wZSmiHgpIn6bvF4GNEkaNVB9lV5I65Z0WkQ8lRTyJqC7wvcOGq2t5zFmzGiam5u57LK5rFu33uONNTZp0p/S2vpufvnLp7nxxhsAuO22O1i3bl3GlRXH6NGjaW9vZ9iwYTQ0NHDPPffw0EMPZV1Wamo4e0HAImBzRHy5n2PeAPwmIkLSdEod2ecHarfS0L0aWCFpOyDgjcD8SosfLDo7H8m6hMLbtGkzF130l1mXUWgbN25k6tSpWZeRmRrO0z0H+CCwUdKjyba/AU4BiIibgfcBH5d0GDgAzI0yU0UqCt2I+IGk04GJyaatEXGw6r+CmVmd1Sp0I2IVpU7mQMcsBBZW025FoStpHaVu9l0RsbeaDzAzS1Pe70irdPDjEkrz034m6VuS/kJ5/5uZ2ZBUq3m69VJR6EbEtoj4AvAW4JvAYuBXkr4k6U/qWaCZWTUKEboAkt4GfBn4Z0pTKN4PvAR01qc0M7Pq5T10qxnTfRG4Bfg/EfG7ZNdqSefUqTYzs6rlfeRzwJ6upLMk/RGlXu1FwJ8C90q6XtIIgIj4q/qXaWZWmbz3dMsNLywGXo6I7cBXgD8CrgdeBm6tb2lmZtXLe+iWG15oiIjDyetpEXF0xvWqHpOFzcxyY1APLwCPSzp659ljkqYBSHoLcKiulZmZHYOGhoaKl0zqK7P/o8BMSU8Bk4CfJLcCfz3ZZ2aWK4N6eCEi9gEfTi6mTUiO3xkRv0mjODOzauV9eKHSZy+8BDxW51rMzI5b3kPXX9djZpYifzGlmRVK3nu6Dl0zK5SsZiVUyqFrZoXinq6ZWYocumZmKXLompmlyKFrZpYih66ZWYocumZmKXLompmlyKFrZpYih66ZWYocumZmKWpoGJZ1CQNy6JpZobina2aWoryHbr4fx2NmVqVafV2PpPGSVkjaJOkJSVf2cYwk/ZukbZJ+LmlqX2315J6umVnfDgOfiYj1kl4PrJO0PCI29ThmDnB6spwF3JT82S/3dM2sUGrV042IXRGxPnn9X8BmYGyvwy4GbouSnwIjJY0eqN2693QvuujCen/EkPexj/mLmdPwH/9xS9YlWAWqeYi5pAXAgh6b2iKirY/jTgWmAKt77RoL7OixvjPZtqu/z/TwgpkVSjUX0pKA/YOQ7dXeicC9wFXJl/QeF4eumRVKLWcvSGqiFLh3RsR9fRzyDDC+x/q4ZFu/PKZrZoVSw9kLAhYBmyPiy/0ctgT4UDKLYQawLyL6HVoA93TNrGBq2NM9B/ggsFHSo8m2vwFOAYiIm4FlwAXANuBlYH65Rh26ZlYotQrdiFgFDNhYRATwiWradeiaWaHk/SvY812dmVnBuKdrZoWS92cvOHTNrFAcumZmKXLompmlKO8X0hy6ZlYo7umamaXIoWtmliKHrplZivIeuvkecTYzKxj3dM2sUDx7wcwsRXkfXnDomlmhOHTNzFLk0DUzS5FD18wsRb6QZmaWIvd0zcxSlPfQzXc/3MysYNzTNbNCyXtP16FrZoXi0DUzS5FnL5iZpcg9XTOzFDl0zcxS5NAdRFavXs3ChV+lu/sIF154IfPmzcu6pMIZPnw4K1euZPjw4TQ2NtLR0cG1116bdVmFMnPmuzjllPEcOPAKHR33ZV1O6vIeuvkecU5Rd3c3N974Fa6//p9ob2+ns/MHPP3001mXVTgHDx6ktbWVyZMnM3nyZGbPns1ZZ52VdVmFsnXrL1i27HtZl5EZSRUvFbS1WNIeSY/3s/88SfskPZosXyzXpkM3sWXLZsaOHcuYMWNoamqitbWVH/94VdZlFdL+/fsBaGpqoqmpiYjIuKJi2b17NwcPHsy6jMzUMnSBbwCzyxzzo4iYnCzXlWuwotCV1CTpCkkdyfIpSU2VvHew6Op6jpaWk19db2lpoavruQwrKq6GhgY2bNjAnj17WL58OWvWrMm6JCuQWoZuRKwEXqhlfZX2dG8C3gH8e7JMTbb1SdICSWslrb3jjtuPv0orlCNHjjBlyhTGjRvH9OnTOeOMM7IuyQqkmtDtmVXJsuAYPvKdkh6T9LCksv+YK72QdmZEvL3Heqekx/o7OCLagDaAXbt2D4rfHVtaRtHVtefV9a6uLlpaRmVYUfHt27ePFStWMHv2bJ544omsy7GCqOZCWs+sOkbrgTdGxG8lXQB8Bzh9oDdU2tPtlnTa0RVJbwK6j7XKPJo48a3s3LmTXbt2cejQITo7Ozn77HOyLqtwRo0axYgRIwBobm5m1qxZbNmyJeOqrEhqPKY7oIh4KSJ+m7xeBjRJGrC3VmlP92pghaTtgIA3AvOPp9i8aWxs5Morr+Lqqz/LkSNHmDPnAiZMmJB1WYUzevRo2tvbGTZsGA0NDdxzzz089NBDWZdVKK2t5zFmzGiam5u57LK5rFu3nq1bn8y6rNSkeRuwpDcAv4mIkDSdUkf2+YHeU1HoRsQPJJ0OTEw2bY2Iwl0enTFjBjNmzMi6jELbuHEjU6dOzbqMQuvsfCTrEjJVy3m6ku4CzgNGSdoJ/D3QBBARNwPvAz4u6TBwAJgbZabjVBS6ktYBi4C7ImLvMf8NzMzqrJahGxGXltm/EFhYTZuV9sMvAcYCP5P0LUl/obzf9mFmlkMVhW5EbIuILwBvAb4JLAZ+JelLkv6kngWamVUjzQtpx6LiEWdJbwP+Ffhn4F7g/cBLQGd9SjMzq17eQ7eaMd0XKY3rfr7HRbTVkjyvysxyY1A/xFzSFcD9wPsjYntfx0TEX9WjMDOzY5H3y03lfiT8X2A10C7p45JaUqjJzOyY5X14oVzobgfGUQrfacAmSd+VdLmk19e9OjOzKg320I2IOBIR34+IjwBjKD3wZjalQDYzy5W8h265C2m/V1VEHAKWAEsknVC3qszMjlHex3TLhe4l/e2IiJdrXIuZ2XEb1KEbEUPnKRlmVgiDOnTNzAYbh66ZWYocumZmKXLompmlaFDfBmxmNti4p2tmliKHrplZivIeuvke/DAzKxj3dM2sUPLe03XomlmhePaCmVmK3NM1M0uRQ9fMLEUOXTOzFDl0zcxS5NA1M0tR3kM333MrzMyqVMvvSJO0WNIeSY/3s1+S/k3SNkk/lzS1XJsOXTMrlBp/MeU3KH0Rb3/mAKcnywLgpnINOnTNrFBqGboRsRJ4YYBDLgZui5KfAiMljR6oTY/pFsCzz+7KuoQhYenSh7IuwSpQzZiupAWUeqhHtUVEWxUfNxbY0WN9Z7Kt3/+UDl0zK5RqbgNOAraakD1uDl0zK5SUZy88A4zvsT4u2dYvj+maWaHU+EJaOUuADyWzGGYA+yJiwPE+93TNrFBq2dOVdBdwHjBK0k7g74EmgIi4GVgGXABsA14G5pdr06FrZoVSy9CNiEvL7A/gE9W06eEFM7MUuadrZoXih5ibmaUo789ecOiaWaE4dM3MUuTQNTNLkUPXzCxFvpBmZpaivPd08/0jwcysYNzTNbNCyXtP16FrZoXi0DUzS5FD18wsRZ69YGaWIvd0zcxS5NA1M0uRQ9fMLEV5D918jzibmRWMe7pmViievWBmlqK8Dy84dM2sUBy6ZmYpcuiamaXIoWtmliJfSDMzS1HOO7oOXTMrFg8vDCKrV69m4cKv0t19hAsvvJB58+ZlXVIh+TzX18yZ7+KUU8Zz4MArdHTcl3U51ku+Bz9S1N3dzY03foXrr/8n2tvb6ez8AU8//XTWZRWOz3P9bd36C5Yt+17WZWRGUsVLBW3NlrRV0jZJn+9j/4cldUl6NFk+Wq5Nh25iy5bNjB07ljFjxtDU1ERrays//vGqrMsqHJ/n+tu9ezcHDx7MuozM1Cp0JQ0DvgbMASYBl0qa1Mehd0fE5GS5pVx9Dt1EV9dztLSc/Op6S0sLXV3PZVhRMfk8W701NDRUvJQxHdgWEdsj4nfAt4CLj7e+isd0Jb0XODdZ/WFELD3eDzczq7UaXkgbC+zosb4TOKuP4/5a0rnAk8CnI2JHH8e8qqKerqR/BK4ENiXLFZL+YYDjF0haK2ntHXfcXslHZK6lZRRdXXteXe/q6qKlZVSGFRWTz7PVWzXDCz2zKlkWVPlxS4FTI+JtwHKgvdwbKh1euBCYFRGLI2IxMBt4T38HR0RbREyLiGkf+MAHK/yIbE2c+FZ27tzJrl27OHToEJ2dnZx99jlZl1U4Ps9Wb9WEbs+sSpa2Hk09A4zvsT4u2faqiHg+Io4OoN8CvKNcfdVMGRsJvJC8HlHF+waFxsZGrrzyKq6++rMcOXKEOXMuYMKECVmXVTg+z/XX2noeY8aMprm5mcsum8u6devZuvXJrMtKTQ2HF34GnC5pAqWwnQtc1uuzRkfErmT1vcDmco1WGrr/CGyQtAIQpbHdP5g+MdjNmDGDGTNmZF1G4fk811dn5yNZl5CpWoVuRByW9Enge8AwYHFEPCHpOmBtRCyhNNT6XuAwpU7ph8u1W1HoRsRdkh4Bzkw2fS4idlf/1zAzq69a3pEWEcuAZb22fbHH62uAa6pps6LQlXQvsAh4MCKOVPMBZmZpyvttwJVeSLsJmAf8QtL/kzSxjjWZmR2zWt6RVg8VhW5E/GdEzAOmAk8D/ynp/0uaL6mpngWamVWjEKELIOkkSoPEHwU2ADdSCuHldanMzOwY5D10Kx3TvR+YCNwOvKfHRbS7Ja2tV3FmZtXK+0PMB6xO0mskfQj4WkRMAn4N/K2kTxwdVoiIaSnUaWZWkcHe0701OeYESZcDrwPuB86n9DCIy+tbnplZdfI+e6Fc6P5ZRLxNUiOlOzLGRES3pDuAx+pfnplZdfIeuuUGPxokvQZ4PXAC/3P773DAsxbMzKpUrqe7CNhC6Ra4LwDflrQdmEHp2ZJmZrmS957ugKEbETdIujt5/ayk24A/B74eEWvSKNDMrBp5n71QdspYRDzb4/WLQEc9CzIzOx6DuqdrZjbYOHTNzFLk0DUzS5FD18wsRQ5dM7MUOXTNzFLk0DUzS5FD18wsRQ5dM7MUOXTNzFLU0ODQNTNLjXu6ZmYpcuiamaUo76Gb72egmZkVjHu6ZlYoee/pOnTNrFDy/hDzfFdnZlalWn4Fu6TZkrZK2ibp833sHy7p7mT/akmnlmvToWtmhVKr0JU0DPgaMAeYBFwqaVKvwz4C7I2INwM3ANeXq8+ha2aFUsOe7nRgW0Rsj4jfUfoy3ot7HXMx0J687gDOV5mG6z6mO3r0G/I9qt0HSQsioi3rOopsMJ7jBQs+knUJVRmM57gWqskcSQuABT02tfU4Z2OBHT327QTO6tXEq8dExGFJ+4CTgOf6+0z3dPu2oPwhdpx8juvP57iMiGiLiGk9lrr/kHLompn17RlgfI/1ccm2Po+R1AiMAJ4fqFGHrplZ334GnC5pgqTXAHOBJb2OWQJcnrx+H9AZETFQo56n27chNw6WAZ/j+vM5Pg7JGO0nge8Bw4DFEfGEpOuAtRGxBFgE3C5pG/ACpWAekMqEspmZ1ZCHF8zMUuTQNTNL0ZAMXUkjJXVI2iJps6R3Zl1T0UiaKOnRHstLkq7Kuq6ikfRpSU9IelzSXZKas67JBjYkx3QltQM/iohbkquSJ0TEixmXVVjJ7ZTPAGdFxK+yrqcoJI0FVgGTIuKApHuAZRHxjWwrs4EMuZ6upBHAuZSuOpLc3neSpPU9jjn96Lqk8yVtkLRR0mJJwzMpfHA7H3gKaPR5rrlG4LXJHNETgGclfefoTkmzJN2fvL40Ob+PSyr7jACrjyEXusAEoAu4NflPfguwG9gnaXJyzPxkfzPwDeCSiPgzSv/AP55+yYPeXOCuiHgKn+eaiYhngH8Bfg3sAvYBy4G3SmpJDpsPLJY0htLDWFqBycCZkv4y7ZptaIZuIzAVuCkipgD7gc8DtwDzk1+FLwG+CUwEfhkRTybvbafUS7YKJcM37wW+nWzyea4RSX9M6YErE4AxwOuAecDtwAckjQTeCTwMnAk8EhFdEXEYuBOf40wMxdDdCeyMiNXJegelEL6X0iPc3gOsi4gBb+Wzis0B1kfEb5J1n+fa+XNKP6y6IuIQcB9wNnAr8AHgUuDbSchaTgy50I2I3cAOSROTTecDmyLiFUp3ntxE6R8twFbgVElvTtY/CPwwzXoL4FLgrqMrPs819WtghqQTkscJng9sjohngWeBv+V/zvEaYKakUclvGZfic5yJIRe6iU8Bd0r6OaXxrX9Itt8JHAG+D68GxHzg25I2JvtuTr3aQUrS64BZlHpgPfk810Dy21oHsB7YSOn/89Fbf+8EdkTE5uTYXZSG0VYAj1H6LeOB1Iu2oTllrD+SPguMiIi/y7qWIvN5rj9JC4ENEbEo61rs9/mBN4lkWs1plK7uWp34PNefpHWULhB/Juta7A+5p2tmlqKhOqZrZpYJh66ZWYocumZmKXLompmlyKFrZpai/wYq6N2i8YKBugAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from mmaction.core import confusion_matrix \n",
    "\n",
    "gt_labels = [ann['label'] for ann in dataset.load_annotations()]\n",
    "pred = np.argmax(outputs, axis=1)\n",
    "cf_mat = confusion_matrix(pred, gt_labels).astype(float)\n",
    "cmap = sns.cubehelix_palette(50, hue=0.05, rot=0, light=0.9, dark=0, as_cmap=True)\n",
    "# /np.sum(cf_mat), fmt='.2%',\n",
    "sns.heatmap(cf_mat, cmap=cmap, annot=True, xticklabels = ['6yo', '7yo', '8yo'], yticklabels = ['6yo', '7yo', '8yo'])\n",
    "# sns.heatmap(cf_mat, cmap=cmap, annot=True, xticklabels = ['7yo', '8yo', '9yo', '10yo'], yticklabels = ['7yo', '8yo', '9yo', '10yo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d913e147-f21f-493b-a2b5-656b6d69ff53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8181818181818182\n"
     ]
    }
   ],
   "source": [
    "mean_error = (2+6+1)/11 #number of all tested videos\n",
    "print(mean_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d97e7e-2c18-486e-a2fb-1f2ef307618f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4a83dfc-f42b-43e1-8a52-685e2608adc1",
   "metadata": {},
   "source": [
    "# CSN age run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "209d941e-d33f-45b0-8c64-5885a3d74dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/robt427nv/childact\n"
     ]
    }
   ],
   "source": [
    "cd /home/robt427nv/childact/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b42ff397-0157-4227-9f3f-fa50a441ebf2",
   "metadata": {
    "id": "LjCcmCKOjktc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mmcv import Config\n",
    "cfg = Config.fromfile('mmaction2/configs/recognition/csn/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2f0f20b-f0c3-4302-8abb-3c58b564c008",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "tlhu9byjjt-K",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "13d1bb01-f351-48c0-9efb-0bdf2c125d29",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "model = dict(\n",
      "    type='Recognizer3D',\n",
      "    backbone=dict(\n",
      "        type='ResNet3dCSN',\n",
      "        pretrained2d=False,\n",
      "        pretrained=\n",
      "        'https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth',\n",
      "        depth=152,\n",
      "        with_pool2=False,\n",
      "        bottleneck_mode='ir',\n",
      "        norm_eval=True,\n",
      "        zero_init_residual=False,\n",
      "        bn_frozen=True),\n",
      "    cls_head=dict(\n",
      "        type='I3DHead',\n",
      "        num_classes=6,\n",
      "        in_channels=2048,\n",
      "        spatial_type='avg',\n",
      "        dropout_ratio=0.5,\n",
      "        init_std=0.01),\n",
      "    train_cfg=None,\n",
      "    test_cfg=dict(average_clips='prob'))\n",
      "checkpoint_config = dict(interval=20)\n",
      "log_config = dict(interval=100, hooks=[dict(type='TextLoggerHook')])\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = 'checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth'\n",
      "resume_from = None\n",
      "workflow = [('train', 1)]\n",
      "dataset_type = 'RawframeDataset'\n",
      "data_root = 'age-gender-3split-rgb-frames/'\n",
      "data_root_val = 'age-gender-3split-rgb-frames/val/'\n",
      "ann_file_train = 'age-gender-3split-rgb-frames/childact_train_rgb320_age_run.txt'\n",
      "ann_file_val = 'age-gender-3split-rgb-frames/childact_val_rgb320_age_run.txt'\n",
      "ann_file_test = 'age-gender-3split-rgb-frames/childact_test_rgb320_age_run.txt'\n",
      "img_norm_cfg = dict(\n",
      "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_bgr=False)\n",
      "train_pipeline = [\n",
      "    dict(type='SampleFrames', clip_len=32, frame_interval=2, num_clips=1),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='RandomResizedCrop'),\n",
      "    dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "    dict(type='Flip', flip_ratio=0.5),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "]\n",
      "val_pipeline = [\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=32,\n",
      "        frame_interval=2,\n",
      "        num_clips=1,\n",
      "        test_mode=True),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='CenterCrop', crop_size=224),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=32,\n",
      "        frame_interval=2,\n",
      "        num_clips=10,\n",
      "        test_mode=True),\n",
      "    dict(type='RawFrameDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='ThreeCrop', crop_size=256),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCTHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "data = dict(\n",
      "    videos_per_gpu=6,\n",
      "    workers_per_gpu=4,\n",
      "    train=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_train_rgb320_age_run.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/train/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=1),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='RandomResizedCrop'),\n",
      "            dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "            dict(type='Flip', flip_ratio=0.5),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'),\n",
      "    val=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file='age-gender-3split-rgb-frames/childact_val_rgb320_age_run.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/val/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=1,\n",
      "                test_mode=True),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='CenterCrop', crop_size=224),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'),\n",
      "    test=dict(\n",
      "        type='RawframeDataset',\n",
      "        ann_file=\n",
      "        'age-gender-3split-rgb-frames/childact_test_rgb320_age_run.txt',\n",
      "        data_prefix='age-gender-3split-rgb-frames/test/',\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=32,\n",
      "                frame_interval=2,\n",
      "                num_clips=10,\n",
      "                test_mode=True),\n",
      "            dict(type='RawFrameDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='ThreeCrop', crop_size=256),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCTHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ],\n",
      "        filename_tmpl='{:03}.jpeg'))\n",
      "evaluation = dict(\n",
      "    interval=5, metrics=['top_k_accuracy', 'mean_class_accuracy'])\n",
      "optimizer = dict(type='SGD', lr=0.000125, momentum=0.9, weight_decay=0.0001)\n",
      "optimizer_config = dict(grad_clip=dict(max_norm=40, norm_type=2))\n",
      "lr_config = dict(\n",
      "    policy='step',\n",
      "    step=[32, 48],\n",
      "    warmup='linear',\n",
      "    warmup_ratio=0.1,\n",
      "    warmup_by_epoch=True,\n",
      "    warmup_iters=16)\n",
      "total_epochs = 50\n",
      "work_dir = './childact-checkpoints/CSN-age-run'\n",
      "find_unused_parameters = True\n",
      "omnisource = False\n",
      "seed = 42\n",
      "gpu_ids = range(0, 1)\n",
      "output_config = dict(out='./childact-checkpoints/CSN-age-run/results.json')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mmcv.runner import set_random_seed\n",
    "\n",
    "# Modify dataset type and path\n",
    "# cfg.dataset_type = 'VideoDataset'\n",
    "cfg.data_root = 'age-gender-3split-rgb-frames/'\n",
    "cfg.data_root_val = 'age-gender-3split-rgb-frames/val/'\n",
    "cfg.ann_file_train = 'age-gender-3split-rgb-frames/childact_train_rgb320_age_run.txt'\n",
    "cfg.ann_file_val = 'age-gender-3split-rgb-frames/childact_val_rgb320_age_run.txt'\n",
    "cfg.ann_file_test = 'age-gender-3split-rgb-frames/childact_test_rgb320_age_run.txt'\n",
    "\n",
    "# cfg.data.test.type = 'VideoDataset'\n",
    "cfg.data.test.ann_file = 'age-gender-3split-rgb-frames/childact_test_rgb320_age_run.txt'\n",
    "cfg.data.test.data_prefix = 'age-gender-3split-rgb-frames/test/'\n",
    "\n",
    "# cfg.data.train.type = 'VideoDataset'\n",
    "cfg.data.train.ann_file = 'age-gender-3split-rgb-frames/childact_train_rgb320_age_run.txt'\n",
    "cfg.data.train.data_prefix = 'age-gender-3split-rgb-frames/train/'\n",
    "\n",
    "# cfg.data.val.type = 'VideoDataset'\n",
    "cfg.data.val.ann_file = 'age-gender-3split-rgb-frames/childact_val_rgb320_age_run.txt'\n",
    "cfg.data.val.data_prefix = 'age-gender-3split-rgb-frames/val/'\n",
    "\n",
    "# cfg.data.test.modality = 'Flow'\n",
    "# cfg.data.val.modality = 'Flow'\n",
    "# cfg.data.train.modality = 'Flow'\n",
    "\n",
    "# cfg.data.train.start_index = 0\n",
    "# cfg.data.test.start_index = 0\n",
    "# cfg.data.val.start_index = 0\n",
    "\n",
    "cfg.data.test.filename_tmpl = '{:03}.jpeg'\n",
    "cfg.data.train.filename_tmpl = '{:03}.jpeg'\n",
    "cfg.data.val.filename_tmpl = '{:03}.jpeg'\n",
    "# The flag is used to determine whether it is omnisource training\n",
    "cfg.setdefault('omnisource', False)\n",
    "# Modify num classes of the model in cls_head\n",
    "cfg.model.cls_head.num_classes = 6\n",
    "# We can use the pre-trained TSN model\n",
    "cfg.load_from = 'checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth'\n",
    "# cfg.resume_from = './childact-mm/latest.pth'\n",
    "\n",
    "# Set up working dir to save files and logs.\n",
    "cfg.work_dir = './childact-checkpoints/CSN-age-run'\n",
    "\n",
    "cfg.total_epochs = 50\n",
    "\n",
    "# cfg.momentum_config = dict(\n",
    "#     policy='cyclic',\n",
    "#     target_ratio=(0.85 / 0.95, 1),\n",
    "#     cyclic_times=1,\n",
    "#     step_ratio_up=0.4,\n",
    "# )\n",
    "\n",
    "# We can set the checkpoint saving interval to reduce the storage cost\n",
    "cfg.checkpoint_config.interval = 20\n",
    "# We can set the log print interval to reduce the the times of printing log\n",
    "cfg.log_config.interval = 100\n",
    "\n",
    "# Set seed thus the results are more reproducible\n",
    "cfg.seed = 42\n",
    "set_random_seed(42, deterministic=False)\n",
    "cfg.gpu_ids = range(1)\n",
    "\n",
    "cfg.data.videos_per_gpu=6\n",
    "\n",
    "# cfg.model.backbone.in_channels = 2\n",
    "\n",
    "cfg.output_config = dict(out=f'{cfg.work_dir}/results.json')\n",
    "# We can initialize the logger for training and have a look\n",
    "# at the final config used for training\n",
    "# del cfg.optimizer['momentum']\n",
    "print(f'Config:\\n{cfg.pretty_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "813f2097-cd2b-4ee5-bd12-e59c09329642",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "dDBWkdDRk6oz",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "85a52ef3-7b5c-4c52-8fef-00322a8c65e6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-24 12:32:38,861 - mmaction - INFO - load model from: https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth\n",
      "2021-08-24 12:32:38,862 - mmaction - INFO - Use load_from_http loader\n",
      "2021-08-24 12:32:40,903 - mmaction - INFO - load checkpoint from checkpoints/ircsn_ig65m_pretrained_bnfrozen_r152_32x2x1_58e_kinetics400_rgb_20200812-9037a758.pth\n",
      "2021-08-24 12:32:40,904 - mmaction - INFO - Use load_from_local loader\n",
      "2021-08-24 12:32:41,073 - mmaction - WARNING - The model and loaded state dict do not match exactly\n",
      "\n",
      "size mismatch for cls_head.fc_cls.weight: copying a param with shape torch.Size([400, 2048]) from checkpoint, the shape in current model is torch.Size([6, 2048]).\n",
      "size mismatch for cls_head.fc_cls.bias: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([6]).\n",
      "2021-08-24 12:32:41,084 - mmaction - INFO - Start running, host: robt427nv@robt427NV, work_dir: /home/robt427nv/childact/childact-checkpoints/CSN-age-run\n",
      "2021-08-24 12:32:41,085 - mmaction - INFO - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(ABOVE_NORMAL) OptimizerHook                      \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "2021-08-24 12:32:41,085 - mmaction - INFO - workflow: [('train', 1)], max: 50 epochs\n",
      "/home/robt427nv/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/mmcv/runner/hooks/evaluation.py:190: UserWarning: runner.meta is None. Creating an empty one.\n",
      "  warnings.warn('runner.meta is None. Creating an empty one.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 23/23, 10.6 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-24 12:34:37,600 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-24 12:34:37,602 - mmaction - INFO - \n",
      "top1_acc\t0.3913\n",
      "top5_acc\t0.8696\n",
      "2021-08-24 12:34:37,602 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-24 12:34:37,604 - mmaction - INFO - \n",
      "mean_acc\t0.2000\n",
      "2021-08-24 12:34:37,957 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_5.pth.\n",
      "2021-08-24 12:34:37,958 - mmaction - INFO - Best top1_acc is 0.3913 at 5 epoch.\n",
      "2021-08-24 12:34:37,959 - mmaction - INFO - Epoch(val) [5][4]\ttop1_acc: 0.3913, top5_acc: 0.8696, mean_class_accuracy: 0.2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 23/23, 11.4 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-24 12:36:33,830 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-24 12:36:33,831 - mmaction - INFO - \n",
      "top1_acc\t0.3913\n",
      "top5_acc\t0.8696\n",
      "2021-08-24 12:36:33,832 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-24 12:36:33,833 - mmaction - INFO - \n",
      "mean_acc\t0.1667\n",
      "2021-08-24 12:36:33,833 - mmaction - INFO - Epoch(val) [10][4]\ttop1_acc: 0.3913, top5_acc: 0.8696, mean_class_accuracy: 0.1667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 23/23, 11.3 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-24 12:38:30,279 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-24 12:38:30,281 - mmaction - INFO - \n",
      "top1_acc\t0.3478\n",
      "top5_acc\t0.8696\n",
      "2021-08-24 12:38:30,282 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-24 12:38:30,283 - mmaction - INFO - \n",
      "mean_acc\t0.2489\n",
      "2021-08-24 12:38:30,284 - mmaction - INFO - Epoch(val) [15][4]\ttop1_acc: 0.3478, top5_acc: 0.8696, mean_class_accuracy: 0.2489\n",
      "2021-08-24 12:40:24,761 - mmaction - INFO - Saving checkpoint at 20 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 23/23, 11.4 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-24 12:40:27,278 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-24 12:40:27,279 - mmaction - INFO - \n",
      "top1_acc\t0.3913\n",
      "top5_acc\t0.8696\n",
      "2021-08-24 12:40:27,280 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-24 12:40:27,280 - mmaction - INFO - \n",
      "mean_acc\t0.2556\n",
      "2021-08-24 12:40:27,281 - mmaction - INFO - Epoch(val) [20][4]\ttop1_acc: 0.3913, top5_acc: 0.8696, mean_class_accuracy: 0.2556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 23/23, 11.8 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-24 12:42:23,667 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-24 12:42:23,669 - mmaction - INFO - \n",
      "top1_acc\t0.3478\n",
      "top5_acc\t0.8696\n",
      "2021-08-24 12:42:23,670 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-24 12:42:23,671 - mmaction - INFO - \n",
      "mean_acc\t0.1778\n",
      "2021-08-24 12:42:23,672 - mmaction - INFO - Epoch(val) [25][4]\ttop1_acc: 0.3478, top5_acc: 0.8696, mean_class_accuracy: 0.1778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 23/23, 11.5 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-24 12:44:19,939 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-24 12:44:19,941 - mmaction - INFO - \n",
      "top1_acc\t0.3913\n",
      "top5_acc\t0.8696\n",
      "2021-08-24 12:44:19,942 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-24 12:44:19,943 - mmaction - INFO - \n",
      "mean_acc\t0.1963\n",
      "2021-08-24 12:44:19,943 - mmaction - INFO - Epoch(val) [30][4]\ttop1_acc: 0.3913, top5_acc: 0.8696, mean_class_accuracy: 0.1963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 23/23, 11.3 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-24 12:46:16,296 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-24 12:46:16,297 - mmaction - INFO - \n",
      "top1_acc\t0.4783\n",
      "top5_acc\t0.8696\n",
      "2021-08-24 12:46:16,298 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-24 12:46:16,299 - mmaction - INFO - \n",
      "mean_acc\t0.2778\n",
      "2021-08-24 12:46:16,731 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_35.pth.\n",
      "2021-08-24 12:46:16,732 - mmaction - INFO - Best top1_acc is 0.4783 at 35 epoch.\n",
      "2021-08-24 12:46:16,732 - mmaction - INFO - Epoch(val) [35][4]\ttop1_acc: 0.4783, top5_acc: 0.8696, mean_class_accuracy: 0.2778\n",
      "2021-08-24 12:48:10,862 - mmaction - INFO - Saving checkpoint at 40 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 23/23, 10.8 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-24 12:48:13,493 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-24 12:48:13,494 - mmaction - INFO - \n",
      "top1_acc\t0.4783\n",
      "top5_acc\t0.8696\n",
      "2021-08-24 12:48:13,495 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-24 12:48:13,495 - mmaction - INFO - \n",
      "mean_acc\t0.3333\n",
      "2021-08-24 12:48:13,496 - mmaction - INFO - Epoch(val) [40][4]\ttop1_acc: 0.4783, top5_acc: 0.8696, mean_class_accuracy: 0.3333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 23/23, 11.1 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-24 12:50:10,030 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-24 12:50:10,032 - mmaction - INFO - \n",
      "top1_acc\t0.4783\n",
      "top5_acc\t0.8696\n",
      "2021-08-24 12:50:10,033 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-24 12:50:10,034 - mmaction - INFO - \n",
      "mean_acc\t0.2778\n",
      "2021-08-24 12:50:10,035 - mmaction - INFO - Epoch(val) [45][4]\ttop1_acc: 0.4783, top5_acc: 0.8696, mean_class_accuracy: 0.2778\n",
      "2021-08-24 12:52:04,428 - mmaction - INFO - Saving checkpoint at 50 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 23/23, 10.6 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-24 12:52:07,107 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-08-24 12:52:07,117 - mmaction - INFO - \n",
      "top1_acc\t0.4783\n",
      "top5_acc\t0.8696\n",
      "2021-08-24 12:52:07,118 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2021-08-24 12:52:07,119 - mmaction - INFO - \n",
      "mean_acc\t0.2926\n",
      "2021-08-24 12:52:07,119 - mmaction - INFO - Epoch(val) [50][4]\ttop1_acc: 0.4783, top5_acc: 0.8696, mean_class_accuracy: 0.2926\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "from mmaction.datasets import build_dataset\n",
    "from mmaction.models import build_model\n",
    "from mmaction.apis import train_model\n",
    "\n",
    "import mmcv\n",
    "\n",
    "# Build the dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# Build the recognizer\n",
    "model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# model = build_detector(cfg.model, train_cfg=cfg.train_cfg, test_cfg=cfg.test_cfg)\n",
    "# CUDA_LAUNCH_BLOCKING=1\n",
    "# Create work_dir\n",
    "mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
    "train_model(model, datasets, cfg, distributed=False, validate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2c065d2-7580-454e-a06c-0b9cafa457b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f\"{cfg.work_dir}/model50e\", 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8561cd07-c5f1-47b0-9a59-31e34a3a994b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 16:54:18,016 - mmaction - INFO - load model from: https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth\n",
      "2021-08-10 16:54:18,017 - mmaction - INFO - Use load_from_http loader\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "from mmaction.datasets import build_dataset\n",
    "from mmaction.models import build_model\n",
    "from mmaction.apis import train_model\n",
    "import pickle\n",
    "import mmcv\n",
    "# Build the dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# Build the recognizer\n",
    "model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "\n",
    "model = pickle.load(open(f\"{cfg.work_dir}/model50e\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e6222c8-56a4-4b5a-9643-346a035f595f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eyY3hCMwyTct",
    "outputId": "200e37c7-0da4-421f-98da-41418c3110ca",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 26/26, 0.6 task/s, elapsed: 41s, ETA:     0s\n",
      "Evaluating top_k_accuracy ...\n",
      "\n",
      "top1_acc\t0.4615\n",
      "top5_acc\t0.9615\n",
      "\n",
      "Evaluating mean_class_accuracy ...\n",
      "\n",
      "mean_acc\t0.3214\n",
      "top1_acc: 0.4615\n",
      "top5_acc: 0.9615\n",
      "mean_class_accuracy: 0.3214\n"
     ]
    }
   ],
   "source": [
    "from mmaction.apis import single_gpu_test\n",
    "from mmaction.datasets import build_dataloader\n",
    "from mmcv.parallel import MMDataParallel\n",
    "# from mmaction.models import build_model\n",
    "# from mmaction.datasets import build_dataset\n",
    "\n",
    "# model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# Build a test dataloader\n",
    "dataset = build_dataset(cfg.data.test, dict(test_mode=True))\n",
    "data_loader = build_dataloader(\n",
    "        dataset,\n",
    "        videos_per_gpu=1,\n",
    "        workers_per_gpu=1,\n",
    "        dist=False,\n",
    "        shuffle=False)\n",
    "model = MMDataParallel(model, device_ids=[0])\n",
    "outputs = single_gpu_test(model, data_loader)\n",
    "\n",
    "eval_config = cfg.evaluation\n",
    "eval_config.pop('interval')\n",
    "eval_res = dataset.evaluate(outputs, **eval_config)\n",
    "for name, val in eval_res.items():\n",
    "    print(f'{name}: {val:.04f}')\n",
    "    \n",
    "output_config = cfg.output_config\n",
    "dataset.dump_results(outputs, **output_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "219f635c-f3c0-4233-835a-3239400b48c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from mmaction.core import confusion_matrix \n",
    "\n",
    "gt_labels = [ann['label'] for ann in dataset.load_annotations()]\n",
    "pred = np.argmax(outputs, axis=1)\n",
    "cf_mat = confusion_matrix(pred, gt_labels).astype(float)\n",
    "cmap = sns.cubehelix_palette(50, hue=0.05, rot=0, light=0.9, dark=0, as_cmap=True)\n",
    "# /np.sum(cf_mat), fmt='.2%',\n",
    "# sns.heatmap(cf_mat, cmap=cmap, annot=True, xticklabels = ['6yo', '7yo', '8yo'], yticklabels = ['6yo', '7yo', '8yo'])\n",
    "sns.heatmap(cf_mat, cmap=cmap, annot=True, xticklabels = ['6yo','7yo', '8yo', '9yo', '10yo', '11yo'], yticklabels = ['6yo','7yo', '8yo', '9yo', '10yo', '11yo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32038607-9caa-4d97-aaf7-8dee6dc7e142",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5769230769230769\n"
     ]
    }
   ],
   "source": [
    "mean_error = (7+6+2)/26 #number of all tested videos\n",
    "print(mean_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ab16ed-4917-4cd5-865f-d23743d1a0f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
